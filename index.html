<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Maxime El Masri">
<meta name="author" content="Jérôme Morio">
<meta name="author" content="Florian Simatos">
<meta name="dcterms.date" content="2023-01-18">
<meta name="keywords" content="Importance sampling, High dimension, Gaussian covariance matrix, Kullback-Leibler divergence, Projection">
<meta name="description" content="This document provides a dimension-reduction strategy in order to improve the performance of importance sampling in high dimension.">

<title>Optimal projection for parametric importance sampling in high dimension</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="content_files/libs/clipboard/clipboard.min.js"></script>
<script src="content_files/libs/quarto-html/quarto.js"></script>
<script src="content_files/libs/quarto-html/popper.min.js"></script>
<script src="content_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="content_files/libs/quarto-html/anchor.min.js"></script>
<link href="content_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="content_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="content_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="content_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="content_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="content_files/libs/quarto-contrib/pseudocode-1.0.0/pseudocode.min.js"></script>
<link href="content_files/libs/quarto-contrib/pseudocode-1.0.0/pseudocode.min.css" rel="stylesheet">
<style>
    .quarto-title-block .quarto-title-banner {
      color: #FFFFFF;
background: #034E79;
    }
    </style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><a href="https://computo.sfds.asso.fr">
        <img src="https://computo.sfds.asso.fr/assets/img/logo_notext_white.png" height="60px">
      </a> &nbsp; Optimal projection for parametric importance sampling in high dimension</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> source</button></div></div>
            <p><a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons BY License"></a>
ISSN 2824-7795</p>
            <div>
        <div class="description">
          <p>This document provides a dimension-reduction strategy in order to improve the performance of importance sampling in high dimension.</p>
        </div>
      </div>
                </div>
  </div>
    
    <div class="quarto-title-meta-author">
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-heading">Affiliations</div>
          
          <div class="quarto-title-meta-contents">
        Maxime El Masri <a href="https://orcid.org/0000-0002-9127-4503" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.onera.fr/">ONERA/DTIS</a>, <a href="https://www.isae-supaero.fr/">ISAE-SUPAERO</a>, <a href="https://www.univ-toulouse.fr/">Université de Toulouse</a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://www.onera.fr/en/staff/jerome-morio?destination=node/981">Jérôme Morio</a> <a href="https://orcid.org/0000-0002-8811-8956" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.onera.fr/">ONERA/DTIS</a>, <a href="https://www.univ-toulouse.fr/">Université de Toulouse</a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://pagespro.isae-supaero.fr/florian-simatos/">Florian Simatos</a> 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.isae-supaero.fr/">ISAE-SUPAERO</a>, <a href="https://www.univ-toulouse.fr/">Université de Toulouse</a>
                </p>
            </div>
        </div>
                    
  <div class="quarto-title-meta">
                                
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 18, 2023</p>
      </div>
    </div>
                                    
      
                  
      <div>
      <div class="quarto-title-meta-heading">Keywords</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Importance sampling, High dimension, Gaussian covariance matrix, Kullback-Leibler divergence, Projection</p>
      </div>
    </div>
    
    <div>
      <div class="quarto-title-meta-heading">Status</div>
      <div class="quarto-title-meta-contents">
              <p class="date">draft</p>
                  </div>
    </div>

  </div>
                                                
  <div>
    <div class="abstract">
    <div class="abstract-title">Abstract</div>
      <p>In this paper we propose a dimension-reduction strategy in order to improve the performance of importance sampling in high dimension. The idea is to estimate variance terms in a small number of suitably chosen directions. We first prove that the optimal directions, i.e., the ones that minimize the Kullback–Leibler divergence with the optimal auxiliary density, are the eigenvectors associated to extreme (small or large) eigenvalues of the optimal covariance matrix. We then perform extensive numerical experiments that show that as dimension increases, these directions give estimations which are very close to optimal. Moreover, we show that the estimation remains accurate even when a simple empirical estimator of the covariance matrix is used to estimate these directions. These theoretical and numerical results open the way for different generalizations, in particular the incorporation of such ideas in adaptive importance sampling schemes.</p>
    </div>
  </div>

  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#sec-choicek" id="toc-sec-choicek" class="nav-link active" data-scroll-target="#sec-choicek"><span class="toc-section-number">0.1</span>  Choice of the number of dimensions <span class="math inline">k</span></a></li>
  <li><a href="#sec-mm" id="toc-sec-mm" class="nav-link" data-scroll-target="#sec-mm"><span class="toc-section-number">0.2</span>  Theoretical result concerning the projection on <span class="math inline">\mathbf{m}^*</span></a></li>
  <li><a href="#sec-proof" id="toc-sec-proof" class="nav-link" data-scroll-target="#sec-proof"><span class="toc-section-number">1</span>  Proof of Theorems Theorem&nbsp;1 and Theorem&nbsp;2</a></li>
  <li><a href="#sec-num-results-framework" id="toc-sec-num-results-framework" class="nav-link" data-scroll-target="#sec-num-results-framework"><span class="toc-section-number">2</span>  Framework for the numerical results</a>
  <ul class="collapse">
  <li><a href="#general-framework" id="toc-general-framework" class="nav-link" data-scroll-target="#general-framework"><span class="toc-section-number">2.1</span>  General framework</a></li>
  <li><a href="#sec-def_cov" id="toc-sec-def_cov" class="nav-link" data-scroll-target="#sec-def_cov"><span class="toc-section-number">2.2</span>  Choice of the auxiliary density <span class="math inline">g'</span> for the Gaussian model</a></li>
  <li><a href="#choice-of-the-auxiliary-density-g-for-the-von-misesfishernakagami-model" id="toc-choice-of-the-auxiliary-density-g-for-the-von-misesfishernakagami-model" class="nav-link" data-scroll-target="#choice-of-the-auxiliary-density-g-for-the-von-misesfishernakagami-model"><span class="toc-section-number">2.3</span>  Choice of the auxiliary density <span class="math inline">g'</span> for the von Mises–Fisher–Nakagami model</a></li>
  </ul></li>
  <li><a href="#sec-test-cases" id="toc-sec-test-cases" class="nav-link" data-scroll-target="#sec-test-cases"><span class="toc-section-number">3</span>  Numerical results on five test cases</a>
  <ul class="collapse">
  <li><a href="#sec-sub:sum" id="toc-sec-sub:sum" class="nav-link" data-scroll-target="#sec-sub\:sum"><span class="toc-section-number">3.1</span>  Test case 1: one-dimensional optimal projection</a>
  <ul class="collapse">
  <li><a href="#evolution-of-the-partial-kl-divergence-and-spectrum" id="toc-evolution-of-the-partial-kl-divergence-and-spectrum" class="nav-link" data-scroll-target="#evolution-of-the-partial-kl-divergence-and-spectrum"><span class="toc-section-number">3.1.1</span>  Evolution of the partial KL divergence and spectrum</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
</nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



<div class="cell" data-execution_count="1">
<details>
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 1. Plot of the function "l"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CEIS_vMFNM <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, Math, Latex</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tabulate <span class="im">import</span> tabulate</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(np.finfo(<span class="bu">float</span>).eps,<span class="fl">4.0</span>,<span class="dv">100</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="op">-</span>np.log(x) <span class="op">+</span> x <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>ax.plot(x, y, linewidth<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">4</span>), xticks<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>       ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), yticks<span class="op">=</span>[<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="fl">1.5</span>])</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$x$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(x)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-l" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="content_files/figure-html/fig-l-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Plot of the function <span class="math inline">\ell=-\log(x) + x - 1</span> given by <strong>?@eq-l</strong></figcaption><p></p>
</figure>
</div>
</div>
</div>
<div id="thm-thm1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 </strong></span>Let <span class="math inline">(\lambda^*_i, \mathbf{d}^*_i)</span> be the eigenpairs of <span class="math inline">\Sigma^*</span> ranked in decreasing <span class="math inline">\ell</span>-order. Then for <span class="math inline">1 \leq k \leq n</span>, the solution <span class="math inline">(\mathbf{m}^*_k, \Sigma^*_k)</span> to <strong>?@eq-argminDkl-k</strong> is given by <span id="eq-Sigma*k"><span class="math display">
\mathbf{m}^*_k = \mathbf{m}^* \ \text{ and } \ \Sigma^*_k = I_n + \sum_{i=1}^k \left( \lambda^*_i - 1 \right) \mathbf{d}^*_i (\mathbf{d}^*_i)^\top.
\tag{1}</span></span></p>
</div>
<p>For <span class="math inline">k = 1</span> for instance, the shape of the function <span class="math inline">\ell</span> depicted in <a href="#fig-l">Figure&nbsp;1</a> implies that <span class="math inline">\Sigma^*_1 = I_n + (\lambda^*-1) \mathbf{d}^* (\mathbf{d}^*)^\top</span> with <span class="math inline">(\lambda^*, \mathbf{d}^*)</span> the eigenpair of <span class="math inline">\Sigma^*</span> with <span class="math inline">\lambda^*</span> either the largest or the smallest eigenvalue of <span class="math inline">\Sigma^*</span>, depending on which one maximizes <span class="math inline">\ell</span>.</p>
<p>This theoretical result therefore suggests to reduce dimension by estimating eigenpairs of <span class="math inline">\Sigma^*</span>, rank them in decreasing <span class="math inline">\ell</span>-order and then use the <span class="math inline">k</span> first eigenpairs <span class="math inline">(({\widehat{\lambda}}^*_i, {\widehat{\mathbf{d}}}^*_i), i = 1, \ldots, k)</span> to build the covariance matrix <span class="math inline">\widehat \Sigma^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n</span> and the corresponding auxiliary density. This scheme is summarized in Algorithm 1. The effective dimension <span class="math inline">k</span> is obtained by Algorithm 2, see <a href="#sec-choicek">Section&nbsp;0.1</a> below.</p>
<pre class="pseudocode"><code>\begin{algorithm}
\caption{Algorithm suggested by Theorem 1.}
\begin{algorithmic}
\State \textbf{Data}: Sample sizes $N$ and $M$
\State \textbf{Result}: Estimation $\widehat{E}_N$ of integral $E$
\State - Generate a sample $\mathbf{X}_1,\ldots,\mathbf{X}_M$  on $\mathbb{R}^n$ independently according to $g^*$
\State  - Estimate $\widehat{\mathbf{m}}^*$ and $\widehat{\Sigma}^*$ defined in Equation 10 and Equation 11 with this sample
\State - Compute the eigenpairs $(\widehat{\lambda}^*_i, \widehat{\mathbf{d}}^*_i)$ of $\widehat{\Sigma}^*$ ranked in decreasing $\ell$-order
\State - Compute the matrix $\widehat{\Sigma}^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n$ with $k$ obtained by applying Algorithm 2 with input $({\widehat{\lambda}}^*_1, \ldots, {\widehat{\lambda}}^*_n)$
\State - Generate a new sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ independently from $g' = g_{\widehat{\mathbf{m}}^*,\widehat{\Sigma}^*_k}$
\State - Return $\displaystyle \widehat{E}_N=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}$
\end{algorithmic}
\end{algorithm}</code></pre>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>The value <span class="math inline">1</span> plays a particular role in <a href="#thm-thm1">Theorem&nbsp;1</a>, in that, as <span class="math inline">\ell</span> is minimized in <span class="math inline">1</span>, eigenvectors with eigenvalues <span class="math inline">1</span> will only be selected once all other eigenvalues will have been picked: in other words, if <span class="math inline">\lambda^*_i = 1</span> then <span class="math inline">\lambda^*_j = 1</span> for all <span class="math inline">j \geq i</span>. The reason why <span class="math inline">1</span> plays this special role is due to the form of the covariance matrix that we impose. More precisely, looking for covariance matrices in the set <span class="math inline">\mathcal{L}_{n,k}</span> amounts to looking for covariance matrices which, once diagonalized, have one’s on the diagonal except possibly for <span class="math inline">k</span> values (the <span class="math inline">\alpha_i</span>’s). As <span class="math inline">k</span> will be small, typically <span class="math inline">k = 1</span> or <span class="math inline">2</span>, this amounts to looking for covariance matrices which are perturbation of the identity. This is particularly relevant as we assume <span class="math inline">f</span> is a standard Gaussian density. What <a href="#thm-thm1">Theorem&nbsp;1</a> tells is that, when trying to approximate <span class="math inline">\Sigma^*</span> by such matrices, we should first consider eigenvectors with eigenvalues as different as possible from <span class="math inline">1</span>, the “distance” to <span class="math inline">1</span> being measured by <span class="math inline">\ell</span>. If one was imposing a different form on <span class="math inline">\Sigma^*_k</span> (which can be interesting if the distribution <span class="math inline">f</span> is not standardized), then a different result would arise. For instance, if one was looking for matrices where the “default” choice would be some <span class="math inline">\lambda &gt; 0</span> for the diagonal entries that are not estimated, i.e., a matrix of the form <span class="math inline">\sum_i (\alpha_i-\lambda) \mathbf{d}_i \mathbf{d}_i^\top + \lambda I_n</span>, then eigenpairs would be ranked according to the function <span class="math inline">\ell(\cdot/\lambda)</span>, meaning that one would look for eigenvectors associated to eigenvalues as different as possible from <span class="math inline">\lambda</span>.</p>
</div>
<p>As mentioned above, we assume in the first step of Algorithm 1 that we can sample according to <span class="math inline">g^*</span>. Since <span class="math inline">g^*</span> is known up to a multiplicative constant, this is a reasonable assumption as classical techniques such as importance sampling with self-normalized weights or Markov Chain Monte–Carlo (MCMC) can be applied in this case (see for instance <span class="citation" data-cites="ChanKroese_ImprovedCrossentropyMethod_2012">(<a href="#ref-ChanKroese_ImprovedCrossentropyMethod_2012" role="doc-biblioref">Chan and Kroese 2012</a>)</span>, <span class="citation" data-cites="GraceEtAl_AutomatedStateDependentImportance_2014">(<a href="#ref-GraceEtAl_AutomatedStateDependentImportance_2014" role="doc-biblioref">Grace, Kroese, and Sandmann 2014</a>)</span>). In this paper, we choose to apply a basic rejection method that yields perfect independent samples from <span class="math inline">g^*</span>, possibly at the price of a high computational cost. As the primary goal of this paper is to understand whether the <span class="math inline">\mathbf{d}^*_i</span>’s are indeed good projection directions, this computational cost to generate from <span class="math inline">g^*</span> is not relevant for us and therefore not taken into account. Possible improvements to relax this assumption are discussed in the conclusion of the paper.</p>
<section id="sec-choicek" class="level2" data-number="0.1">
<h2 data-number="0.1" class="anchored" data-anchor-id="sec-choicek"><span class="header-section-number">0.1</span> Choice of the number of dimensions <span class="math inline">k</span></h2>
<p>The choice of the effective dimension <span class="math inline">k</span>, i.e., the number of projection directions considered, is important. If it is close to <span class="math inline">n</span>, then the matrix <span class="math inline">\widehat{\Sigma}^*_k</span> will be close to <span class="math inline">\widehat{\Sigma}^*</span> which is the situation we want to avoid in the first place. On the other hand, setting <span class="math inline">k=1</span> in all cases may be too simple and lead to suboptimal results. In practice however, this is often a good choice. In order to adapt <span class="math inline">k</span> dynamically, we consider a simple method based on the value of the KL divergence. Given the eigenvalues <span class="math inline">\lambda_1, \ldots, \lambda_n</span> ranked in decreasing <span class="math inline">\ell</span>-order, we look for the maximal gap in the sequence <span class="math inline">(\ell(\lambda_1), \ldots, \ell(\lambda_n))</span>. This allows to choose <span class="math inline">k</span> such that <span class="math inline">\sum_{i=1}^k \ell(\lambda_i)</span> is close to <span class="math inline">\sum_{i=1}^n \ell(\lambda_i)</span> which is equal, up to an additive constant, to the minimal KL divergence (see <a href="#eq-Dell">Equation&nbsp;3</a> below). The precise method is described in Algorithm 2.</p>
<pre class="pseudocode"><code>\begin{algorithm}
\caption{Choice of the number of dimensions}
\begin{algorithmic}
\State \textbf{Data}: Sequence of positive numbers $\lambda_1, \ldots, \lambda_n$ in decreasing $\ell$-order
\State \textbf{Result}: Number of selected dimensions $k$
\State - Compute the increments $\delta_i = \ell(\lambda_{i+1}) - \ell(\lambda_i)$ for $i=1\ldots n-1$
\State - Return $k=\arg\max \delta_i$, the index of the maximum of the differences.
\end{algorithmic}
\end{algorithm}</code></pre>
</section>
<section id="sec-mm" class="level2" data-number="0.2">
<h2 data-number="0.2" class="anchored" data-anchor-id="sec-mm"><span class="header-section-number">0.2</span> Theoretical result concerning the projection on <span class="math inline">\mathbf{m}^*</span></h2>
<p>In <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span>, the authors propose to project on the mean <span class="math inline">\mathbf{m}^*</span> of the optimal auxiliary density <span class="math inline">g^*</span>. Numerically, this algorithm is shown to perform well, but only a very heuristic explanation based on the light tail of the Gaussian distribution is provided to motivate this choice. It turns out that the techniques used in the proof of <a href="#thm-thm1">Theorem&nbsp;1</a> can shed light on why projecting on <span class="math inline">\mathbf{m}^*</span> may indeed be a good idea. Let us first state our theoretical result, and then explain why it justifies the idea of projecting on <span class="math inline">\mathbf{m}^*</span>.</p>
<div id="thm-thm2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 </strong></span>Consider <span class="math inline">\Sigma \in \mathcal{L}_{n,1}</span> of the form <span class="math inline">\Sigma = I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top</span> with <span class="math inline">\alpha &gt; 0</span> and <span class="math inline">\lVert \mathbf{d} \rVert = 1</span>. Then the minimizer in <span class="math inline">(\alpha, \mathbf{d})</span> of the KL divergence between <span class="math inline">f</span> and <span class="math inline">g_{\mathbf{m}^*, \Sigma}</span> is <span class="math inline">(1+\lVert m^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert)</span>: <span class="math display">\left( 1+\lVert \mathbf{m}^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert \right) = \arg \min_{\alpha, \mathbf{d}} \left\{ D(f, g_{\mathbf{m}^*, I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top}): \alpha &gt; 0, \ \lVert \mathbf{d} \rVert = 1 \right\}. </span></p>
</div>
<p>In other words, <span class="math inline">\mathbf{m}^*</span> appears as an optimal projection direction when one seeks to minimize the KL divergence between <span class="math inline">f</span> and the Gaussian density with mean <span class="math inline">\mathbf{m}^*</span> and covariance of the form <span class="math inline">I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top</span>. Let us now explain why this minimization problem is indeed relevant, and why choosing an auxiliary density which minimizes this KL divergence may indeed lead to an accurate estimation. The justification deeply relies on the recent results by <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span>.</p>
<p>As mentioned above, in a reliability context where one seeks to estimate a small probability <span class="math inline">p = \mathbb{P}(\mathbf{X} \in A),</span> Theorem <span class="math inline">1.3</span> in <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span> shows that <span class="math inline">D(g^*, g)</span> governs the sample size required for an accurate estimation of <span class="math inline">p</span>: more precisely, the estimation is accurate if the sample size is larger than <span class="math inline">e^{D(g^*, g)}</span>, and inaccurate otherwise. This motivates the rationale for minimizing the KL divergence with <span class="math inline">g^*</span>.</p>
<p>However, in high dimension, importance sampling is known to fail because of the weight degeneracy problem whereby <span class="math inline">\max_i L_i / \sum_i L_i \approx 1</span>, with the <span class="math inline">L_i</span>’s the unnormalized importance weights, or likelihood ratios: <span class="math inline">L_i = f(\mathbf{X}_i) / g(\mathbf{X}_i)</span> with the <span class="math inline">\mathbf{X}_i</span>’s i.i.d. drawn according to <span class="math inline">g</span>. Theorem <span class="math inline">2.3</span> in <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span> shows that the weight degeneracy problem is avoided if the empirical mean of the likelihood ratios is close to <span class="math inline">1</span>, and for this, Theorem <span class="math inline">1.1</span> in <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span> shows that the sample size should be larger than <span class="math inline">e^{D(f, g)}</span>. In other words, these results suggest that the KL divergence with <span class="math inline">g^*</span> governs the sample size for an accurate estimation of <span class="math inline">p</span>, while the KL divergence with <span class="math inline">f</span> governs the weight degeneracy problem.</p>
<p>In light of these results, it becomes natural to consider the KL divergence with <span class="math inline">f</span> and not only <span class="math inline">g^*</span>. Of course, minimizing <span class="math inline">D(f, g_{\mathbf{m}, \Sigma})</span> without constraints on <span class="math inline">\mathbf{m}</span> and <span class="math inline">\Sigma</span> is trivial since <span class="math inline">g_{\mathbf{m}, \Sigma} = f</span> for <span class="math inline">\mathbf{m} = 0</span> and <span class="math inline">\Sigma = I_n</span>. However, these choices are the ones we want to avoid in the first place, and so it makes sense to impose some constraints on <span class="math inline">\mathbf{m}</span> and <span class="math inline">\Sigma</span>. If one keeps in mind the other objective of getting close to <span class="math inline">g^*</span>, then the choice <span class="math inline">\mathbf{m} = \mathbf{m}^*</span> becomes very natural, and we are led, when <span class="math inline">\Sigma \in \mathcal{L}_{n,1}</span> is sought as a rank-<span class="math inline">1</span> perturbation of the identity, to considering the optimization problem of <a href="#thm-thm2">Theorem&nbsp;2</a>.</p>
</section>
<section id="sec-proof" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Proof of Theorems <a href="#thm-thm1">Theorem&nbsp;1</a> and <a href="#thm-thm2">Theorem&nbsp;2</a></h1>
<p>We begin with a preliminary lemma.</p>
<div id="lem-D" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1 </strong></span>Let <span class="math inline">f</span> be the density of the standard Gaussian vector in dimension <span class="math inline">n</span>, <span class="math inline">\phi: \mathbb{R}^n \to \mathbb{R}_+</span> and <span class="math inline">g_* = f \phi / E</span> with <span class="math inline">E = \int f \phi</span>. Then for any <span class="math inline">\mathbf{m}</span> and any <span class="math inline">\Sigma</span> of the form <span class="math inline">\Sigma = I_n + \sum_i (\alpha_i - 1) \mathbf{d}_i \mathbf{d}_i^\top</span> with <span class="math inline">\alpha_i &gt; 0</span> and the <span class="math inline">\mathbf{d}_i</span>’s orthonormal, we have <span id="eq-D"><span class="math display">
\begin{aligned}
D(g^*, g_{\mathbf{m}, \Sigma}) =&amp;  \frac{1}{2} \sum_i \left( \log \alpha_i - \left(1 - \frac{1}{\alpha_i} \right) \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i \right) + \frac{1}{2} (\mathbf{m} - \mathbf{m}^*)^\top \Sigma^{-1} (\mathbf{m} - \mathbf{m}^*)\\
&amp;- \frac{1}{2} \lVert \mathbf{m}^* \rVert^2 - \log E + \mathbb{E}_{g^*}(\log \phi(\mathbf{X})).
\end{aligned}
\tag{2}</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>(of <a href="#lem-D">Lemma&nbsp;1</a>) For any <span class="math inline">\mathbf{m} \in \mathbb{R}^n</span> and <span class="math inline">\Sigma \in \mathcal{S}^+_n</span>, we have by definition <span class="math display"> D(g^*, g_{\mathbf{m}, \Sigma}) = \mathbb{E}_{g^*} \left( \log \left( \frac{g^*(\mathbf{X})}{g_{\mathbf{m}, \Sigma}(\mathbf{X})} \right) \right) = \mathbb{E}_{g^*} \left( \log \left( \frac{\frac{\phi(\mathbf{X}) e^{-\frac{1}{2} \lVert \mathbf{X} \rVert^2}}{E(2\pi)^{d/2}}}{ \frac{e^{-\frac{1}{2} (\mathbf{X} - \mathbf{m})^\top \Sigma^{-1} (\mathbf{X} - \mathbf{m})}}{(2\pi)^{d/2} \lvert \Sigma \rvert^{1/2}} } \right) \right) </span> and so <span class="math display">
D(g^*, g_{\mathbf{m}, \Sigma}) = - \frac{1}{2} \mathbb{E}_{g^*}(\lVert \mathbf{X} \rVert^2) + \frac{1}{2} \mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m})^\top \Sigma^{-1} (\mathbf{X} - \mathbf{m}) \right) + \frac{1}{2} \log \lvert \Sigma \rvert\\
- \log E + \mathbb{E}_{g^*}(\log \phi(\mathbf{X})).
</span> Because <span class="math inline">\mathbb{E}_{g^*}(\mathbf{X}) = \mathbf{m}^*</span>, we have <span class="math inline">\mathbb{E}_{g^*}(\lVert \mathbf{X} \rVert^2) = \mathbb{E}_{g^*}(\lVert \mathbf{X} - \mathbf{m}^* \rVert^2) + \lVert \mathbf{m}^* \rVert^2</span> and <span class="math display">
\mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m})^\top \Sigma^{-1} (\mathbf{X} - \mathbf{m}) \right) = \mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m}^*)^\top \Sigma^{-1} (\mathbf{X} - \mathbf{m}^*) \right)\\
+ (\mathbf{m} - \mathbf{m}^*)^\top \Sigma^{-1} (\mathbf{m} - \mathbf{m}^*).
</span> In the following derivations, we use the linearity of the trace and of the expectation, which make these two operators commute, as well as the identity <span class="math inline">a^\top b = \textrm{tr}(a b^\top)</span> for any two vectors <span class="math inline">a</span> and <span class="math inline">b</span>. With this caveat, we obtain <span class="math display">
\mathbb{E}_{g^*}\left[ \lVert \mathbf{X} - \mathbf{m}^* \rVert^2 \right] = \mathbb{E}_{g^*} \left[ \textrm{tr}((\mathbf{X} - \mathbf{m}^*) (\mathbf{X} - \mathbf{m}^*)^\top) \right] = \textrm{tr} (\Sigma^*)
</span> and we obtain with similar arguments <span class="math inline">\mathbb{E}_{g^*}( (\mathbf{X} - \mathbf{m}^*)^\top \Sigma^{-1} (\mathbf{X} - \mathbf{m}^*) ) = \textrm{tr} ( \Sigma^{-1} \Sigma^*)</span>. Consider now <span class="math inline">\Sigma = I_n + \sum_i (\alpha_i - 1) \mathbf{d}_i \mathbf{d}_i^\top</span> with <span class="math inline">\alpha_i &gt; 0</span> and the <span class="math inline">\mathbf{d}_i</span>’s orthonormal. Then the eigenvalues of <span class="math inline">\Sigma</span> potentially different from <span class="math inline">1</span> are the <span class="math inline">\alpha_i</span>’s (<span class="math inline">\alpha_i</span> is the eigenvalue associated with <span class="math inline">\mathbf{d}_i</span>), so that <span class="math display">\log \lvert \Sigma \rvert = \sum_i \log \alpha_i.
</span> Moreover, we have <span class="math inline">\Sigma^{-1} = I_n - \sum_i \beta_i \mathbf{d}_i \mathbf{d}_i^\top</span> with <span class="math inline">\beta_i = 1 - 1/\alpha_i</span> and so <span class="math display">
\textrm{tr}(\Sigma^{-1} \Sigma^*) = \textrm{tr}(\Sigma^*) - \sum_i \beta_i \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i.
</span> Gathering the previous relation, we finally obtain the desired result.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>(of <a href="#thm-thm1">Theorem&nbsp;1</a>) From <a href="#eq-D">Equation&nbsp;2</a> we see that the only dependency of <span class="math inline">D(g^*, g_{\mathbf{m}, \Sigma})</span> in <span class="math inline">\mathbf{m}</span> is in the quadratic term <span class="math inline">(\mathbf{m} - \mathbf{m}^*)^\top \Sigma^{-1} (\mathbf{m} - \mathbf{m}^*)</span>. As <span class="math inline">\Sigma</span> is definite positive, this term is <span class="math inline">\geq 0</span>, and so it is minimized for <span class="math inline">\mathbf{m} = \mathbf{m}^*</span>. Next, we see that the derivative in <span class="math inline">\alpha_i</span> is given by (here and in the sequel, we see <span class="math inline">D(g^*, g_{\mathbf{m}, \Sigma})</span> as a function of <span class="math inline">\mathbf{v} = (\alpha_i)_i</span> and <span class="math inline">\mathbf{d} = (\mathbf{d}_i)_i</span>) <span class="math display"> \dfrac{\partial D}{\partial \alpha_i}(\mathbf{v}, \mathbf{d}) = \dfrac{1}{\alpha_i} - \frac{1}{\alpha_i^2} \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i = \frac{1}{\alpha_i^2} \left( \alpha_i - \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i \right). </span> Thus, for fixed <span class="math inline">\mathbf{d}</span>, <span class="math inline">D</span> is decreasing in <span class="math inline">\alpha_i</span> for <span class="math inline">\alpha_i &lt; \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i</span> and then increasing for <span class="math inline">\alpha_i &gt; \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i</span>, which shows that, for fixed <span class="math inline">\mathbf{d}</span>, it is minimized for <span class="math inline">\alpha_i = \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i</span>. For this value (and <span class="math inline">\mathbf{m} = \mathbf{m}^*</span>) we have <span id="eq-Dell"><span class="math display">
D(g^*, g_{\mathbf{m}^*, \Sigma}) = \sum_{i=1}^k \left[ \log(\mathbf{d}_i^\top \Sigma^* \mathbf{d}_i) + 1 - \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i \right] + C = -\sum_{i=1}^k \ell(\mathbf{d}_i^\top \Sigma^* \mathbf{d}_i) + C
\tag{3}</span></span> with <span class="math inline">C = - \frac{1}{2} \lVert \mathbf{m}^* \rVert^2 - \log E + \mathbb{E}_{g^*}(\log \phi(\mathbf{X}))</span> independent from the <span class="math inline">\mathbf{d}_i</span>’s. Since <span class="math inline">\ell</span> is decreasing and then increasing, it is clear from this expression that in order to minimize <span class="math inline">D</span>, one must choose the <span class="math inline">\mathbf{d}_i</span>’s in order to either maximize or minimize <span class="math inline">\mathbf{d}_i^\top \Sigma^* \mathbf{d}_i</span>, whichever maximizes <span class="math inline">\ell</span>. Since the variational characterization of eigenvalues shows that eigenvectors precisely solve this problem, we get the desired result.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>(of <a href="#thm-thm2">Theorem&nbsp;2</a>) In <a href="#eq-D">Equation&nbsp;2</a>, the <span class="math inline">\mathbf{m}^*</span> and the <span class="math inline">\Sigma^*</span> that appear in the right-hand side are the mean and variance of the density <span class="math inline">g^*</span> considered in the first argument of the Kullback–Leibler divergence. In particular, if we apply <a href="#eq-D">Equation&nbsp;2</a> with <span class="math inline">\phi \equiv 1</span>, we have <span class="math inline">g^* = f</span>, and the <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\Sigma^*</span> of the right-hand side become <span class="math inline">0</span> and <span class="math inline">I_n</span>, respectively, so that <span class="math display">
D(f, g_{\mathbf{m}, \Sigma}) =  \frac{1}{2} \sum_i \left( \log \alpha_i - \left(1 - \frac{1}{\alpha_i} \right) \right) + \frac{1}{2} \mathbf{m}^\top \Sigma^{-1} \mathbf{m}.
</span> Now, if we consider <span class="math inline">\mathbf{m} = \mathbf{m}^*</span> and <span class="math inline">\Sigma = I + (\alpha - 1) \mathbf{d} \mathbf{d}^\top</span>, we obtain (using <span class="math inline">\Sigma^{-1} = I - (1-1/\alpha) \mathbf{d} \mathbf{d}^\top</span> as mentioned in the proof of <a href="#lem-D">Lemma&nbsp;1</a>) <span class="math display">
D(f, g_{\mathbf{m}^*, \Sigma}) =  \frac{1}{2} \left( \log \alpha - \left(1 - \frac{1}{\alpha} \right) \left( 1 + (\mathbf{d}^\top \mathbf{m}^*)^2 \right) \right) + \frac{1}{2} \lVert \mathbf{m}^* \rVert^2.
</span> We have seen in the proof of <a href="#thm-thm1">Theorem&nbsp;1</a> that the function <span class="math inline">x \mapsto \log x + (1/x-1)\gamma</span> is minimized for <span class="math inline">x = \gamma</span> where it takes the value <span class="math inline">-\ell(\gamma)</span>: <span class="math inline">D(f, g_{\mathbf{m}^*, \Sigma})</span> is therefore minimized for <span class="math inline">\alpha = 1 + (\mathbf{d}^\top \mathbf{m}^*)^2</span> and for this value, we have <span class="math display">
D(f, g_{\mathbf{m}^*, \Sigma}) =  - \frac{1}{2} \ell(1 + (\mathbf{d}^\top \mathbf{m}^*)^2) + \frac{1}{2} \lVert \mathbf{m}^* \rVert^2.
</span> As <span class="math inline">\ell</span> is increasing in <span class="math inline">[1, \infty)</span>, this last quantity is minimized by maximizing <span class="math inline">(\mathbf{d}^\top \mathbf{m}^*)^2</span>, which is obtained for <span class="math inline">\mathbf{d} = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert</span>. The result is proved.</p>
</div>
</section>
<section id="sec-num-results-framework" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Framework for the numerical results</h1>
<section id="general-framework" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="general-framework"><span class="header-section-number">2.1</span> General framework</h2>
<p>The objective of the numerical simulations is to evaluate the impact of the choice of the covariance matrix on the estimation accuracy of a high dimensional integral <span class="math inline">E</span>. We compare in this section the estimation results for different choices of the auxiliary covariance matrix when the IS auxiliary density is Gaussian. To extend this comparison, we also compute the results when the IS auxiliary density is chosen with the von Mises–Fisher– Nakagami (vMFN) model recently proposed in <span class="citation" data-cites="PapaioannouEtAl_ImprovedCrossEntropybased_2019">(<a href="#ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" role="doc-biblioref">Papaioannou, Geyer, and Straub 2019</a>)</span> for high dimensional probability estimation.</p>
<p>In the following section we test these different models of auxiliary densities on five test cases, where <span class="math inline">f</span> is a standard Gaussian density. This choice is not a theoretical limitation as we can in principle always come back to this case by transforming the vector <span class="math inline">\mathbf{X}</span> with isoprobabilistic transformations (see for instance <span class="citation" data-cites="HohenbichlerRackwitz_NonNormalDependentVectors_1981">(<a href="#ref-HohenbichlerRackwitz_NonNormalDependentVectors_1981" role="doc-biblioref">Hohenbichler and Rackwitz 1981</a>)</span>, <span class="citation" data-cites="LiuDerKiureghian_MultivariateDistributionModels_1986">(<a href="#ref-LiuDerKiureghian_MultivariateDistributionModels_1986" role="doc-biblioref">Liu and Der Kiureghian 1986</a>)</span>).</p>
<p>The precise numerical framework that we will consider to assess the efficiency of the different auxiliary models is as follows. We assume first that <span class="math inline">M</span> i.i.d.&nbsp;random samples <span class="math inline">\mathbf{X}_1,\ldots,\mathbf{X}_M</span> distributed from <span class="math inline">g^*</span> are available from rejection sampling. From these samples, the parameters of the Gaussian and of the vMFN auxiliary density are computed to get an auxiliary density <span class="math inline">g'</span>. Finally, <span class="math inline">N</span> samples are generated from <span class="math inline">g'</span> to provide an estimation of <span class="math inline">E</span> with IS. This procedure is summarized by the following stages:</p>
<ol type="1">
<li>Generate a sample <span class="math inline">\mathbf{X}_1,\ldots,\mathbf{X}_M</span> independently according to <span class="math inline">g^*</span>;</li>
<li>From <span class="math inline">\mathbf{X}_1,\ldots,\mathbf{X}_M</span>, compute the parameters of the auxiliary parametric density <span class="math inline">g'</span>;</li>
<li>Generate a new sample <span class="math inline">\mathbf{X}_1,\ldots,\mathbf{X}_N</span> independently from <span class="math inline">g'</span>;</li>
<li>Estimate <span class="math inline">E</span> with <span class="math inline">\widehat{E}_N=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}</span>.</li>
</ol>
<p>The number of samples <span class="math inline">M</span> and <span class="math inline">N</span> are respectively set to <span class="math inline">M=500</span> and <span class="math inline">N=2000</span>. This procedure is then repeated <span class="math inline">100</span> times to provide a mean estimation <span class="math inline">\widehat{E}</span> of <span class="math inline">E</span>. In the result tables, for each auxiliary density <span class="math inline">g'</span> we report the corresponding value for the relative error <span class="math inline">\widehat{E}/ E-1</span> and the coefficient of variation of the <span class="math inline">100</span> iterations (the empirical standard deviation divided by <span class="math inline">E</span>). As was established in the proof of <a href="#thm-thm1">Theorem&nbsp;1</a>, the KL divergence is, up to an additive constant, equal to <span class="math inline">D'(\Sigma) = \log \lvert \Sigma \rvert + \textrm{tr}(\Sigma^* \Sigma^{-1})</span> which we will refer to as partial KL divergence. In the result tables, we also report thus the mean value of <span class="math inline">D'(\Sigma)</span> to analyse the relevancy of the auxiliary density <span class="math inline">g_{\widehat{\mathbf{m}}^*, \Sigma}</span> for these six choices of covariance matrix <span class="math inline">\Sigma</span>. The next sections specify the different parameters of <span class="math inline">g'</span> for the Gaussian model and for the vMFN model we have considered in the simulations.</p>
</section>
<section id="sec-def_cov" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-def_cov"><span class="header-section-number">2.2</span> Choice of the auxiliary density <span class="math inline">g'</span> for the Gaussian model</h2>
<p>The goal is to get benchmark results to assess whether one can improve estimations of Gaussian IS auxiliary density by projecting the covariance matrix <span class="math inline">\Sigma^*</span> in the proposed directions <span class="math inline">\mathbf{d}^*_i</span>. The algorithm that we study here (Algorithms 1+2) aims more precisely at understanding whether:</p>
<ul>
<li>projecting can improve the situation with respect to the empirical covariance matrix;</li>
<li>the <span class="math inline">\mathbf{d}^*_i</span>’s are good candidates, in particular compared to the choice <span class="math inline">\mathbf{m}^*</span> suggested in <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span>;</li>
<li>what is the impact in making errors in estimating the eigenpairs <span class="math inline">(\lambda^*_i, \mathbf{d}^*_i)</span>.</li>
</ul>
<p>Let us define the estimate <span class="math inline">\widehat{\mathbf{m}}^*</span> of <span class="math inline">\mathbf{m}^*</span> from the <span class="math inline">M</span> i.i.d. random samples <span class="math inline">\mathbf{X}_1,\ldots,\mathbf{X}_M</span> distributed from <span class="math inline">g^*</span> with <span id="eq-hatm"><span class="math display">
    \widehat{\mathbf{m}}^* = \frac{1}{M}\sum_{i=1}^M \mathbf{X}_i.
\tag{4}</span></span> In our numerical test cases, we will compare six different choices of Gaussian auxiliary distributions <span class="math inline">g'</span> with mean <span class="math inline">\widehat{\mathbf{m}}^*</span> and the following covariance matrices (see <a href="#tbl-sigma">Table&nbsp;1</a>):</p>
<ol type="1">
<li><p><span class="math inline">\Sigma^*</span>: the optimal covariance matrix given by <strong>?@eq-mstar</strong>;</p></li>
<li><p><span class="math inline">\widehat{\Sigma}^*</span>: the empirical estimation of <span class="math inline">\Sigma^*</span> given by <span id="eq-hatSigma"><span class="math display">
\widehat{\Sigma}^* = \frac{1}{M}\sum_{i=1}^M (\mathbf{X}_i-\widehat{\mathbf{m}}^*)(\mathbf{X}_i-\widehat{\mathbf{m}}^*)^\top.
\tag{5}</span></span></p></li>
</ol>
<p>The four other covariance matrices considered in the numerical simulations are of the form <span class="math inline">\sum_{i=1}^k (v_i-1) \mathbf{d}_i \mathbf{d}^\top_i + I_n</span> where <span class="math inline">v_i</span> is the variance of <span class="math inline">\widehat{\Sigma}^*</span> in the direction <span class="math inline">\mathbf{d}_i</span>, <span class="math inline">v_i = \mathbf{d}_i^\top \widehat{\Sigma}^* \mathbf{d}_i</span>. The considered choice of <span class="math inline">k</span> and <span class="math inline">\mathbf{d}_i</span> gives the following covariance matrices:</p>
<ol start="3" type="1">
<li><p><span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{opt}}</span> is obtained by choosing <span class="math inline">\mathbf{d}_i = \mathbf{d}^*_i</span> of <a href="#thm-thm1">Theorem&nbsp;1</a>, which is supposed to be perfectly known from <span class="math inline">\Sigma^*</span> and <span class="math inline">k</span> is computed with Algorithm 2;</p></li>
<li><p><span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{opt}}</span> is obtained by choosing <span class="math inline">\mathbf{d}_i = {\widehat{\mathbf{d}}}^*_i</span> the <span class="math inline">i</span>-th eigenvector of <span class="math inline">\widehat{\Sigma}^*</span> (in <span class="math inline">\ell</span>-order), which is an estimation of <span class="math inline">\mathbf{d}^*_i</span>, and <span class="math inline">k</span> is computed with Algorithm 2;</p></li>
<li><p><span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{mean}}</span> is obtained by choosing <span class="math inline">k = 1</span> and <span class="math inline">\mathbf{d}_1 = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert</span>;</p></li>
<li><p><span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{mean}}</span> is obtained by choosing <span class="math inline">k = 1</span> and <span class="math inline">\mathbf{d}_1 = {\widehat{\mathbf{m}}}^* / \lVert {\widehat{\mathbf{m}}}^* \rVert</span>, where <span class="math inline">\widehat{\mathbf{m}}^*</span> given by <a href="#eq-hatm">Equation&nbsp;4</a>.</p></li>
</ol>
<p>The matrices <span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{opt}}</span> and <span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{mean}}</span> use the estimation <span class="math inline">\widehat{\Sigma}^*</span> but the actual directions <span class="math inline">\mathbf{d}^*_i</span> or <span class="math inline">\mathbf{m}^*</span>, while the matrices <span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{opt}}</span> and <span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{mean}}</span> involve an additional estimation of the directions. By definition, <span class="math inline">\Sigma^*</span> will give optimal results, while results for <span class="math inline">\widehat{\Sigma}^*</span> will deteriorate as the dimension increases, which is the well-known behavior which we try to improve. Moreover, for <span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{mean}}</span> and <span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{opt}}</span>, the projection directions, if not known analytically, are obtained by a brute force Monte Carlo scheme with a very high simulation budget. Finally, we emphasize that Algorithm 1 corresponds to estimating and projecting on the <span class="math inline">\mathbf{d}^*_i</span>’s, and so the matrix <span class="math inline">\widehat{\Sigma}^*_k</span> of Algorithm 1 is equal to the matrix <span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{opt}}</span>, i.e., <span class="math inline">\widehat{\Sigma}^*_k = {\widehat{\Sigma}^{\text{+d}}_\text{opt}}</span>.</p>
<div id="tbl-sigma" class="anchored">
<table class="table">
<caption>Table&nbsp;1: Presentation of the six covariance matrices considered in the numerical examples. Except <span class="math inline">\Sigma^*</span>, the five other matrices involve one or two estimations: <span class="math inline">\widehat \Sigma^*</span> is the empirical estimation of <span class="math inline">\Sigma^*</span> given by <a href="#eq-hatSigma">Equation&nbsp;5</a>. The four others are obtained by projecting <span class="math inline">\widehat \Sigma^*</span> on: (i) the optimal directions <span class="math inline">\mathbf{d}^*_i</span> for <span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{opt}}</span>; (ii) estimations <span class="math inline">\widehat{\mathbf{d}}^*_i</span> of the optimal directions <span class="math inline">\mathbf{d}^*_i</span> for <span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{opt}}</span>; (iii) <span class="math inline">\mathbf{m}^*</span> for <span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{mean}}</span>; (iv) the estimation <span class="math inline">\widehat{\mathbf{m}}^*</span> in <strong>?@eq-mstar</strong> of <span class="math inline">\mathbf{m}^*</span> for <span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{mean}}</span>. The subscript therefore indicates the choice for the projection direction, while the superscript +d indicates whether these directions are estimated or not.</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\Sigma^*</span></th>
<th><span class="math inline">\widehat{\Sigma}^*</span></th>
<th><span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{opt}}</span></th>
<th><span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{mean}}</span></th>
<th><span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{opt}}</span></th>
<th><span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{mean}}</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Initial covariance matrix</td>
<td><span class="math inline">\Sigma^*</span></td>
<td><span class="math inline">\widehat \Sigma^*</span></td>
<td><span class="math inline">\widehat \Sigma^*</span></td>
<td><span class="math inline">\widehat \Sigma^*</span></td>
<td><span class="math inline">\widehat \Sigma^*</span></td>
<td><span class="math inline">\widehat \Sigma^*</span></td>
<td></td>
</tr>
<tr class="even">
<td>Projection directions (exact or estimated)</td>
<td>-</td>
<td>-</td>
<td>Exact</td>
<td>Exact</td>
<td>Estimated</td>
<td>Estimated</td>
<td></td>
</tr>
<tr class="odd">
<td>Choice for the projection direction</td>
<td>None</td>
<td>None</td>
<td>Opt</td>
<td>Mean</td>
<td>Opt</td>
<td>Mean</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="choice-of-the-auxiliary-density-g-for-the-von-misesfishernakagami-model" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="choice-of-the-auxiliary-density-g-for-the-von-misesfishernakagami-model"><span class="header-section-number">2.3</span> Choice of the auxiliary density <span class="math inline">g'</span> for the von Mises–Fisher–Nakagami model</h2>
<p>Von Mises–Fisher–Nakagami (vMFN) distributions were proposed in <span class="citation" data-cites="PapaioannouEtAl_ImprovedCrossEntropybased_2019">(<a href="#ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" role="doc-biblioref">Papaioannou, Geyer, and Straub 2019</a>)</span> as an alternative to the Gaussian parametric family to perform IS for high dimensional probability estimation. A random vector <span class="math inline">\mathbf{X}</span> drawn according to the vMFN distribution can be written as <span class="math inline">\mathbf{X}=R {\bf A}</span> where <span class="math inline">{\bf A}=\frac{\mathbf{X}}{\lVert\mathbf{X}\rVert}</span> is a unit random vector following the von Mises-Fisher distribution, and <span class="math inline">R=\lVert\mathbf{X}\rVert</span> is a positive random variable with a Nakagami distribution; further, <span class="math inline">R</span> and <span class="math inline">\bf A</span> are independent. The vMFN pdf can be written as <span id="eq-vMFN"><span class="math display">
g_\text{vMFN}({\bf x})= g_\text{N}(\lVert{\bf x}\rVert, p, \omega) \times g_\text{vMF} \left( \frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa \right).
\tag{6}</span></span> The density <span class="math inline">g_\text{N}(\lVert {\bf x}\rVert, p, \omega)</span> is the Nakagami distribution with shape parameter <span class="math inline">p \geq 0.5</span> and a spread parameter <span class="math inline">\omega&gt;0</span> defined by <span class="math display">
g_\text{N}(\lVert {\bf x}\rVert, p, \omega) = \frac{2 p^p}{\Gamma(p) \omega^p} \lVert {\bf x}\rVert^{2p-1} \exp\left( - \frac{p}{\omega}\lVert {\bf x}\rVert^2\right)
</span> and the density <span class="math inline">g_\text{vMF}(\frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa)</span> is the von Mises-Fisher distribution, given by <span class="math display">g_\text{vMF} \left( \frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa \right) = C_n(\kappa) \exp\left(\kappa {\boldsymbol{\mu}}^T \frac{{\bf x}}{\lVert{\bf x}\rvert\rvert} \right),
</span> where <span class="math inline">C_n(\kappa)</span> is a normalizing constant, <span class="math inline">\boldsymbol{\mu}</span> is a mean direction <span class="math inline">\boldsymbol{\mu}</span> (with <span class="math inline">\lvert\lvert\boldsymbol{\mu}\rvert\rvert=1</span>) and <span class="math inline">\kappa &gt; 0</span> is a concentration parameter <span class="math inline">\kappa&gt;0</span>.</p>
<p>Choosing a vMFN distribution therefore amounts to choosing the parameters <span class="math inline">p, \omega, {\boldsymbol{\mu}},</span> and <span class="math inline">\kappa</span>. There are therefore <span class="math inline">n+3</span> parameters to estimate, which is a significant reduction compared to the <span class="math inline">\frac{n(n+3)}{2}</span> required parameters of the Gaussian model with full covariance matrix.</p>
<p>Following <span class="citation" data-cites="PapaioannouEtAl_ImprovedCrossEntropybased_2019">(<a href="#ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" role="doc-biblioref">Papaioannou, Geyer, and Straub 2019</a>)</span>, given a sample <span class="math inline">\mathbf{X}_1,\ldots,\mathbf{X}_M</span> distributed from <span class="math inline">g^*</span>, the parameters <span class="math inline">\omega</span>, <span class="math inline">p</span>, <span class="math inline">\boldsymbol{\mu}</span> and <span class="math inline">\kappa</span> are set in the following way in order to define <span class="math inline">g'</span>: <span class="math display"> \widehat{\omega}=\frac{1}{M}\sum_{i=1}^M \lVert\mathbf{X}_i\rVert^2 \ \text{ and } \ \widehat{p}=\frac{\widehat{\omega}^2}{\widehat{\tau}-\widehat{\omega}^2} \text{ with } \widehat{\tau}=\frac{1}{M}\sum_{i=1}^M \lVert\mathbf{X}_i\rVert^4
</span> and <span class="math display"> \widehat{\boldsymbol{\mu}}=\frac{\sum_{i=1}^M \frac{\mathbf{X}_i}{\lvert\lvert\mathbf{X}_i\rvert\rvert}}{\lvert\lvert\sum_{i=1}^M \frac{\mathbf{X}_i}{\lvert\lvert\mathbf{X}_i\rvert\rvert} \rvert\rvert} \ \text{ and } \  \widehat{\kappa}=\dfrac{n\widehat{\chi}-\widehat{\chi}^3}{1-\widehat{\chi}^2} \text{ with } \widehat{\chi} = \min \left( \left \lVert \frac{1}{M}\sum_{i=1}^M \frac{\mathbf{X}_i}{\lVert \mathbf{X}_i \rVert} \right \rVert, 0.95 \right).
</span></p>
</section>
</section>
<section id="sec-test-cases" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Numerical results on five test cases</h1>
<p>The proposed numerical framework is applied on three examples that are often considered to assess the performance of importance sampling algorithms and also two test cases from the area of financial mathematics.</p>
<section id="sec-sub:sum" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-sub:sum"><span class="header-section-number">3.1</span> Test case 1: one-dimensional optimal projection</h2>
<p>We consider a test case where all computations can be made exactly. This is a classical example of rare event probability estimation, often used to test the robustness of a method in high dimension. It is given by <span class="math inline">\phi(\mathbf{x})=\mathbb{I}_{\{\varphi(\mathbf{x})\geq 0\}}</span> with <span class="math inline">\varphi</span> the following affine function: <span id="eq-sum"><span class="math display">
    \varphi: \mathbf{x}=(x_1,\ldots,x_n)\in\mathbb{R}^n \mapsto\underset{j=1}{\overset{n}{\sum}} x_j-3\sqrt{n}.
\tag{7}</span></span> The quantity of interest <span class="math inline">E</span> is defined as <span class="math inline">E=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)\simeq 1.35\cdot 10^{-3}</span> for all <span class="math inline">n</span> where the density <span class="math inline">f</span> is the standard <span class="math inline">n</span>-dimensional Gaussian distribution. Here, the zero-variance density is <span class="math inline">g^*(\mathbf{x})=\dfrac{f(\mathbf{x})\mathbb{I}_{\{\varphi(\mathbf{x})\geq 0\}}}{E}</span>, and the optimal parameters <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\Sigma^*</span> in <strong>?@eq-mstar</strong> can be computed exactly, namely <span class="math inline">\mathbf{m}^* = \alpha \textbf{1}</span> with <span class="math inline">\alpha = e^{-9/2}/(E(2\pi)^{1/2})</span> and <span class="math inline">\textbf{1} = \frac{1}{\sqrt n} (1,\ldots,1) \in \mathbb{R}^n</span> the normalized constant vector, and <span class="math inline">\Sigma^* =(v-1) \mathbf{1} \mathbf{1}^\top + I_n</span> with <span class="math inline">v=3\alpha-\alpha^2+1</span>.</p>
<section id="evolution-of-the-partial-kl-divergence-and-spectrum" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="evolution-of-the-partial-kl-divergence-and-spectrum"><span class="header-section-number">3.1.1</span> Evolution of the partial KL divergence and spectrum</h3>
<p><strong>?@fig-eigsum-1</strong> represents the evolution as the dimension varies between <span class="math inline">5</span> and <span class="math inline">100</span> of the partial KL divergence <span class="math inline">D'</span> for three different choices of covariance matrix: the optimal matrix <span class="math inline">\Sigma^*</span>, its empirical estimation <span class="math inline">\widehat{\Sigma}^*</span> and the estimation <span class="math inline">\widehat{\Sigma}^*_k</span> of the optimal lower-dimensional covariance matrix. We can notice that the partial KL divergence for <span class="math inline">\widehat{\Sigma}^*</span> grows much faster than the other two, and that the partial KL divergence for <span class="math inline">\widehat{\Sigma}^*_k</span> remains very close to the optimal value <span class="math inline">D'(\Sigma^*)</span>. As the KL divergence is a proxy for the efficiency of the auxiliary density (it is for instance closely related to the number of samples required for a given precision <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span>), this suggests that using <span class="math inline">\widehat{\Sigma}^*_k</span> will provide results close to optimal.</p>
<p>We now check this claim. As <span class="math inline">\Sigma^* = (v-1) \textbf{1} \textbf{1}^\top + I_n</span>, its eigenpairs are <span class="math inline">(v, \textbf{1})</span> and <span class="math inline">(1,\mathbf{d}_i)</span> where the <span class="math inline">\mathbf{d}_i</span>’s form an orthonormal basis of the space orthogonal to the space spanned by <span class="math inline">\textbf{1}</span>. In particular, <span class="math inline">(v, \textbf{1})</span> is the largest (in <span class="math inline">\ell</span>-order) eigenpair of <span class="math inline">\Sigma^*</span> and <span class="math inline">\Sigma^*_k = \Sigma^*</span> for any <span class="math inline">k \geq 1</span>.</p>
<p>In practice, we do not use this theoretical knowledge and <span class="math inline">\Sigma^*</span>, <span class="math inline">\Sigma^*_k</span> and the eigenpairs are estimated. The six covariance matrices introduced in <a href="#sec-def_cov">Section&nbsp;2.2</a> and in which we are interested are as follows:</p>
<ul>
<li><span class="math inline">\Sigma^* = (v-1) \textbf{1} \textbf{1}^\top + I_n</span>;</li>
<li><span class="math inline">\widehat{\Sigma}^*</span> given by <a href="#eq-hatSigma">Equation&nbsp;5</a>;</li>
<li><span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{opt}}</span> and <span class="math inline">{\widehat{\Sigma}^{\text{}}_\text{mean}}</span> are equal and given by <span class="math inline">(\widehat \lambda-1) \textbf{1} \textbf{1}^\top + I_n</span> with <span class="math inline">\widehat{\lambda} = \textbf{1}^\top \widehat{\Sigma}^* \textbf{1}</span>. This amounts to assuming that the projection direction <span class="math inline">\textbf{1}</span> is perfectly known, whereas the variance in this direction is estimated;</li>
<li><span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{opt}} = (\widehat{\lambda} - 1) \widehat{\mathbf{d}} {\widehat{\mathbf{d}}}^\top + I_n</span> with <span class="math inline">(\widehat{\lambda}, \widehat{\mathbf{d}})</span> the smallest eigenpair of <span class="math inline">\widehat{\Sigma}^*</span>. The difference with the previous case is that we do not assume anymore that the optimal projection direction <span class="math inline">\textbf{1}</span> is known, and so it needs to be estimated;</li>
<li><span class="math inline">{\widehat{\Sigma}^{\text{+d}}_\text{mean}} = (\widehat{\lambda} - 1) \frac{\widehat{\mathbf{m}}^* {(\widehat{\mathbf{m}}^*)}^\top}{\lVert \widehat{\mathbf{m}}^* \rVert^2} + I_n</span> with <span class="math inline">\widehat{\mathbf{m}}^*</span> given by <a href="#eq-hatm">Equation&nbsp;4</a> and <span class="math inline">\widehat{\lambda} = \frac{{(\widehat{\mathbf{m}}^*)}^\top \widehat{\Sigma}^* \widehat{\mathbf{m}}^*}{\lVert \widehat{\mathbf{m}}^* \rVert^2}</span>. Here we assume that <span class="math inline">\mathbf{m}^*</span> is a good projection direction, but is unknown and therefore needs to be estimated.</li>
</ul>
<p>Note that in the particularly simple case considered here, both <span class="math inline">\widehat{\mathbf{m}}^* / \lVert \widehat{\mathbf{m}}^* \rVert</span> and <span class="math inline">\widehat{\mathbf{d}}</span> are estimators of <span class="math inline">\textbf{1}</span> but they are obtained by different methods. In the next example we will consider a case where <span class="math inline">\mathbf{m}^*</span> is not an optimal projection direction as given by <a href="#thm-thm1">Theorem&nbsp;1</a>.</p>
<p><strong>?@fig-eigsum-2</strong> represents the images by <span class="math inline">\ell</span> of the eigenvalues of <span class="math inline">\Sigma^*</span> and <span class="math inline">\widehat{\Sigma}^*</span>. This picture carries a very important insight. We notice that the estimation of most eigenvalues is poor: indeed, all the blue crosses except the leftmost one are meant to be estimator of <span class="math inline">1</span>, whereas we see that they are more or less uniformly spread between <span class="math inline">0.2</span> and <span class="math inline">2.5</span>. This means that the variance terms in the corresponding directions are poorly estimated, which could be the explanation on why the use of <span class="math inline">\widehat{\Sigma}^*</span> gives an inaccurate estimation. But what we remark also is that the function <span class="math inline">\ell</span> is quite flat around one: as a consequence, although the eigenvalues offer significant variability, this variability is smoothed by the action of <span class="math inline">\ell</span>. Indeed, the images of the eigenvalues by <span class="math inline">\ell</span> take values between <span class="math inline">0</span> and <span class="math inline">0.8</span> and have smaller variability. Moreover, <span class="math inline">\ell(x)</span> increases sharply as <span class="math inline">x</span> approaches <span class="math inline">0</span> and thus efficiently distinguishes between the two leftmost estimated eigenvalues and is able to separate them.</p>
<!-- -->

</section>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">Bibliography</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-ChanKroese_ImprovedCrossentropyMethod_2012" class="csl-entry" role="doc-biblioentry">
Chan, Joshua C. C., and Dirk P. Kroese. 2012. <span>“Improved Cross-Entropy Method for Estimation.”</span> <em>Statistics and Computing</em> 22 (5): 1031–40. <a href="https://doi.org/10.1007/s11222-011-9275-7">https://doi.org/10.1007/s11222-011-9275-7</a>.
</div>
<div id="ref-Chatterjee18:0" class="csl-entry" role="doc-biblioentry">
Chatterjee, Sourav, and Persi Diaconis. 2018. <span>“The Sample Size Required in Importance Sampling.”</span> <em>Ann. Appl. Probab.</em> 28 (2): 1099–1135. <a href="https://doi.org/10.1214/17-AAP1326">https://doi.org/10.1214/17-AAP1326</a>.
</div>
<div id="ref-MasriEtAl_ImprovementCrossentropyMethod_2020" class="csl-entry" role="doc-biblioentry">
El Masri, Maxime, Jérôme Morio, and Florian Simatos. 2021. <span>“Improvement of the Cross-Entropy Method in High Dimension for Failure Probability Estimation Through a One-Dimensional Projection Without Gradient Estimation.”</span> <em>Reliability Engineering &amp; System Safety</em> 216: 107991. <a href="https://doi.org/10.1016/j.ress.2021.107991">https://doi.org/10.1016/j.ress.2021.107991</a>.
</div>
<div id="ref-GraceEtAl_AutomatedStateDependentImportance_2014" class="csl-entry" role="doc-biblioentry">
Grace, Adam W., Dirk P. Kroese, and Werner Sandmann. 2014. <span>“Automated <span>State</span>-<span>Dependent Importance Sampling</span> for <span>Markov Jump Processes</span> via <span>Sampling</span> from the <span>Zero</span>-<span>Variance Distribution</span>.”</span> <em>Journal of Applied Probability</em> 51 (3): 741–55. <a href="https://doi.org/10.1239/jap/1409932671">https://doi.org/10.1239/jap/1409932671</a>.
</div>
<div id="ref-HohenbichlerRackwitz_NonNormalDependentVectors_1981" class="csl-entry" role="doc-biblioentry">
Hohenbichler, Michael, and Rüdiger Rackwitz. 1981. <span>“Non-<span>Normal Dependent Vectors</span> in <span>Structural Safety</span>.”</span> <em>Journal of the Engineering Mechanics Division</em> 107 (6): 1227–38. <a href="https://doi.org/10.1061/JMCEA3.0002777">https://doi.org/10.1061/JMCEA3.0002777</a>.
</div>
<div id="ref-LiuDerKiureghian_MultivariateDistributionModels_1986" class="csl-entry" role="doc-biblioentry">
Liu, Pei-Ling, and Armen Der Kiureghian. 1986. <span>“Multivariate Distribution Models with Prescribed Marginals and Covariances.”</span> <em>Probabilistic Engineering Mechanics</em> 1 (2): 105–12. <a href="https://doi.org/10.1016/0266-8920(86)90033-0">https://doi.org/10.1016/0266-8920(86)90033-0</a>.
</div>
<div id="ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" class="csl-entry" role="doc-biblioentry">
Papaioannou, Iason, Sebastian Geyer, and Daniel Straub. 2019. <span>“Improved Cross Entropy-Based Importance Sampling with a Flexible Mixture Model.”</span> <em>Reliability Engineering &amp; System Safety</em> 191 (November): 106564. <a href="https://doi.org/10.1016/j.ress.2019.106564">https://doi.org/10.1016/j.ress.2019.106564</a>.
</div>
</div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
    var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Optimal projection for parametric importance sampling in high dimension</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Maxime El Masri</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: '[ONERA/DTIS](https://www.onera.fr/), [ISAE-SUPAERO](https://www.isae-supaero.fr/), [Université de Toulouse](https://www.univ-toulouse.fr/)'</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-9127-4503</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Jérôme Morio</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">    url: 'https://www.onera.fr/en/staff/jerome-morio?destination=node/981'</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: '[ONERA/DTIS](https://www.onera.fr/), [Université de Toulouse](https://www.univ-toulouse.fr/)'</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-8811-8956</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Florian Simatos</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">    url: 'https://pagespro.isae-supaero.fr/florian-simatos/'</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: '[ISAE-SUPAERO](https://www.isae-supaero.fr/), [Université de Toulouse](https://www.univ-toulouse.fr/)'</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> last-modified</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> |</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">  This document provides a dimension-reduction strategy in order to improve the performance of importance sampling in high dimension.</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> |</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">  In this paper we propose a dimension-reduction strategy in order to improve the performance of importance sampling in high dimension. The idea is to estimate variance terms in a small number of suitably chosen directions. We first prove that the optimal directions, i.e., the ones that minimize the Kullback--Leibler divergence with the optimal auxiliary density, are the eigenvectors associated to extreme (small or large) eigenvalues of the optimal covariance matrix. We then perform extensive numerical experiments that show that as dimension increases, these directions give estimations which are very close to optimal. Moreover, we show that the estimation remains accurate even when a simple empirical estimator of the covariance matrix is used to estimate these directions. These theoretical and numerical results open the way for different generalizations, in particular the incorporation of such ideas in adaptive importance sampling schemes.</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co">  - Importance sampling</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co">  - High dimension</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co">  - Gaussian covariance matrix</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co">  - Kullback-Leibler divergence</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co">  - Projection</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="an">github-user:</span><span class="co"> jmorio44</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="an">repo:</span><span class="co"> optimal-projection-IS</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> true</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="an">published:</span><span class="co"> false</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-html: default</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-pdf: default</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co">  keep-ipynb: true</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="co">  jupytext:</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="co">    text_representation:</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co">      extension: .qmd</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="co">      format_name: quarto</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="co">      format_version: '1.0'</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="co">      jupytext_version: 1.14.2</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="co">  kernelspec:</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="co">    display_name: Python 3 (ipykernel)</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="co">    language: python</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="co">    name: python3</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-l</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Plot of the function $\ell=-\log(x) + x - 1$ given by @eq-l</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 1. Plot of the function "l"</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CEIS_vMFNM <span class="im">import</span> <span class="op">*</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, Math, Latex</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tabulate <span class="im">import</span> tabulate</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(np.finfo(<span class="bu">float</span>).eps,<span class="fl">4.0</span>,<span class="dv">100</span>)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="op">-</span>np.log(x) <span class="op">+</span> x <span class="op">-</span><span class="dv">1</span></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>ax.plot(x, y, linewidth<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">4</span>), xticks<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>       ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), yticks<span class="op">=</span>[<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="fl">1.5</span>])</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$x$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(x)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>::: {#thm-thm1}</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>Let $(\lambda^*_i, \mathbf{d}^*_i)$ be the eigenpairs of $\Sigma^*$ ranked in decreasing $\ell$-order. Then for $1 \leq k \leq n$, the solution $(\mathbf{m}^*_k, \Sigma^*_k)$ to @eq-argminDkl-k is given by</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>\mathbf{m}^*_k = \mathbf{m}^* \ \text{ and } \ \Sigma^*_k = I_n + \sum_{i=1}^k \left( \lambda^*_i - 1 \right) \mathbf{d}^*_i (\mathbf{d}^*_i)^\top. </span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Sigma*k}</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>For $k = 1$ for instance, the shape of the function $\ell$ depicted in @fig-l implies that $\Sigma^*_1 = I_n + (\lambda^*-1) \mathbf{d}^* (\mathbf{d}^*)^\top$ with $(\lambda^*, \mathbf{d}^*)$ the eigenpair of $\Sigma^*$ with $\lambda^*$ either the largest or the smallest eigenvalue of $\Sigma^*$, depending on which one maximizes $\ell$.</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>This theoretical result therefore suggests to reduce dimension by estimating eigenpairs of $\Sigma^*$, rank them in decreasing $\ell$-order and then use the $k$ first eigenpairs $(({\widehat{\lambda}}^*_i, {\widehat{\mathbf{d}}}^*_i), i = 1, \ldots, k)$ to build the covariance matrix $\widehat \Sigma^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n$ and the corresponding auxiliary density. This scheme is summarized in Algorithm 1. The effective dimension $k$ is obtained by Algorithm 2, see @sec-choicek below.</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a><span class="in">```{.pseudocode}</span></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{Algorithm suggested by Theorem 1.}</span></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Data}: Sample sizes $N$ and $M$</span></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Result}: Estimation $\widehat{E}_N$ of integral $E$</span></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Generate a sample $\mathbf{X}_1,\ldots,\mathbf{X}_M$  on $\mathbb{R}^n$ independently according to $g^*$</span></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a><span class="in">\State  - Estimate $\widehat{\mathbf{m}}^*$ and $\widehat{\Sigma}^*$ defined in Equation 10 and Equation 11 with this sample</span></span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Compute the eigenpairs $(\widehat{\lambda}^*_i, \widehat{\mathbf{d}}^*_i)$ of $\widehat{\Sigma}^*$ ranked in decreasing $\ell$-order</span></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Compute the matrix $\widehat{\Sigma}^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n$ with $k$ obtained by applying Algorithm 2 with input $({\widehat{\lambda}}^*_1, \ldots, {\widehat{\lambda}}^*_n)$</span></span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Generate a new sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ independently from $g' = g_{\widehat{\mathbf{m}}^*,\widehat{\Sigma}^*_k}$</span></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Return $\displaystyle \widehat{E}_N=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}$</span></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a> The value $1$ plays a particular role in @thm-thm1, in that, as $\ell$ is minimized in $1$, eigenvectors with eigenvalues $1$ will only be selected once all other eigenvalues will have been picked: in other words, if $\lambda^*_i = 1$ then $\lambda^*_j = 1$ for all $j \geq i$. The reason why $1$ plays this special role is due to the form of the covariance matrix that we impose. More precisely, looking for covariance matrices in the set $\mathcal{L}_{n,k}$ amounts to looking for covariance matrices which, once diagonalized, have one's on the diagonal except possibly for $k$ values (the $\alpha_i$'s). As $k$ will be small, typically $k = 1$ or $2$, this amounts to looking for covariance matrices which are perturbation of the identity. This is particularly relevant as we assume $f$ is a standard Gaussian density. What @thm-thm1 tells is that, when trying to approximate $\Sigma^*$ by such matrices, we should first consider eigenvectors with eigenvalues as different as possible from $1$, the "distance" to $1$ being measured by $\ell$. If one was imposing a different form on $\Sigma^*_k$ (which can be interesting if the distribution $f$ is not standardized), then a different result would arise. For instance, if one was looking for matrices where the "default" choice would be some $\lambda &gt; 0$ for the diagonal entries that are not estimated, i.e., a matrix of the form $\sum_i (\alpha_i-\lambda) \mathbf{d}_i \mathbf{d}_i^\top + \lambda I_n$, then eigenpairs would be ranked according to the function $\ell(\cdot/\lambda)$, meaning that one would look for eigenvectors associated to eigenvalues as different as possible from $\lambda$.</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>As mentioned above, we assume in the first step of Algorithm 1 that we can sample according to $g^*$. Since $g^*$ is known up to a multiplicative constant, this is a reasonable assumption as classical techniques such as importance sampling with self-normalized weights or Markov Chain Monte--Carlo (MCMC) can be applied in this case (see for instance [@ChanKroese_ImprovedCrossentropyMethod_2012], [@GraceEtAl_AutomatedStateDependentImportance_2014]). In this paper, we choose to apply a basic rejection method that yields perfect independent samples from $g^*$, possibly at the price of a high computational cost. As the primary goal of this paper is to understand whether the $\mathbf{d}^*_i$'s are indeed good projection directions, this computational cost to generate from $g^*$ is not relevant for us and therefore not taken into account. Possible improvements to relax this assumption are discussed in the conclusion of the paper.</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choice of the number of dimensions $k$ {#sec-choicek} </span></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>The choice of the effective dimension $k$, i.e., the number of projection directions considered, is important. If it is close to $n$, then the matrix $\widehat{\Sigma}^*_k$ will be close to $\widehat{\Sigma}^*$ which is the situation we want to avoid in the first place. On the other hand, setting $k=1$ in all cases may be too simple and lead to suboptimal results. In practice however, this is often a good choice. In order to adapt $k$ dynamically, we consider a simple method based on the value of the KL divergence. Given the eigenvalues $\lambda_1, \ldots, \lambda_n$ ranked in decreasing $\ell$-order, we look for the maximal gap in the sequence $(\ell(\lambda_1), \ldots, \ell(\lambda_n))$. This allows to choose $k$ such that $\sum_{i=1}^k \ell(\lambda_i)$ is close to $\sum_{i=1}^n \ell(\lambda_i)$ which is equal, up to an additive constant, to the minimal KL divergence (see @eq-Dell below). The precise method is described in Algorithm 2. </span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a><span class="in">```{.pseudocode}</span></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{Choice of the number of dimensions}</span></span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Data}: Sequence of positive numbers $\lambda_1, \ldots, \lambda_n$ in decreasing $\ell$-order</span></span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Result}: Number of selected dimensions $k$</span></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Compute the increments $\delta_i = \ell(\lambda_{i+1}) - \ell(\lambda_i)$ for $i=1\ldots n-1$</span></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Return $k=\arg\max \delta_i$, the index of the maximum of the differences.</span></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theoretical result concerning the projection on $\mathbf{m}^*$ {#sec-mm} </span></span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span>, the authors propose to project on the mean $\mathbf{m}^*$ of the optimal auxiliary density $g^*$. Numerically, this algorithm is shown to perform well, but only a very heuristic explanation based on the light tail of the Gaussian distribution is provided to motivate this choice. It turns out that the techniques used in the proof of @thm-thm1 can shed light on why projecting on $\mathbf{m}^*$ may indeed be a good idea. Let us first state our theoretical result, and then explain why it justifies the idea of projecting on $\mathbf{m}^*$.</span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>::: {#thm-thm2}</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a>Consider $\Sigma \in \mathcal{L}_{n,1}$ of the form $\Sigma = I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top$ with $\alpha &gt; 0$ and $\lVert \mathbf{d} \rVert = 1$. Then the minimizer in $(\alpha, \mathbf{d})$ of the KL divergence between $f$ and $g_{\mathbf{m}^*, \Sigma}$ is $(1+\lVert m^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert)$:</span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a>        $$\left( 1+\lVert \mathbf{m}^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert \right) = \arg \min_{\alpha, \mathbf{d}} \left\{ D(f, g_{\mathbf{m}^*, I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top}): \alpha &gt; 0, \ \lVert \mathbf{d} \rVert = 1 \right<span class="sc">\}</span>. $$</span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a>In other words, $\mathbf{m}^*$ appears as an optimal projection direction when one seeks to minimize the KL divergence between $f$ and the Gaussian density with mean $\mathbf{m}^*$ and covariance of the form $I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top$. Let us now explain why this minimization problem is indeed relevant, and why choosing an auxiliary density which minimizes this KL divergence may indeed lead to an accurate estimation. The justification deeply relies on the recent results by <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span>.</span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>As mentioned above, in a reliability context where one seeks to estimate a small probability $p = \mathbb{P}(\mathbf{X} \in A),$ Theorem $1.3$ in <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span> shows that $D(g^*, g)$ governs the sample size required for an accurate estimation of $p$: more precisely, the estimation is accurate if the sample size is larger than $e^{D(g^*, g)}$, and inaccurate otherwise. This motivates the rationale for minimizing the KL divergence with $g^*$.</span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a>However, in high dimension, importance sampling is known to fail because of the weight degeneracy problem whereby $\max_i L_i / \sum_i L_i \approx 1$, with the $L_i$'s the unnormalized importance weights, or likelihood ratios: $L_i = f(\mathbf{X}_i) / g(\mathbf{X}_i)$ with the $\mathbf{X}_i$'s i.i.d. drawn according to $g$. Theorem $2.3$ in <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span> shows that the weight degeneracy problem is avoided if the empirical mean of the likelihood ratios is close to $1$, and for this, Theorem $1.1$ in <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span> shows that the sample size should be larger than $e^{D(f, g)}$. In other words, these results suggest that the KL divergence with $g^*$ governs the sample size for an accurate estimation of $p$, while the KL divergence with $f$ governs the weight degeneracy problem.</span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>In light of these results, it becomes natural to consider the KL divergence with $f$ and not only $g^*$. Of course, minimizing $D(f, g_{\mathbf{m}, \Sigma})$ without constraints on $\mathbf{m}$ and $\Sigma$ is trivial since $g_{\mathbf{m}, \Sigma} = f$ for $\mathbf{m} = 0$ and $\Sigma = I_n$. However, these choices are the ones we want to avoid in the first place, and so it makes sense to impose some constraints on $\mathbf{m}$ and $\Sigma$. If one keeps in mind the other objective of getting close to $g^*$, then the choice $\mathbf{m} = \mathbf{m}^*$ becomes very natural, and we are led, when $\Sigma \in \mathcal{L}_{n,1}$ is sought as a rank-$1$ perturbation of the identity, to considering the optimization problem of @thm-thm2.</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a><span class="fu"># Proof of Theorems @thm-thm1 and @thm-thm2 {#sec-proof} </span></span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>We begin with a preliminary lemma.</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a>::: {#lem-D}</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>Let $f$ be the density of the standard Gaussian vector in dimension $n$, $\phi: \mathbb{R}^n \to \mathbb{R}_+$ and $g_* = f \phi / E$ with $E = \int f \phi$. Then for any $\mathbf{m}$ and any $\Sigma$ of the form $\Sigma = I_n + \sum_i (\alpha_i - 1) \mathbf{d}_i \mathbf{d}_i^\top$ with $\alpha_i &gt; 0$ and the $\mathbf{d}_i$'s orthonormal, we have</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a> D(g^*, g_{\mathbf{m}, \Sigma}) =&amp;  \frac{1}{2} \sum_i \left( \log \alpha_i - \left(1 - \frac{1}{\alpha_i} \right) \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i \right) + \frac{1}{2} (\mathbf{m} - \mathbf{m}^*)^\top \Sigma^{-1} (\mathbf{m} - \mathbf{m}^*)<span class="sc">\\</span></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a> &amp;- \frac{1}{2} \lVert \mathbf{m}^* \rVert^2 - \log E + \mathbb{E}_{g^*}(\log \phi(\mathbf{X})).</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>$$ {#eq-D}</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>(of @lem-D)</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>For any $\mathbf{m} \in \mathbb{R}^n$ and $\Sigma \in \mathcal{S}^+_n$, we have by definition</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>$$ D(g^*, g_{\mathbf{m}, \Sigma}) = \mathbb{E}_{g^*} \left( \log \left( \frac{g^*(\mathbf{X})}{g_{\mathbf{m}, \Sigma}(\mathbf{X})} \right) \right) = \mathbb{E}_{g^*} \left( \log \left( \frac{\frac{\phi(\mathbf{X}) e^{-\frac{1}{2} \lVert \mathbf{X} \rVert^2}}{E(2\pi)^{d/2}}}{ \frac{e^{-\frac{1}{2} (\mathbf{X} - \mathbf{m})^\top \Sigma^{-1} (\mathbf{X} - \mathbf{m})}}{(2\pi)^{d/2} \lvert \Sigma \rvert^{1/2}} } \right) \right) $$</span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>and so</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>D(g^*, g_{\mathbf{m}, \Sigma}) = - \frac{1}{2} \mathbb{E}_{g^*}(\lVert \mathbf{X} \rVert^2) + \frac{1}{2} \mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m})^\top \Sigma^{-1} (\mathbf{X} - \mathbf{m}) \right) + \frac{1}{2} \log \lvert \Sigma \rvert<span class="sc">\\</span></span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\log E + \mathbb{E}_{g^*}(\log \phi(\mathbf{X})).</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>Because $\mathbb{E}_{g^*}(\mathbf{X}) = \mathbf{m}^*$, we have $\mathbb{E}_{g^*}(\lVert \mathbf{X} \rVert^2) = \mathbb{E}_{g^*}(\lVert \mathbf{X} - \mathbf{m}^* \rVert^2) + \lVert \mathbf{m}^* \rVert^2$ and</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>\mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m})^\top \Sigma^{-1} (\mathbf{X} - \mathbf{m}) \right) = \mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m}^*)^\top \Sigma^{-1} (\mathbf{X} - \mathbf{m}^*) \right)<span class="sc">\\</span></span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>(\mathbf{m} - \mathbf{m}^*)^\top \Sigma^{-1} (\mathbf{m} - \mathbf{m}^*).</span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>In the following derivations, we use the linearity of the trace and of the expectation, which make these two operators commute, as well as the identity $a^\top b = \textrm{tr}(a b^\top)$ for any two vectors $a$ and $b$. With this caveat, we obtain</span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>\mathbb{E}_{g^*}\left[ \lVert \mathbf{X} - \mathbf{m}^* \rVert^2 \right] = \mathbb{E}_{g^*} \left[ \textrm{tr}((\mathbf{X} - \mathbf{m}^*) (\mathbf{X} - \mathbf{m}^*)^\top) \right] = \textrm{tr} (\Sigma^*) </span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>and we obtain with similar arguments $\mathbb{E}_{g^*}( (\mathbf{X} - \mathbf{m}^*)^\top \Sigma^{-1} (\mathbf{X} - \mathbf{m}^*) ) = \textrm{tr} ( \Sigma^{-1} \Sigma^*)$. Consider now $\Sigma = I_n + \sum_i (\alpha_i - 1) \mathbf{d}_i \mathbf{d}_i^\top$ with $\alpha_i &gt; 0$ and the $\mathbf{d}_i$'s orthonormal. Then the eigenvalues of $\Sigma$ potentially different from $1$ are the $\alpha_i$'s ($\alpha_i$ is the eigenvalue associated with $\mathbf{d}_i$), so that</span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a>$$\log \lvert \Sigma \rvert = \sum_i \log \alpha_i. </span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>Moreover, we have $\Sigma^{-1} = I_n - \sum_i \beta_i \mathbf{d}_i \mathbf{d}_i^\top$ with $\beta_i = 1 - 1/\alpha_i$ and so</span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>\textrm{tr}(\Sigma^{-1} \Sigma^*) = \textrm{tr}(\Sigma^*) - \sum_i \beta_i \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i. </span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>Gathering the previous relation, we finally obtain the desired result.</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>(of @thm-thm1)</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>From @eq-D we see that the only dependency of $D(g^*, g_{\mathbf{m}, \Sigma})$ in $\mathbf{m}$ is in the quadratic term $(\mathbf{m} - \mathbf{m}^*)^\top \Sigma^{-1} (\mathbf{m} - \mathbf{m}^*)$. As $\Sigma$ is definite positive, this term is $\geq 0$, and so it is minimized for $\mathbf{m} = \mathbf{m}^*$. Next, we see that the derivative in $\alpha_i$ is given by (here and in the sequel, we see $D(g^*, g_{\mathbf{m}, \Sigma})$ as a function of $\mathbf{v} = (\alpha_i)_i$ and $\mathbf{d} = (\mathbf{d}_i)_i$)</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>    $$ \dfrac{\partial D}{\partial \alpha_i}(\mathbf{v}, \mathbf{d}) = \dfrac{1}{\alpha_i} - \frac{1}{\alpha_i^2} \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i = \frac{1}{\alpha_i^2} \left( \alpha_i - \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i \right). $$</span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>Thus, for fixed $\mathbf{d}$, $D$ is decreasing in $\alpha_i$ for $\alpha_i &lt; \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i$ and then increasing for $\alpha_i &gt; \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i$, which shows that, for fixed $\mathbf{d}$, it is minimized for $\alpha_i = \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i$. For this value (and $\mathbf{m} = \mathbf{m}^*$) we have</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>D(g^*, g_{\mathbf{m}^*, \Sigma}) = \sum_{i=1}^k \left[ \log(\mathbf{d}_i^\top \Sigma^* \mathbf{d}_i) + 1 - \mathbf{d}_i^\top \Sigma^* \mathbf{d}_i \right] + C = -\sum_{i=1}^k \ell(\mathbf{d}_i^\top \Sigma^* \mathbf{d}_i) + C</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Dell}</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>with $C = - \frac{1}{2} \lVert \mathbf{m}^* \rVert^2 - \log E + \mathbb{E}_{g^*}(\log \phi(\mathbf{X}))$ independent from the $\mathbf{d}_i$'s. Since $\ell$ is decreasing and then increasing, it is clear from this expression that in order to minimize $D$, one must choose the $\mathbf{d}_i$'s in order to either maximize or minimize $\mathbf{d}_i^\top \Sigma^* \mathbf{d}_i$, whichever maximizes $\ell$. Since the variational characterization of eigenvalues shows that eigenvectors precisely solve this problem, we get the desired result.</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a>(of @thm-thm2)</span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>In @eq-D, the $\mathbf{m}^*$ and the $\Sigma^*$ that appear in the right-hand side are the mean and variance of the density $g^*$ considered in the first argument of the Kullback--Leibler divergence. In particular, if we apply @eq-D with $\phi \equiv 1$, we have $g^* = f$, and the $\mathbf{m}^*$ and $\Sigma^*$ of the right-hand side become $0$ and $I_n$, respectively, so that</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a> D(f, g_{\mathbf{m}, \Sigma}) =  \frac{1}{2} \sum_i \left( \log \alpha_i - \left(1 - \frac{1}{\alpha_i} \right) \right) + \frac{1}{2} \mathbf{m}^\top \Sigma^{-1} \mathbf{m}. </span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>Now, if we consider $\mathbf{m} = \mathbf{m}^*$ and $\Sigma = I + (\alpha - 1) \mathbf{d} \mathbf{d}^\top$, we obtain (using $\Sigma^{-1} = I - (1-1/\alpha) \mathbf{d} \mathbf{d}^\top$ as mentioned in the proof of @lem-D)</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a>D(f, g_{\mathbf{m}^*, \Sigma}) =  \frac{1}{2} \left( \log \alpha - \left(1 - \frac{1}{\alpha} \right) \left( 1 + (\mathbf{d}^\top \mathbf{m}^*)^2 \right) \right) + \frac{1}{2} \lVert \mathbf{m}^* \rVert^2.</span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a>We have seen in the proof of @thm-thm1 that the function $x \mapsto \log x + (1/x-1)\gamma$ is minimized for $x = \gamma$ where it takes the value $-\ell(\gamma)$: $D(f, g_{\mathbf{m}^*, \Sigma})$ is therefore minimized for $\alpha = 1 + (\mathbf{d}^\top \mathbf{m}^*)^2$ and for this value, we have</span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a>D(f, g_{\mathbf{m}^*, \Sigma}) =  - \frac{1}{2} \ell(1 + (\mathbf{d}^\top \mathbf{m}^*)^2) + \frac{1}{2} \lVert \mathbf{m}^* \rVert^2. </span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>As $\ell$ is increasing in $[1, \infty)$, this last quantity is minimized by maximizing $(\mathbf{d}^\top \mathbf{m}^*)^2$, which is obtained for $\mathbf{d} = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert$. The result is proved.</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a><span class="fu"># Framework for the numerical results {#sec-num-results-framework} </span></span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a><span class="fu">## General framework</span></span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a>The objective of the numerical simulations is to evaluate the impact of the choice of the covariance matrix on the estimation accuracy of a high dimensional integral $E$. We compare in this section the estimation results for different choices of the auxiliary covariance matrix when the IS auxiliary density is Gaussian. To extend this comparison, we also compute the results when the IS auxiliary density is chosen with the von Mises–Fisher– Nakagami (vMFN) model recently proposed in <span class="co">[</span><span class="ot">@PapaioannouEtAl_ImprovedCrossEntropybased_2019</span><span class="co">]</span> for high dimensional probability estimation.</span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>In the following section we test these different models of auxiliary densities on five test cases, where $f$ is a standard Gaussian density. This choice is not a theoretical limitation as we can in principle always come back to this case by transforming the vector $\mathbf{X}$ with isoprobabilistic transformations (see for instance <span class="co">[</span><span class="ot">@HohenbichlerRackwitz_NonNormalDependentVectors_1981</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@LiuDerKiureghian_MultivariateDistributionModels_1986</span><span class="co">]</span>).</span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a>The precise numerical framework that we will consider to assess the efficiency of the different auxiliary models is as follows. We assume first that $M$ i.i.d.\ random samples $\mathbf{X}_1,\ldots,\mathbf{X}_M$ distributed from $g^*$ are available from rejection sampling. From these samples, the parameters of the Gaussian and of the vMFN auxiliary density are computed to get an auxiliary density $g'$. Finally, $N$ samples are generated from $g'$ to provide an estimation of $E$ with IS. This procedure is summarized by the following stages: </span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate a sample $\mathbf{X}_1,\ldots,\mathbf{X}_M$ independently according to $g^*$;</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>From $\mathbf{X}_1,\ldots,\mathbf{X}_M$, compute the parameters of the auxiliary parametric density $g'$;</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Generate a new sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ independently from $g'$;</span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Estimate $E$ with $\widehat{E}_N=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}$.</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>The number of samples $M$ and $N$ are respectively set to $M=500$ and $N=2000$. This procedure is then repeated $100$ times to provide a mean estimation $\widehat{E}$ of $E$. In the result tables, for each auxiliary density $g'$ we report the corresponding value for the relative error $\widehat{E}/ E-1$ and the coefficient of variation of the $100$ iterations (the empirical standard deviation divided by $E$). As was established in the proof of @thm-thm1, the KL divergence is, up to an additive constant, equal to $D'(\Sigma) = \log \lvert \Sigma \rvert + \textrm{tr}(\Sigma^* \Sigma^{-1})$ which we will refer to as partial KL divergence. In the result tables, we also report thus the mean value of $D'(\Sigma)$ to analyse the relevancy of the auxiliary density $g_{\widehat{\mathbf{m}}^*, \Sigma}$ for these six choices of covariance matrix $\Sigma$. The next sections specify the different parameters of $g'$ for the Gaussian model and for the vMFN model we have considered in the simulations. </span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choice of the auxiliary density $g'$ for the Gaussian model  {#sec-def_cov} </span></span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a>The goal is to get benchmark results to assess whether one can improve estimations of Gaussian IS auxiliary density by projecting the covariance matrix $\Sigma^*$ in the proposed directions $\mathbf{d}^*_i$. The algorithm that we study here (Algorithms 1+2) aims more precisely at understanding whether:</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>projecting can improve the situation with respect to the empirical covariance matrix;</span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the $\mathbf{d}^*_i$'s are good candidates, in particular compared to the choice $\mathbf{m}^*$ suggested in <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span>;</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>what is the impact in making errors in estimating the eigenpairs $(\lambda^*_i, \mathbf{d}^*_i)$.</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a>Let us define the estimate  $\widehat{\mathbf{m}}^*$ of $\mathbf{m}^*$  from the $M$ i.i.d. random samples $\mathbf{X}_1,\ldots,\mathbf{X}_M$ distributed from $g^*$ with</span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a>    \widehat{\mathbf{m}}^* = \frac{1}{M}\sum_{i=1}^M \mathbf{X}_i.</span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a>$$ {#eq-hatm}</span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>In our numerical test cases, we will compare six different choices of Gaussian auxiliary distributions $g'$ with mean $\widehat{\mathbf{m}}^*$ and the following covariance matrices (see @tbl-sigma):</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\Sigma^*$: the optimal covariance matrix given by @eq-mstar;</span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\widehat{\Sigma}^*$: the empirical estimation of $\Sigma^*$ given by</span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>    \widehat{\Sigma}^* = \frac{1}{M}\sum_{i=1}^M (\mathbf{X}_i-\widehat{\mathbf{m}}^*)(\mathbf{X}_i-\widehat{\mathbf{m}}^*)^\top.</span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a>$$ {#eq-hatSigma}</span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a>The four other covariance matrices considered in the numerical simulations are of the form </span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a>     $\sum_{i=1}^k (v_i-1) \mathbf{d}_i \mathbf{d}^\top_i + I_n$ where $v_i$ is the variance of $\widehat{\Sigma}^*$ in the direction $\mathbf{d}_i$, $v_i = \mathbf{d}_i^\top \widehat{\Sigma}^* \mathbf{d}_i$. The considered choice of $k$ and $\mathbf{d}_i$ gives the following covariance matrices:   </span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>${\widehat{\Sigma}^{\text{}}_\text{opt}}$ is obtained by choosing $\mathbf{d}_i = \mathbf{d}^*_i$ of @thm-thm1, which is supposed to be perfectly known from $\Sigma^*$ and $k$ is computed with Algorithm 2;</span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>${\widehat{\Sigma}^{\text{+d}}_\text{opt}}$ is obtained by choosing $\mathbf{d}_i = {\widehat{\mathbf{d}}}^*_i$ the $i$-th eigenvector of $\widehat{\Sigma}^*$ (in $\ell$-order), which is an estimation of $\mathbf{d}^*_i$, and $k$ is computed with Algorithm 2;</span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>${\widehat{\Sigma}^{\text{}}_\text{mean}}$ is obtained by choosing $k = 1$ and $\mathbf{d}_1 = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert$;   </span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>${\widehat{\Sigma}^{\text{+d}}_\text{mean}}$ is obtained by choosing $k = 1$ and $\mathbf{d}_1 = {\widehat{\mathbf{m}}}^* / \lVert {\widehat{\mathbf{m}}}^* \rVert$, where $\widehat{\mathbf{m}}^*$ given by @eq-hatm.  </span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>The matrices ${\widehat{\Sigma}^{\text{}}_\text{opt}}$ and ${\widehat{\Sigma}^{\text{}}_\text{mean}}$ use the estimation $\widehat{\Sigma}^*$ but the actual directions $\mathbf{d}^*_i$ or $\mathbf{m}^*$, while the matrices ${\widehat{\Sigma}^{\text{+d}}_\text{opt}}$ and ${\widehat{\Sigma}^{\text{+d}}_\text{mean}}$ involve an additional estimation of the directions. By definition, $\Sigma^*$ will give optimal results, while results for $\widehat{\Sigma}^*$ will deteriorate as the dimension increases, which is the well-known behavior which we try to improve. Moreover, for ${\widehat{\Sigma}^{\text{}}_\text{mean}}$ and ${\widehat{\Sigma}^{\text{}}_\text{opt}}$, the projection directions, if not known analytically, are obtained by a brute force Monte Carlo scheme with a very high simulation budget. Finally, we emphasize that Algorithm 1 corresponds to estimating and projecting on the $\mathbf{d}^*_i$'s, and so the matrix $\widehat{\Sigma}^*_k$ of Algorithm 1 is equal to the matrix ${\widehat{\Sigma}^{\text{+d}}_\text{opt}}$, i.e., $\widehat{\Sigma}^*_k = {\widehat{\Sigma}^{\text{+d}}_\text{opt}}$.</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>|   |$\Sigma^*$|$\widehat{\Sigma}^*$|${\widehat{\Sigma}^{\text{}}_\text{opt}}$|${\widehat{\Sigma}^{\text{}}_\text{mean}}$|${\widehat{\Sigma}^{\text{+d}}_\text{opt}}$|${\widehat{\Sigma}^{\text{+d}}_\text{mean}}$|</span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>|-------|---|---|---|---|---|---|---|</span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a>|Initial covariance matrix|$\Sigma^*$|$\widehat \Sigma^*$|$\widehat \Sigma^*$|$\widehat \Sigma^*$|$\widehat \Sigma^*$|$\widehat \Sigma^*$|</span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>|Projection directions (exact or estimated)|-|-|Exact|Exact|Estimated|Estimated|</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>|Choice for the projection direction|None|None|Opt|Mean|Opt|Mean|</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a>:  Presentation of the six covariance matrices considered in the numerical examples. Except $\Sigma^*$, the five other matrices involve one or two estimations: $\widehat \Sigma^*$ is the empirical estimation of $\Sigma^*$ given by @eq-hatSigma. The four others are obtained by projecting $\widehat \Sigma^*$ on: (i) the optimal directions $\mathbf{d}^*_i$ for ${\widehat{\Sigma}^{\text{}}_\text{opt}}$; (ii) estimations $\widehat{\mathbf{d}}^*_i$ of the optimal directions $\mathbf{d}^*_i$ for ${\widehat{\Sigma}^{\text{+d}}_\text{opt}}$; (iii) $\mathbf{m}^*$ for ${\widehat{\Sigma}^{\text{}}_\text{mean}}$; (iv) the estimation $\widehat{\mathbf{m}}^*$ in @eq-mstar of $\mathbf{m}^*$ for ${\widehat{\Sigma}^{\text{+d}}_\text{mean}}$. The subscript therefore indicates the choice for the projection direction, while the superscript +d indicates whether these directions are estimated or not. {#tbl-sigma}</span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choice of the auxiliary density $g'$ for the von Mises–Fisher–Nakagami model</span></span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>Von Mises–Fisher–Nakagami (vMFN) distributions were proposed in <span class="co">[</span><span class="ot">@PapaioannouEtAl_ImprovedCrossEntropybased_2019</span><span class="co">]</span> as an alternative to the Gaussian parametric family to perform IS for high dimensional probability estimation. A random vector $\mathbf{X}$ drawn according to the vMFN distribution can be written as $\mathbf{X}=R {\bf A}$ where ${\bf A}=\frac{\mathbf{X}}{\lVert\mathbf{X}\rVert}$ is a unit random vector following the von Mises-Fisher distribution, and $R=\lVert\mathbf{X}\rVert$ is a positive random variable with a Nakagami distribution; further, $R$ and $\bf A$ are independent. The vMFN pdf can be written as</span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>g_\text{vMFN}({\bf x})= g_\text{N}(\lVert{\bf x}\rVert, p, \omega) \times g_\text{vMF} \left( \frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa \right).</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>$${#eq-vMFN}</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a>The density $g_\text{N}(\lVert {\bf x}\rVert, p, \omega)$ is the Nakagami distribution with shape parameter $p \geq 0.5$ and a spread parameter $\omega&gt;0$ defined by</span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>g_\text{N}(\lVert {\bf x}\rVert, p, \omega) = \frac{2 p^p}{\Gamma(p) \omega^p} \lVert {\bf x}\rVert^{2p-1} \exp\left( - \frac{p}{\omega}\lVert {\bf x}\rVert^2\right) </span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a>and the density $g_\text{vMF}(\frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa)$ is the von Mises-Fisher distribution, given by</span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a>$$g_\text{vMF} \left( \frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa \right) = C_n(\kappa) \exp\left(\kappa {\boldsymbol{\mu}}^T \frac{{\bf x}}{\lVert{\bf x}\rvert\rvert} \right),</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>where $C_n(\kappa)$ is a normalizing constant, $\boldsymbol{\mu}$ is a mean direction $\boldsymbol{\mu}$ (with $\lvert\lvert\boldsymbol{\mu}\rvert\rvert=1$) and $\kappa &gt; 0$ is a concentration parameter $\kappa&gt;0$.</span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>Choosing a vMFN distribution therefore amounts to choosing the parameters $p, \omega, {\boldsymbol{\mu}},$ and $\kappa$. There are therefore $n+3$ parameters to estimate, which is a significant reduction compared to the $\frac{n(n+3)}{2}$ required parameters of the Gaussian model with full covariance matrix.</span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a>Following <span class="co">[</span><span class="ot">@PapaioannouEtAl_ImprovedCrossEntropybased_2019</span><span class="co">]</span>, given a sample $\mathbf{X}_1,\ldots,\mathbf{X}_M$ distributed from $g^*$, the parameters $\omega$, $p$, $\boldsymbol{\mu}$ and $\kappa$ are set in the following way in order to define $g'$:</span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a>$$ \widehat{\omega}=\frac{1}{M}\sum_{i=1}^M \lVert\mathbf{X}_i\rVert^2 \ \text{ and } \ \widehat{p}=\frac{\widehat{\omega}^2}{\widehat{\tau}-\widehat{\omega}^2} \text{ with } \widehat{\tau}=\frac{1}{M}\sum_{i=1}^M \lVert\mathbf{X}_i\rVert^4 </span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a>$$ \widehat{\boldsymbol{\mu}}=\frac{\sum_{i=1}^M \frac{\mathbf{X}_i}{\lvert\lvert\mathbf{X}_i\rvert\rvert}}{\lvert\lvert\sum_{i=1}^M \frac{\mathbf{X}_i}{\lvert\lvert\mathbf{X}_i\rvert\rvert} \rvert\rvert} \ \text{ and } \  \widehat{\kappa}=\dfrac{n\widehat{\chi}-\widehat{\chi}^3}{1-\widehat{\chi}^2} \text{ with } \widehat{\chi} = \min \left( \left \lVert \frac{1}{M}\sum_{i=1}^M \frac{\mathbf{X}_i}{\lVert \mathbf{X}_i \rVert} \right \rVert, 0.95 \right). </span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a><span class="fu">#  Numerical results on five test cases   {#sec-test-cases} </span></span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a> The proposed numerical framework is applied on three examples that are often considered to assess the performance of importance sampling algorithms and also two test cases  from the area of financial mathematics. </span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a><span class="fu">##  Test case 1: one-dimensional optimal projection   {#sec-sub:sum} </span></span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>We consider a test case where all computations can be made exactly. This is a classical example of rare event probability estimation, often used to test the robustness of a method in high dimension. It is given by $\phi(\mathbf{x})=\mathbb{I}_{<span class="sc">\{</span>\varphi(\mathbf{x})\geq 0<span class="sc">\}</span>}$ with $\varphi$ the following affine function:</span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a>    \varphi: \mathbf{x}=(x_1,\ldots,x_n)\in\mathbb{R}^n \mapsto\underset{j=1}{\overset{n}{\sum}} x_j-3\sqrt{n}.</span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a>$${#eq-sum}</span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>The quantity of interest $E$ is defined as $E=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)\simeq 1.35\cdot 10^{-3}$ for all $n$ where the density $f$ is the standard $n$-dimensional Gaussian distribution. Here, the zero-variance density is $g^*(\mathbf{x})=\dfrac{f(\mathbf{x})\mathbb{I}_{<span class="sc">\{</span>\varphi(\mathbf{x})\geq 0<span class="sc">\}</span>}}{E}$, and the optimal parameters $\mathbf{m}^*$ and $\Sigma^*$ in @eq-mstar can be computed exactly, namely $\mathbf{m}^* = \alpha \textbf{1}$ with $\alpha = e^{-9/2}/(E(2\pi)^{1/2})$ and $\textbf{1} = \frac{1}{\sqrt n} (1,\ldots,1) \in \mathbb{R}^n$ the normalized constant vector, and $\Sigma^* =(v-1) \mathbf{1} \mathbf{1}^\top + I_n$ with $v=3\alpha-\alpha^2+1$.</span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Evolution of the partial KL divergence and spectrum</span></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a>@fig-eigsum-1 represents the evolution as the dimension varies between $5$ and $100$ of the partial KL divergence $D'$ for three different choices of covariance matrix: the optimal matrix $\Sigma^*$, its empirical estimation $\widehat{\Sigma}^*$ and the estimation $\widehat{\Sigma}^*_k$ of the optimal lower-dimensional covariance matrix. We can notice that the partial KL divergence for $\widehat{\Sigma}^*$ grows much faster than the other two, and that the partial KL divergence for $\widehat{\Sigma}^*_k$ remains very close to the optimal value $D'(\Sigma^*)$. As the KL divergence is a proxy for the efficiency of the auxiliary density (it is for instance closely related to the number of samples required for a given precision [@Chatterjee18:0]), this suggests that using $\widehat{\Sigma}^*_k$ will provide results close to optimal.</span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a>We now check this claim. As $\Sigma^* = (v-1) \textbf{1} \textbf{1}^\top + I_n$, its eigenpairs are $(v, \textbf{1})$ and $(1,\mathbf{d}_i)$ where the $\mathbf{d}_i$'s form an orthonormal basis of the space orthogonal to the space spanned by $\textbf{1}$. In particular, $(v, \textbf{1})$ is the largest (in $\ell$-order) eigenpair of $\Sigma^*$ and $\Sigma^*_k = \Sigma^*$ for any $k \geq 1$.</span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a>In practice, we do not use this theoretical knowledge and $\Sigma^*$, $\Sigma^*_k$ and the eigenpairs are estimated. The six covariance matrices introduced in @sec-def_cov and in which we are interested are as follows:</span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Sigma^* = (v-1) \textbf{1} \textbf{1}^\top + I_n$;</span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\widehat{\Sigma}^*$ given by @eq-hatSigma;</span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\widehat{\Sigma}^{\text{}}_\text{opt}}$ and ${\widehat{\Sigma}^{\text{}}_\text{mean}}$ are equal and given by $(\widehat \lambda-1) \textbf{1} \textbf{1}^\top + I_n$ with $\widehat{\lambda} = \textbf{1}^\top \widehat{\Sigma}^* \textbf{1}$. This amounts to assuming that the projection direction $\textbf{1}$ is perfectly known, whereas the variance in this direction is estimated;</span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\widehat{\Sigma}^{\text{+d}}_\text{opt}} = (\widehat{\lambda} - 1) \widehat{\mathbf{d}} {\widehat{\mathbf{d}}}^\top + I_n$ with $(\widehat{\lambda}, \widehat{\mathbf{d}})$ the smallest eigenpair of $\widehat{\Sigma}^*$. The difference with the previous case is that we do not assume anymore that the optimal projection direction $\textbf{1}$ is known, and so it needs to be estimated;</span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\widehat{\Sigma}^{\text{+d}}_\text{mean}} = (\widehat{\lambda} - 1) \frac{\widehat{\mathbf{m}}^* {(\widehat{\mathbf{m}}^*)}^\top}{\lVert \widehat{\mathbf{m}}^* \rVert^2} + I_n$ with $\widehat{\mathbf{m}}^*$ given by @eq-hatm and $\widehat{\lambda} = \frac{{(\widehat{\mathbf{m}}^*)}^\top \widehat{\Sigma}^* \widehat{\mathbf{m}}^*}{\lVert \widehat{\mathbf{m}}^* \rVert^2}$. Here we assume that $\mathbf{m}^*$ is a good projection direction, but is unknown and therefore needs to be estimated.</span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a>Note that in the particularly simple case considered here, both $\widehat{\mathbf{m}}^* / \lVert \widehat{\mathbf{m}}^* \rVert$ and $\widehat{\mathbf{d}}$ are estimators of $\textbf{1}$ but they are obtained by different methods. In the next example we will consider a case where $\mathbf{m}^*$ is not an optimal projection direction as given by @thm-thm1.</span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a>@fig-eigsum-2 represents the images by $\ell$ of the eigenvalues of $\Sigma^*$ and $\widehat{\Sigma}^*$. This picture carries a very important insight. We notice that the estimation of most eigenvalues is poor: indeed, all the blue crosses except the leftmost one are meant to be estimator of $1$, whereas we see that they are more or less uniformly spread between $0.2$ and $2.5$. This means that the variance terms in the corresponding directions are poorly estimated, which could be the explanation on why the use of $\widehat{\Sigma}^*$ gives an inaccurate estimation. But what we remark also is that the function $\ell$ is quite flat around one: as a consequence, although the eigenvalues offer significant variability, this variability is smoothed by the action of $\ell$. Indeed, the images of the eigenvalues by $\ell$ take values between $0$ and $0.8$ and have smaller variability. Moreover, $\ell(x)$ increases sharply as $x$ approaches $0$ and thus efficiently distinguishes between the two leftmost estimated eigenvalues and is able to separate them.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>
for (const element of document.getElementsByClassName("pseudocode")){
    pseudocode.renderElement(element);
}
</script>
<script>
for (const element of document.getElementsByClassName("pseudocode")){
    pseudocode.renderElement(element);
}
</script>



</body></html>