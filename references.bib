@article{computo,
  title = {Computo: reproducible computational/algorithmic contributions in statistics and machine learning},
  author = {{Computo Team}},
  year = {2021},
  journal = {computo}
}

@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2020},
  url = {https://www.R-project.org/},
}

@Manual{R-reticulate,
  title = {reticulate: Interface to Python},
  author = {Kevin Ushey and JJ Allaire and Yuan Tang},
  year = {2020},
  note = {R package version 1.18},
  url = {https://github.com/rstudio/reticulate},
}

@article{perez2011python,
    title	= {Python: an ecosystem for scientific computing},
    author	= {Perez, Fernando and Granger, Brian E and Hunter, John D},
    journal	= {Computing in Science \\& Engineering},
    volume	= {13},
    number	= {2},
    pages	= {13--21},
    year	= {2011},
    publisher	= {AIP Publishing}
}

%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Florian Simatos at 2021-06-21 12:38:32 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{Benaych-Georges11:0,
	author = {Florent Benaych-Georges and Raj Rao Nadakuditi},
	date-added = {2021-06-21 12:37:49 +0200},
	date-modified = {2021-06-21 12:37:49 +0200},
	doi = {10.1016/j.aim.2011.02.007},
	issn = {0001-8708},
	journal = {Advances in Mathematics},
	keywords = {Random matrices, Haar measure, Principal components analysis, Informational limit, Free probability, Phase transition, Random eigenvalues, Random eigenvectors, Random perturbation, Sample covariance matrices},
	number = {1},
	pages = {494--521},
	title = {The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices},
	volume = {227},
	year = {2011}}

@article{Nadakuditi08:0,
	author = {Nadakuditi, Raj Rao and Edelman, Alan},
	date-added = {2021-06-21 12:37:49 +0200},
	date-modified = {2021-06-21 12:37:49 +0200},
	doi = {10.1109/TSP.2008.917356},
	journal = {IEEE Transactions on Signal Processing},
	number = {7},
	pages = {2625-2638},
	title = {Sample Eigenvalue Based Detection of High-Dimensional Signals in White Noise Using Relatively Few Samples},
	volume = {56},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/TSP.2008.917356}}

@article{Chatterjee18:0,
	author = {Chatterjee, Sourav and Diaconis, Persi},
	journal = {Ann. Appl. Probab.},
	month = {04},
	number = {2},
	pages = {1099--1135},
	title = {The sample size required in importance sampling},
	volume = {28},
	year = {2018},
	doi ={10.1214/17-AAP1326}
	}

@inbook{Bengtsson08:0,
	address = {Beachwood, Ohio, USA},
	author = {Bengtsson, Thomas and Bickel, Peter and Li, Bo},
	editor = {Nolan, Deborah and Speed, Terry},
	pages = {316--334},
	publisher = {Institute of Mathematical Statistics},
	series = {Collections},
	title = {Curse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems},
	volume = {Volume 2},
	year = {2008}}

@article{AgapiouEtAl_ImportanceSamplingIntrinsic_2015,
	abstract = {The basic idea of importance sampling is to use independent samples from a proposal measure in order to approximate expectations with respect to a target measure. It is key to understand how many samples are required in order to guarantee accurate approximations. Intuitively, some notion of distance between the target and the proposal should determine the computational cost of the method. A major challenge is to quantify this distance in terms of parameters or statistics that are pertinent for the practitioner. The subject has attracted substantial interest from within a variety of communities. The objective of this paper is to overview and unify the resulting literature by creating an overarching framework. A general theory is presented, with a focus on the use of importance sampling in Bayesian inverse problems and filtering.},
	archiveprefix = {arXiv},
	author = {Agapiou, S. and Papaspiliopoulos, O. and {Sanz-Alonso}, D. and Stuart, A. M.},
	eprint = {1511.06196},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\3KF2IGG6\\Agapiou et al. - 2015 - Importance Sampling Intrinsic Dimension and Compu.pdf},
	journal = {arXiv:1511.06196 [stat]},
	keywords = {Statistics - Computation},
	language = {en},
	month = nov,
	primaryclass = {stat},
	shorttitle = {Importance {{Sampling}}},
	title = {Importance {{Sampling}}: {{Intrinsic Dimension}} and {{Computational Cost}}},
	year = {2015}}

@article{AgapiouEtAl_ImportanceSamplingIntrinsic_2017,
	author = {Agapiou, S. and Papaspiliopoulos, O. and {Sanz-Alonso}, D. and Stuart, A. M.},
	journal = {Statistical Science, Volume 32, p405-431},
	title = {Importance {{Sampling}} : {{Intrinsic Dimension}} and {{Computational Cost}}},
	year = {2017},
	doi = {10.1214/17-STS611},		
	}

@article{AshurbekovaEtAl_OptimalShrinkageRobust_,
	abstract = {When estimating covariance matrices, traditional sample covariance-based estimators are straightforward but suffer from two main issues: 1) a lack of robustness, which occurs as soon as the samples do not come from a Gaussian distribution or are contaminated with outliers and 2) a lack of data, which occurs as soon as the covariance matrix dimension is greater than the sample size. The first issue can be handled by assuming that samples are drawn from a heavy-tailed distribution, at the cost of more complex derivations, while the second issue can be addressed by shrinkage with the difficulty of choosing the appropriate level of regularization. This work offers both a tractable and optimal framework based on shrinked likelihood-based M-estimators. First, a closed-form expression is provided for a regularized covariance matrix estimator with an optimal shrinkage coefficient for any sample distribution in the elliptical family. Then, a complete inference procedure is proposed which can handle both unknown mean and tail parameter, in contrast to most existing methods that focus on the covariance matrix parameter requiring pre-set values for the others. An illustration on synthetic and real brain connectivity data is provided in the case of the t-distribution with unknown mean and degrees-of-freedom parameters.},
	author = {Ashurbekova, Karina and {Usseglio-Carleve}, Antoine and Forbes, Florence and Achard, Sophie},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\TWRVUYEI\\Ashurbekova et al. - Optimal shrinkage for robust covariance matrix est.pdf},
	language = {en},
	year={2020},
	note={hal-02378034v2},
	title = {Optimal Shrinkage for Robust Covariance Matrix Estimators in a Small Sample Size Setting}}

@book{AsmussenGlynn_StochasticSimulationAlgorithms_2007,
	address = {{New York, NY}},
	annotation = {OCLC: 255027977},
	author = {Asmussen, S{\o}ren and Glynn, Peter W.},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\I9NAKVYH\\Asmussen et Glynn - 2007 - Stochastic simulation algorithms and analysis.pdf},
	isbn = {978-0-387-30679-7 978-0-387-69033-9},
	language = {en},
	number = {57},
	publisher = {{Springer}},
	series = {Stochastic Modelling and Applied Probability},
	shorttitle = {Stochastic Simulation},
	title = {Stochastic Simulation: Algorithms and Analysis},
	year = {2007}}

@article{AuBeck_EstimationSmallFailure_2001,
	abstract = {A new simulation approach, called `subset simulation', is proposed to compute small failure probabilities encountered in reliability analysis of engineering systems. The basic idea is to express the failure probability as a product of larger conditional failure probabilities by introducing intermediate failure events. With a proper choice of the conditional events, the conditional failure probabilities can be made suf\textregistered ciently large so that they can be estimated by means of simulation with a small number of samples. The original problem of calculating a small failure probability, which is computationally demanding, is reduced to calculating a sequence of conditional probabilities, which can be readily and ef\textregistered ciently estimated by means of simulation. The conditional probabilities cannot be estimated ef\textregistered ciently by a standard Monte Carlo procedure, however, and so a Markov chain Monte Carlo simulation (MCS) technique based on the Metropolis algorithm is presented for their estimation. The proposed method is robust to the number of uncertain parameters and ef\textregistered cient in computing small probabilities. The ef\textregistered ciency of the method is demonstrated by calculating the \textregistered rst-excursion probabilities for a linear oscillator subjected to white noise excitation and for a \textregistered ve-story nonlinear hysteretic shear building under uncertain seismic excitation. q 2001 Elsevier Science Ltd. All rights reserved.},
	author = {Au, Siu-Kui and Beck, James L.},
	doi = {10.1016/S0266-8920(01)00019-4},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\DSN3IX4M\\Au et Beck - 2001 - Estimation of small failure probabilities in high .pdf},
	issn = {02668920},
	journal = {Probabilistic Engineering Mechanics},
	language = {en},
	month = oct,
	number = {4},
	pages = {263--277},
	title = {Estimation of Small Failure Probabilities in High Dimensions by Subset Simulation},
	volume = {16},
	year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1016/S0266-8920(01)00019-4}}

@article{AuBeck_ImportantSamplingHigh_2003,
	abstract = {This paper draws attention to a fundamental problem that occurs in applying importance sampling to `high-dimensional' reliability problems, i.e., those with a large number of uncertain parameters. This question of applicability carries an important bearing on the potential use of importance sampling for solving dynamic first-excursion problems and static reliability problems for structures with a large number of uncertain structural model parameters. The conditions under which importance sampling is applicable in high dimensions are investigated, where the focus is put on the common case of standard Gaussian uncertain parameters. It is found that importance sampling densities using design points are applicable if the covariance matrix associated with each design point does not deviate significantly from the identity matrix. The study also suggests that importance sampling densities using random pre-samples are generally not applicable in high dimensions.},
	author = {Au, S.K. and Beck, J.L.},
	doi = {10.1016/S0167-4730(02)00047-4},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\LPRKZE5A\\Au et Beck - 2003 - Important sampling in high dimensions.pdf},
	issn = {01674730},
	journal = {Structural Safety},
	language = {en},
	month = apr,
	number = {2},
	pages = {139--163},
	title = {Important Sampling in High Dimensions},
	volume = {25},
	year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1016/S0167-4730(02)00047-4}}

@article{AuEtAl_ApplicationSubsetSimulation_2007,
	abstract = {This paper presents the reliability analysis of three benchmark problems using three variants of Subset Simulation. The original version of Subset Simulation, SubSim/MCMC, employs a Markov chain Monte Carlo (MCMC) method to simulate samples conditional on intermediate failure events; it is a general method that is applicable to all the benchmark problems. SubSim/Splitting is a variant of Subset Simulation applicable to first-passage problems involving deterministic dynamical systems. It makes use of trajectory splitting for generating conditional samples. Another variant, SubSim/ Hybrid, uses a combined MCMC/Splitting strategy and so it has the advantages of MCMC and splitting; it is applicable to uncertain and deterministic dynamical systems. Results show that all three Subset Simulation variants are effective in high-dimensional problems and that some computational efficiency can be gained by exploiting and incorporating system characteristics into the simulation procedure.},
	author = {Au, S.K. and Ching, J. and Beck, J.L.},
	doi = {10.1016/j.strusafe.2006.07.008},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\TX5NSATA\\Au et al. - 2007 - Application of subset simulation methods to reliab.pdf},
	issn = {01674730},
	journal = {Structural Safety},
	language = {en},
	month = jul,
	number = {3},
	pages = {183--193},
	title = {Application of Subset Simulation Methods to Reliability Benchmark Problems},
	volume = {29},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.strusafe.2006.07.008}}

@article{BalesdentEtAl_KrigingbasedAdaptiveImportance_2013,
	abstract = {Very efficient sampling algorithms have been proposed to estimate rare event probabilities, such as Importance Sampling or Importance Splitting. Even if the number of samples required to apply these techniques is relatively low compared to Monte-Carlo simulations of same efficiency, it is often difficult to implement them on time-consuming simulation codes. A joint use of sampling techniques and surrogate models may thus be of use. In this article, we develop a Kriging-based adaptive Importance Sampling approach for rare event probability estimation. The novelty resides in the use of adaptive Importance Sampling and consequently the ability to estimate very rare event probabilities (lower than 10\`A3) that have not been considered in previous work on similar subjects. The statistical properties of Kriging also make it possible to compute a confidence measure for the resulting estimation. Results on both analytical and engineering test cases show the efficiency of the approach in terms of accuracy and low number of samples.},
	author = {Balesdent, Mathieu and Morio, J{\'e}r{\^o}me and Marzat, Julien},
	doi = {10.1016/j.strusafe.2013.04.001},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\S6H9RM2W\\Balesdent et al. - 2013 - Kriging-based adaptive Importance Sampling algorit.pdf},
	issn = {01674730},
	journal = {Structural Safety},
	language = {en},
	month = sep,
	pages = {1--10},
	title = {Kriging-Based Adaptive {{Importance Sampling}} Algorithms for Rare Event Estimation},
	volume = {44},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.strusafe.2013.04.001}}

@article{BassambooEtAl_PortfolioCreditRisk_2008,
	author = {Bassamboo, Achal and Juneja, Sandeep and Zeevi, Assaf},
	doi = {10.1287/opre.1080.0513},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\XMW8JRK6\\Bassamboo et al. - 2008 - Portfolio Credit Risk with Extremal Dependence As.pdf},
	issn = {0030-364X, 1526-5463},
	journal = {Operations Research},
	language = {en},
	month = jun,
	number = {3},
	pages = {593--606},
	shorttitle = {Portfolio {{Credit Risk}} with {{Extremal Dependence}}},
	title = {Portfolio {{Credit Risk}} with {{Extremal Dependence}}: {{Asymptotic Analysis}} and {{Efficient Simulation}}},
	volume = {56},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1287/opre.1080.0513}}

@incollection{BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008,
	abstract = {It has been widely realized that Monte Carlo methods (approximation via a sample ensemble) may fail in large scale systems. This work offers some theoretical insight into this phenomenon in the context of the particle filter. We demonstrate that the maximum of the weights associated with the sample ensemble converges to one as both the sample size and the system dimension tends to infinity. Specifically, under fairly weak assumptions, if the ensemble size grows sub-exponentially in the cube root of the system dimension, the convergence holds for a single update step in state-space models with independent and identically distributed kernels. Further, in an important special case, more refined arguments show (and our simulations suggest) that the convergence to unity occurs unless the ensemble grows super-exponentially in the system dimension. The weight singularity is also established in models with more general multivariate likelihoods, e.g. Gaussian and Cauchy. Although presented in the context of atmospheric data assimilation for numerical weather prediction, our results are generally valid for high-dimensional particle filters.},
	address = {{Beachwood, Ohio, USA}},
	author = {Bengtsson, Thomas and Bickel, Peter and Li, Bo},
	booktitle = {Institute of {{Mathematical Statistics Collections}}},
	doi = {10.1214/193940307000000518},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\4J79AEMB\\Bengtsson et al. - 2008 - Curse-of-dimensionality revisited Collapse of the.pdf},
	isbn = {978-0-940600-74-4},
	language = {en},
	pages = {316--334},
	publisher = {{Institute of Mathematical Statistics}},
	shorttitle = {Curse-of-Dimensionality Revisited},
	title = {Curse-of-Dimensionality Revisited: {{Collapse}} of the Particle Filter in Very Large Scale Systems},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1214/193940307000000518}}

@article{BlatmanSudret_EfficientComputationGlobal_2010,
	abstract = {Global sensitivity analysis aims at quantifying the relative importance of uncertain input variables onto the response of a mathematical model of a physical system. ANOVA-based indices such as the Sobol' indices are well-known in this context. These indices are usually computed by direct Monte Carlo or quasi-Monte Carlo simulation, which may reveal hardly applicable for computationally demanding industrial models. In the present paper, sparse polynomial chaos (PC) expansions are introduced in order to compute sensitivity indices. An adaptive algorithm allows the analyst to build up a PC-based metamodel that only contains the significant terms whereas the PC coefficients are computed by leastsquare regression using a computer experimental design. The accuracy of the metamodel is assessed by leave-one-out cross validation. Due to the genuine orthogonality properties of the PC basis, ANOVAbased sensitivity indices are post-processed analytically. This paper also develops a bootstrap technique which eventually yields confidence intervals on the results. The approach is illustrated on various application examples up to 21 stochastic dimensions. Accurate results are obtained at a computational cost 2\textendash 3 orders of magnitude smaller than that associated with Monte Carlo simulation.},
	author = {Blatman, G{\'e}raud and Sudret, Bruno},
	doi = {10.1016/j.ress.2010.06.015},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\KEXK83T9\\Blatman et Sudret - 2010 - Efficient computation of global sensitivity indice.pdf},
	issn = {09518320},
	journal = {Reliability Engineering \& System Safety},
	language = {en},
	month = nov,
	number = {11},
	pages = {1216--1229},
	title = {Efficient Computation of Global Sensitivity Indices Using Sparse Polynomial Chaos Expansions},
	volume = {95},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.ress.2010.06.015}}

@article{BotevEtAl_GeneralizedCrossEntropyMethods_,
	abstract = {The cross-entropy and minimum cross-entropy methods are well-known Monte Carlo simulation techniques for rare-event probability estimation and optimization. In this paper we investigate how these methods can be extended to provide a general non-parametric cross-entropy framework based on {$\varphi$}-divergence distance measures. We show how the {$\chi$}2 distance in particular yields a viable alternative to Kullback-Leibler distance. The theory is illustrated with various examples from density estimation, rareevent simulation and continuous multi-extremal optimization.},
	author = {Botev, Z I and Kroese, D P and Taimre, T},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\9GVTVDGF\\Botev et al. - Generalized Cross-Entropy Methods.pdf},
	language = {en},
	pages = {30},
	title = {Generalized {{Cross}}-{{Entropy Methods}}}}

@article{BotevEtAl_MarkovChainImportance_2013,
	abstract = {We present a versatile Monte Carlo method for estimating multidimensional integrals, with applications to rare-event probability estimation. The method fuses two distinct and popular Monte Carlo simulation methods\textemdash Markov chain Monte Carlo and importance sampling\textemdash into a single algorithm. We show that for some applied numerical examples the proposed Markov Chain importance sampling algorithm performs better than methods based solely on importance sampling or MCMC.},
	author = {Botev, Zdravko I. and L'Ecuyer, Pierre and Tuffin, Bruno},
	doi = {10.1007/s11222-011-9308-2},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\SW766R4J\\Botev et al. - 2013 - Markov chain importance sampling with applications.pdf},
	issn = {0960-3174, 1573-1375},
	journal = {Statistics and Computing},
	language = {en},
	month = mar,
	number = {2},
	pages = {271--285},
	title = {Markov Chain Importance Sampling with Applications to Rare Event Probability Estimation},
	volume = {23},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11222-011-9308-2}}

@article{BotevKroese_EfficientAlgorithmRareevent_2008,
	abstract = {Although importance sampling is an established and effective sampling and estimation technique, it becomes unstable and unreliable for high-dimensional problems. The main reason is that the likelihood ratio in the importance sampling estimator degenerates when the dimension of the problem becomes large. Various remedies to this problem have been suggested, including heuristics such as resampling. Even so, the consensus is that for large-dimensional problems, likelihood ratios (and hence importance sampling) should be avoided. In this paper we introduce a new adaptive simulation approach that does away with likelihood ratios, while retaining the multi-level approach of the cross-entropy method. Like the latter, the method can be used for rare-event probability estimation, optimization, and counting. Moreover, the method allows one to sample exactly from the target distribution rather than asymptotically as in Markov chain Monte Carlo. Numerical examples demonstrate the effectiveness of the method for a variety of applications.},
	author = {Botev, Zdravko I. and Kroese, Dirk P.},
	doi = {10.1007/s11009-008-9073-7},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\WWTWB9Z5\\Botev et Kroese - 2008 - An Efficient Algorithm for Rare-event Probability .pdf},
	issn = {1387-5841, 1573-7713},
	journal = {Methodology and Computing in Applied Probability},
	language = {en},
	month = dec,
	number = {4},
	pages = {471--505},
	title = {An {{Efficient Algorithm}} for {{Rare}}-Event {{Probability Estimation}}, {{Combinatorial Optimization}}, and {{Counting}}},
	volume = {10},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11009-008-9073-7}}

@inproceedings{BotevKroese_GlobalLikelihoodOptimization_2004,
	abstract = {Global likelihood maximization is an important aspect of many statistical analyses. Often the likelihood function is highly multi-extremal. This presents a significant challenge to standard search procedures, which often settle too quickly into an inferior local maximum. We present a new approach based on the cross-entropy (CE) method, and illustrate its use for the analysis of mixture models.},
	address = {{Washington, D.C.}},
	author = {Botev, Z. and Kroese, D.P.},
	booktitle = {Proceedings of the 2004 {{Winter Simulation Conference}}, 2004.},
	doi = {10.1109/WSC.2004.1371358},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\ADKUXPVJ\\Botev et Kroese - 2004 - Global Likelihood Optimization Via the Cross-Entro.pdf},
	isbn = {978-0-7803-8786-7},
	language = {en},
	pages = {517--523},
	publisher = {{IEEE}},
	title = {Global {{Likelihood Optimization Via}} the {{Cross}}-{{Entropy Method}}, with an {{Application}} to {{Mixture Models}}},
	volume = {1},
	year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1109/WSC.2004.1371358}}

@article{Bourinet_ReliabilityAnalysisOptimal_,
	author = {Bourinet, Jean-Marc},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\N5258N4H\\Bourinet - Reliability analysis and optimal design under unce.pdf},
	language = {en},
	pages = {252},
	title = {Reliability Analysis and Optimal Design under Uncertainty - {{Focus}} on Adaptive Surrogate-Based Approaches}}

@inproceedings{BreretonEtAl_FittingMixtureImportance_2011,
	abstract = {In some rare-event settings, exponentially twisted distributions perform very badly. One solution to this problem is to use mixture distributions. However, it is difficult to select a good mixture distribution for importance sampling. We here introduce a simple adaptive method for choosing good mixture importance sampling distributions.},
	address = {{Phoenix, AZ, USA}},
	author = {Brereton, Tim J. and Chan, Joshua C.C. and Kroese, Dirk P.},
	booktitle = {Proceedings of the 2011 {{Winter Simulation Conference}} ({{WSC}})},
	doi = {10.1109/WSC.2011.6147769},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\WEPZBIFH\\Brereton et al. - 2011 - Fitting mixture importance sampling distributions .pdf},
	isbn = {978-1-4577-2109-0 978-1-4577-2108-3 978-1-4577-2106-9 978-1-4577-2107-6},
	language = {en},
	month = dec,
	pages = {422--428},
	publisher = {{IEEE}},
	title = {Fitting Mixture Importance Sampling Distributions via Improved Cross-Entropy},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/WSC.2011.6147769}}

@article{BridgesEtAl_ActiveManifoldsNonlinear_2019,
	abstract = {We present an approach to analyze C1(Rm) functions that addresses limitations present in the Active Subspaces (AS) method of Constantine et al. (2015; 2014). Under appropriate hypotheses, our Active Manifolds (AM) method identifies a 1-D curve in the domain (the active manifold) on which nearly all values of the unknown function are attained, and which can be exploited for approximation or analysis, especially when m is large (high-dimensional input space). We provide theorems justifying our AM technique and an algorithm permitting functional approximation and sensitivity analysis. Using accessible, low-dimensional functions as initial examples, we show AM reduces approximation error by an order of magnitude compared to AS, at the expense of more computation. Following this, we revisit the sensitivity analysis by Glaws et al. (2017), who apply AS to analyze a magnetohydrodynamic power generator model, and compare the performance of AM on the same data. Our analysis provides detailed information not captured by AS, exhibiting the influence of each parameter individually along an active manifold. Overall, AM represents a novel technique for analyzing functional models with benefits including: reducing m-dimensional analysis to a 1-D analogue, permitting more accurate regression than AS (at more computational expense), enabling more informative sensitivity analysis, and granting accessible visualizations (2-D plots) of parameter sensitivity along the AM.},
	archiveprefix = {arXiv},
	author = {Bridges, Robert A. and Gruber, Anthony D. and Felder, Christopher and Verma, Miki and Hoff, Chelsey},
	eprint = {1904.13386},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\6643N96Y\\Bridges et al. - 2019 - Active Manifolds A non-linear analogue to Active .pdf},
	journal = {arXiv:1904.13386 [cs, stat]},
	keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
	language = {en},
	month = may,
	primaryclass = {cs, stat},
	shorttitle = {Active {{Manifolds}}},
	title = {Active {{Manifolds}}: {{A}} Non-Linear Analogue to {{Active Subspaces}}},
	year = {2019}}

@article{BridgesEtAl_DimensionReductionUsing_2018,
	abstract = {Scientists and engineers rely on accurate mathematical models to quantify the objects of their studies, which are often high-dimensional. Unfortunately, high-dimensional models are inherently difficult, i.e. when observations are sparse or expensive to determine. One way to address this problem is to approximate the original model with fewer input dimensions. Our project goal was to recover a function f that takes n inputs and returns one output, where n is potentially large. For any given n-tuple, we assume that we can observe a sample of the gradient and output of the function but it is computationally expensive to do so. This project was inspired by an approach known as Active Subspaces, which works by linearly projecting to a linear subspace where the function changes most on average. Our research gives mathematical developments informing a novel algorithm for this problem. Our approach, Active Manifolds, increases accuracy by seeking nonlinear analogues that approximate the function. The benefits of our approach are eliminated unprincipled parameter, choices, guaranteed accessible visualization, and improved estimation accuracy.},
	archiveprefix = {arXiv},
	author = {Bridges, Robert A. and Felder, Chris and Hoff, Chelsey},
	eprint = {1802.04178},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\7MJ3GRLN\\Bridges et al. - 2018 - Dimension Reduction Using Active Manifolds.pdf},
	journal = {arXiv:1802.04178 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning,Mathematics - Classical Analysis and ODEs,Statistics - Machine Learning},
	language = {en},
	month = feb,
	primaryclass = {cs, math, stat},
	title = {Dimension {{Reduction Using Active Manifolds}}},
	year = {2018}}

@article{BugalloEtAl_AdaptiveImportanceSampling_2015,
	author = {Bugallo, M{\'o}nica F. and Martino, Luca and Corander, Jukka},
	doi = {10.1016/j.dsp.2015.05.014},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\THYZT6YR\\Bugallo et al. - 2015 - Adaptive importance sampling in signal processing.pdf},
	issn = {10512004},
	journal = {Digital Signal Processing},
	language = {en},
	month = dec,
	pages = {36--49},
	title = {Adaptive Importance Sampling in Signal Processing},
	volume = {47},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.dsp.2015.05.014}}

@article{BugalloEtAl_AdaptiveImportanceSampling_2017,
	author = {Bugallo, Monica F. and Elvira, Victor and Martino, Luca and Luengo, David and Miguez, Joaquin and Djuric, Petar M.},
	doi = {10.1109/MSP.2017.2699226},
	issn = {1053-5888, 1558-0792},
	journal = {IEEE Signal Processing Magazine},
	language = {en},
	month = jul,
	number = {4},
	pages = {60--79},
	shorttitle = {Adaptive {{Importance Sampling}}},
	title = {Adaptive {{Importance Sampling}}: {{The}} Past, the Present, and the Future},
	volume = {34},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/MSP.2017.2699226}}

@article{CadiniEtAl_ImprovedAdaptiveKrigingbased_2014,
	abstract = {The estimation of system failure probabilities may be a difficult task when the values involved are very small, so that sampling-based Monte Carlo methods may become computationally impractical, especially if the computer codes used to model the system response require large computational efforts, both in terms of time and memory. This paper proposes a modification of an algorithm proposed in literature for the efficient estimation of small failure probabilities, which combines FORM to an adaptive krigingbased importance sampling strategy (AK-IS). The modification allows overcoming an important limitation of the original AK-IS in that it provides the algorithm with the flexibility to deal with multiple failure regions characterized by complex, non-linear limit states. The modified algorithm is shown to offer satisfactory results with reference to four case studies of literature, outperforming in general several other alternative methods of literature.},
	author = {Cadini, F. and Santos, F. and Zio, E.},
	doi = {10.1016/j.ress.2014.06.023},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\YLCRX4T6\\Cadini et al. - 2014 - An improved adaptive kriging-based importance tech.pdf},
	issn = {09518320},
	journal = {Reliability Engineering \& System Safety},
	language = {en},
	month = nov,
	pages = {109--117},
	title = {An Improved Adaptive Kriging-Based Importance Technique for Sampling Multiple Failure Regions of Low Probability},
	volume = {131},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.ress.2014.06.023}}

@article{CappeEtAl_AdaptiveImportanceSampling_2008,
	author = {Capp{\'e}, Olivier and Douc, Randal and Guillin, Arnaud and Marin, Jean-Michel and Robert, Christian P.},
	doi = {10.1007/s11222-008-9059-x},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\3D8UMNX5\\Capp{\'e} et al. - 2008 - Adaptive importance sampling in general mixture cl.pdf},
	issn = {0960-3174, 1573-1375},
	journal = {Statistics and Computing},
	language = {en},
	month = dec,
	number = {4},
	pages = {447--459},
	title = {Adaptive Importance Sampling in General Mixture Classes},
	volume = {18},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11222-008-9059-x}}

@article{CappeEtAl_PopulationMonteCarlo_2004,
	author = {Capp{\'e}, O and Guillin, A and Marin, J. M and Robert, C. P},
	doi = {10.1198/106186004X12803},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\IEX3LM3V\\Capp{\'e} et al. - 2004 - Population Monte Carlo.pdf},
	issn = {1061-8600, 1537-2715},
	journal = {Journal of Computational and Graphical Statistics},
	language = {en},
	month = dec,
	number = {4},
	pages = {907--929},
	title = {Population {{Monte Carlo}}},
	volume = {13},
	year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1198/106186004X12803}}

@article{ChanEtAl_COMPARISONCROSSENTROPYVARIANCE_,
	abstract = {The variance minimization (VM) and cross-entropy (CE) methods are two versatile adaptive importance sampling procedures that have been successfully applied to a wide variety of difficult rare-event estimation problems. We compare these two methods via various examples where the optimal VM and CE importance densities can be obtained analytically. We find that in the cases studied both VM and CE methods prescribe the same importance sampling parameters, suggesting that the criterion of minimizing the CE distance is very close, if not asymptotically identical, to minimizing the variance of the associated importance sampling estimator.},
	author = {Chan, Joshua C C and Glynn, Peter W and Kroese, Dirk P},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\42LG6GGC\\Chan et al. - A COMPARISON OF CROSS-ENTROPY AND VARIANCE MINIMIZ.pdf},
	language = {en},
	pages = {13},
	title = {A {{COMPARISON OF CROSS}}-{{ENTROPY AND VARIANCE MINIMIZATION STRATEGIES}}}}

@article{ChanKroese_ImprovedCrossentropyMethod_2012,
	abstract = {The cross-entropy (CE) method is an adaptive importance sampling procedure that has been successfully applied to a diverse range of complicated simulation problems. However, recent research has shown that in some highdimensional settings, the likelihood ratio degeneracy problem becomes severe and the importance sampling estimator obtained from the CE algorithm becomes unreliable. We consider a variation of the CE method whose performance does not deteriorate as the dimension of the problem increases. We then illustrate the algorithm via a highdimensional estimation problem in risk management.},
	author = {Chan, Joshua C. C. and Kroese, Dirk P.},
	doi = {10.1007/s11222-011-9275-7},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\U4IADDZC\\Chan et Kroese - 2012 - Improved cross-entropy method for estimation.pdf},
	issn = {0960-3174, 1573-1375},
	journal = {Statistics and Computing},
	language = {en},
	month = sep,
	number = {5},
	pages = {1031--1040},
	title = {Improved Cross-Entropy Method for Estimation},
	volume = {22},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11222-011-9275-7}}

@article{ChanKroese_RareeventProbabilityEstimation_2011,
	abstract = {Estimation of rare-event probabilities in high-dimensional settings via importance sampling is a difficult problem due to the degeneracy of the likelihood ratio. In fact, it is generally recommended that Monte Carlo estimators involving likelihood ratios should not be used in such settings. In view of this, we develop efficient algorithms based on conditional Monte Carlo to estimate rare-event probabilities in situations where the degeneracy problem is expected to be severe. By utilizing an asymptotic description of how the rare event occurs, we derive algorithms that involve generating random variables only from the nominal distributions, thus avoiding any likelihood ratio. We consider two settings that occur frequently in applied probability: systems involving bottleneck elements and models involving heavytailed random variables. We first consider the problem of estimating P(X1 + {$\cdot$} {$\cdot$} {$\cdot$} + Xn {$>$} {$\gamma$} ), where X1, . . . , Xn are independent but not identically distributed (ind) heavy-tailed random variables. Guided by insights obtained from this model, we then study a variety of more general settings. Specifically, we consider a complex bridge network and a generalization of the widely popular normal copula model used in managing portfolio credit risk, both of which involve hundreds of random variables. We show that the same conditioning idea, guided by an asymptotic description of the way in which the rare event happens, can be used to derive estimators that outperform existing ones.},
	author = {Chan, Joshua C. C. and Kroese, Dirk P.},
	doi = {10.1007/s10479-009-0539-y},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\T7IPQFPY\\Chan et Kroese - 2011 - Rare-event probability estimation with conditional.pdf},
	issn = {0254-5330, 1572-9338},
	journal = {Annals of Operations Research},
	language = {en},
	month = sep,
	number = {1},
	pages = {43--61},
	title = {Rare-Event Probability Estimation with Conditional {{Monte Carlo}}},
	volume = {189},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10479-009-0539-y}}

@article{ChatterjeeDiaconis_SampleSizeRequired_2015,
	abstract = {The goal of importance sampling is to estimate the expected value of a given function with respect to a probability measure {$\nu$} using a random sample of size n drawn from a different probability measure \textmu. If the two measures \textmu{} and {$\nu$} are nearly singular with respect to each other, which is often the case in practice, the sample size required for accurate estimation is large. In this article it is shown that in a fairly general setting, a sample of size approximately exp(D({$\nu$}||\textmu )) is necessary and sufficient for accurate estimation by importance sampling, where D({$\nu$}||\textmu ) is the Kullback\textendash Leibler divergence of \textmu{} from {$\nu$}. In particular, the required sample size exhibits a kind of cut-off in the logarithmic scale. The theory is applied to obtain a general formula for the sample size required in importance sampling for one-parameter exponential families (Gibbs measures).},
	archiveprefix = {arXiv},
	author = {Chatterjee, Sourav and Diaconis, Persi},
	eprint = {1511.01437},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\9J6QENQC\\Chatterjee et Diaconis - 2015 - The sample size required in importance sampling.pdf},
	journal = {arXiv:1511.01437 [physics, stat]},
	keywords = {60F05,65C05,65C05; 65C60; 60F05; 82B80,65C60,82B80,Mathematics - Numerical Analysis,Mathematics - Probability,Mathematics - Statistics Theory,Physics - Data Analysis,Physics - Data Analysis; Statistics and Probability,Statistics and Probability},
	language = {en},
	month = nov,
	primaryclass = {physics, stat},
	title = {The Sample Size Required in Importance Sampling},
	year = {2015}}

@book{Constantine_ActiveSubspacesEmerging_2015,
	address = {{Philadelphia}},
	author = {Constantine, Paul G.},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\SR2H8M24\\Constantine - 2015 - Active subspaces emerging ideas for dimension red.pdf},
	isbn = {978-1-61197-385-3},
	keywords = {Function spaces,Functional analysis,Hilbert space,Linear topological spaces,Shift operators (Operator theory)},
	language = {en},
	lccn = {QA322 .C64 2015},
	number = {2},
	publisher = {{Society for Industrial and Applied Mathematics}},
	series = {{{SIAM}} Spotlights},
	shorttitle = {Active Subspaces},
	title = {Active Subspaces: Emerging Ideas for Dimension Reduction in Parameter Studies},
	year = {2015}}

@article{ConstantineDiaz_GlobalSensitivityMetrics_2017,
	abstract = {Predictions from science and engineering models depend on several input parameters. Global sensitivity analysis quantifies the importance of each input parameter, which can lead to insight into the model and reduced computational cost; commonly used sensitivity metrics include Sobol' total sensitivity indices and derivative-based global sensitivity measures. Active subspaces are part of an emerging set of tools for identifying important directions in a model's input parameter space; these directions can be exploited to reduce the model's dimension enabling otherwise infeasible parameter studies. In this paper, we develop global sensitivity metrics called activity scores from the active subspace, which yield insight into the important model parameters. We mathematically relate the activity scores to established sensitivity metrics, and we discuss computational methods to estimate the activity scores. We show two numerical examples with algebraic functions taken from simplified engineering models. For each model, we analyze the active subspace and discuss how to exploit the low-dimensional structure. We then show that input rankings produced by the activity scores are consistent with rankings produced by the standard metrics.},
	author = {Constantine, Paul G. and Diaz, Paul},
	doi = {10.1016/j.ress.2017.01.013},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\N3JSGPKD\\Constantine et Diaz - 2017 - Global sensitivity metrics from active subspaces.pdf},
	issn = {09518320},
	journal = {Reliability Engineering \& System Safety},
	language = {en},
	month = jun,
	pages = {1--13},
	title = {Global Sensitivity Metrics from Active Subspaces},
	volume = {162},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.ress.2017.01.013}}

@article{ConstantineEtAl_AcceleratingMCMCActive_2016,
	abstract = {The Markov chain Monte Carlo (MCMC) method is the computational workhorse for Bayesian inverse problems. However, MCMC struggles in high-dimensional parameter spaces, since its iterates must sequentially explore the high-dimensional space. This struggle is compounded in physical applications when the nonlinear forward model is computationally expensive. One approach to accelerate MCMC is to reduce the dimension of the state space. Active subspaces are part of an emerging set of tools for subspace-based dimension reduction. An active subspace in a given inverse problem indicates a separation between a low-dimensional subspace that is informed by the data and its orthogonal complement that is constrained by the prior. With this information, one can run the sequential MCMC on the active variables while sampling independently according to the prior on the inactive variables. However, this approach to increase efficiency may introduce bias. We provide a bound on the Hellinger distance between the true posterior and its active subspaceexploiting approximation. And we demonstrate the active subspace-accelerated MCMC on two computational examples: (i) a two-dimensional parameter space with a quadratic forward model and one-dimensional active subspace and (ii) a 100-dimensional parameter space with a PDE-based forward model and a two-dimensional active subspace.},
	archiveprefix = {arXiv},
	author = {Constantine, Paul G. and Kent, Carson and {Bui-Thanh}, Tan},
	doi = {10.1137/15M1042127},
	eprint = {1510.00024},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\XFCD8LFG\\Constantine et al. - 2016 - Accelerating MCMC with active subspaces.pdf},
	issn = {1064-8275, 1095-7197},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {Mathematics - Numerical Analysis,Statistics - Computation},
	language = {en},
	month = jan,
	number = {5},
	pages = {A2779-A2805},
	title = {Accelerating {{MCMC}} with Active Subspaces},
	volume = {38},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1137/15M1042127}}

@article{ConstantineEtAl_ActiveSubspaceMethods_2014,
	abstract = {Many multivariate functions in engineering models vary primarily along a few directions in the space of input parameters. When these directions correspond to coordinate directions, one may apply global sensitivity measures to determine the most influential parameters. However, these methods perform poorly when the directions of variability are not aligned with the natural coordinates of the input space. We present a method to first detect the directions of the strongest variability using evaluations of the gradient and subsequently exploit these directions to construct a response surface on a low-dimensional subspace\textemdash i.e., the active subspace\textemdash of the inputs. We develop a theoretical framework with error bounds, and we link the theoretical quantities to the parameters of a kriging response surface on the active subspace. We apply the method to an elliptic PDE model with coefficients parameterized by 100 Gaussian random variables and compare it with a local sensitivity analysis method for dimension reduction.},
	archiveprefix = {arXiv},
	author = {Constantine, Paul G. and Dow, Eric and Wang, Qiqi},
	doi = {10.1137/130916138},
	eprint = {1304.2070},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\VM3JLVQM\\Constantine et al. - 2014 - Active subspace methods in theory and practice ap.pdf},
	issn = {1064-8275, 1095-7197},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {65D10,Mathematics - Numerical Analysis},
	language = {en},
	month = jan,
	number = {4},
	pages = {A1500-A1524},
	shorttitle = {Active Subspace Methods in Theory and Practice},
	title = {Active Subspace Methods in Theory and Practice: Applications to Kriging Surfaces},
	volume = {36},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1137/130916138}}

@article{ConstantineEtAl_ComputingActiveSubspaces_2015,
	abstract = {Active subspaces are an emerging set of tools for identifying and exploiting the most important directions in the space of a computer simulation's input parameters; these directions depend on the simulation's quantity of interest, which we treat as a function from inputs to outputs. To identify a function's active subspace, one must compute the eigenpairs of a matrix derived from the function's gradient, which presents challenges when the gradient is not available as a subroutine. We numerically study two methods for estimating the necessary eigenpairs using only linear measurements of the function's gradient. In practice, these measurements can be estimated by finite differences using only two function evaluations, regardless of the dimension of the function's input space.},
	archiveprefix = {arXiv},
	author = {Constantine, Paul G. and Eftekhari, Armin and Wakin, Michael B.},
	eprint = {1506.04190},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\VBVKQ3F7\\Constantine et al. - 2015 - Computing Active Subspaces Efficiently with Gradie.pdf},
	journal = {arXiv:1506.04190 [math]},
	keywords = {Mathematics - Numerical Analysis},
	language = {en},
	month = jun,
	primaryclass = {math},
	title = {Computing {{Active Subspaces Efficiently}} with {{Gradient Sketching}}},
	year = {2015}}

@article{ConstantineGleich_ComputingActiveSubspaces_2014,
	abstract = {Active subspaces can effectively reduce the dimension of high-dimensional parameter studies enabling otherwise infeasible experiments with expensive simulations. The key components of active subspace methods are the eigenvectors of a symmetric, positive semidefinite matrix whose elements are the average products of partial derivatives of the simulation's input/output map. We study a Monte Carlo method for approximating the eigenpairs of this matrix. We offer both theoretical results based on recent non-asymptotic random matrix theory and a practical approach based on the bootstrap. We extend the analysis to the case when the gradients are approximated, for example, with finite differences. Our goal is to provide guidance for two questions that arise in active subspaces: (i) How many gradient samples does one need to accurately approximate the eigenvalues and subspaces? (ii) What can be said about the accuracy of the estimated subspace, both theoretically and practically? We test the approach on both simple quadratic functions where the active subspace is known and a parameterized PDE with 100 variables characterizing the coefficients of the differential operator.},
	archiveprefix = {arXiv},
	author = {Constantine, Paul and Gleich, David},
	eprint = {1408.0545},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\66S5QNLL\\Constantine et Gleich - 2014 - Computing active subspaces with Monte Carlo.pdf},
	journal = {arXiv:1408.0545 [math]},
	keywords = {Mathematics - Numerical Analysis},
	language = {en},
	month = aug,
	primaryclass = {math},
	title = {Computing Active Subspaces with {{Monte Carlo}}},
	year = {2014}}

@article{CornuetEtAl_AdaptiveMultipleImportance_2012,
	abstract = {The Adaptive Multiple Importance Sampling algorithm is aimed at an optimal recycling of past simulations in an iterated importance sampling (IS) scheme. The difference with earlier adaptive IS implementations like Population Monte Carlo is that the importance weights of all simulated values, past as well as present, are recomputed at each iteration, following the technique of the deterministic multiple mixture estimator of Owen \& Zhou (J. Amer. Statist. Assoc., 95, 2000, 135). Although the convergence properties of the algorithm cannot be investigated, we demonstrate through a challenging banana shape target distribution and a population genetics example that the improvement brought by this technique is substantial.},
	author = {Cornuet, Jean-Marie and Marin, Jean-Michel and Mira, Antonietta and Robert, Christian P.},
	doi = {10.1111/j.1467-9469.2011.00756.x},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\MTVRS39W\\Cornuet et al. - 2012 - Adaptive Multiple Importance Sampling iAdaptive.pdf},
	issn = {03036898},
	journal = {Scandinavian Journal of Statistics},
	language = {en},
	month = dec,
	number = {4},
	pages = {798--812},
	shorttitle = {Adaptive {{Multiple Importance Sampling}}},
	title = {Adaptive {{Multiple Importance Sampling}}: {{{\emph{Adaptive}}}}{\emph{ Multiple Importance Sampling}}},
	volume = {39},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1111/j.1467-9469.2011.00756.x}}

@article{CostaEtAl_ConvergencePropertiesCrossentropy_2007,
	abstract = {We present new theoretical convergence results on the Cross-Entropy method for discrete optimization. Our primary contribution is to show that a popular implementation of the Cross-Entropy method converges, and finds the optimal solution with probability arbitrarily close to 1. We also give necessary conditions and sufficient conditions under which the optimal solution is generated eventually with probability 1.},
	author = {Costa, Andre and Jones, Owen Dafydd and Kroese, Dirk},
	doi = {10.1016/j.orl.2006.11.005},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\9R2QUW9D\\Costa et al. - 2007 - Convergence properties of the cross-entropy method.pdf},
	issn = {01676377},
	journal = {Operations Research Letters},
	language = {en},
	month = sep,
	number = {5},
	pages = {573--580},
	title = {Convergence Properties of the Cross-Entropy Method for Discrete Optimization},
	volume = {35},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.orl.2006.11.005}}

@article{Dambreville_CrossEntropyMethodConvergence_,
	abstract = {The cross-entropy method (CE) developed by R. Rubinstein is an elegant practical principle for simulating rare events. The method approximates the probability of the rare event by means of a family of probabilistic models. The method has been extended to optimization, by considering an optimal event as a rare event. CE works rather good when dealing with deterministic function optimization. Now, it appears that two conditions are needed for a good convergence of the method. First, it is necessary to have a family of models sufficiently flexible for discriminating the optimal events. Indirectly, it appears also that the function to be optimized should be deterministic. The purpose of this paper is to consider the case of partially discriminating model family, and of stochastic functions. It will be shown on simple examples that the CE could fail when relaxing these hypotheses. Alternative improvements of the CE method are investigated and compared on random examples in order to handle this issue.},
	author = {Dambreville, Frederic},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\EMDQYJTZ\\Dambreville - Cross-Entropy method convergence issues for exten.pdf},
	language = {en},
	pages = {12},
	title = {Cross-{{Entropy}} Method: Convergence Issues for Extended Implementation}}

@article{deBoerEtAl_TutorialCrossEntropyMethod_2005,
	abstract = {The cross-entropy (CE) method is a new generic approach to combinatorial and multi-extremal optimization and rare event simulation. The purpose of this tutorial is to give a gentle introduction to the CE method. We present the CE methodology, the basic algorithm and its modifications, and discuss applications in combinatorial optimization and machine learning.},
	author = {{de Boer}, Pieter-Tjerk and Kroese, Dirk P. and Mannor, Shie and Rubinstein, Reuven Y.},
	doi = {10.1007/s10479-005-5724-z},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\CDAY4Y5S\\de Boer et al. - 2005 - A Tutorial on the Cross-Entropy Method.pdf},
	issn = {0254-5330, 1572-9338},
	journal = {Annals of Operations Research},
	language = {en},
	month = feb,
	number = {1},
	pages = {19--67},
	title = {A {{Tutorial}} on the {{Cross}}-{{Entropy Method}}},
	volume = {134},
	year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10479-005-5724-z}}

@article{DerKiureghianothers_FirstandSecondorderReliability_2005,
	author = {Der Kiureghian, Armen and others},
	journal = {Engineering design reliability handbook},
	publisher = {{CRC Press Boca Raton, FL}},
	title = {First-and Second-Order Reliability Methods},
	volume = {14},
	year = {2005}}

@book{DitlevsenMadsen_StructuralReliabilityMethods_1996,
	address = {{Chichester ; New York}},
	author = {Ditlevsen, Ove and Madsen, H. O.},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\C6L2SS3L\\Ditlevsen et Madsen - 1996 - Structural reliability methods.pdf},
	isbn = {978-0-471-96086-7},
	keywords = {Reliability (Engineering),Structural stability},
	language = {en},
	lccn = {TA656 .D5713 1996},
	publisher = {{Wiley}},
	title = {Structural Reliability Methods},
	year = {1996}}

@article{DoucEtAl_ConvergenceAdaptiveMixtures_2007,
	abstract = {In the design of efficient simulation algorithms, one is often beset with a poor choice of proposal distributions. Although the performance of a given simulation kernel can clarify a posteriori how adequate this kernel is for the problem at hand, a permanent on-line modification of kernels causes concerns about the validity of the resulting algorithm. While the issue is most often intractable for MCMC algorithms, the equivalent version for importance sampling algorithms can be validated quite precisely. We derive sufficient convergence conditions for adaptive mixtures of population Monte Carlo algorithms and show that Rao--Blackwellized versions asymptotically achieve an optimum in terms of a Kullback divergence criterion, while more rudimentary versions do not benefit from repeated updating.},
	archiveprefix = {arXiv},
	author = {Douc, R. and Guillin, A. and Marin, J.-M. and Robert, C. P.},
	doi = {10.1214/009053606000001154},
	eprint = {0708.0711},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\YUVHTFAL\\Douc et al. - 2007 - Convergence of adaptive mixtures of importance sam.pdf},
	issn = {0090-5364},
	journal = {The Annals of Statistics},
	keywords = {60F05,60F05; 62L12; 65-04; 65C05; 65C40; 65C60 (Primary),62L12,65-04,65C05,65C40,65C60 (Primary),Mathematics - Statistics Theory,Statistics - Computation},
	language = {en},
	month = feb,
	number = {1},
	pages = {420--448},
	title = {Convergence of Adaptive Mixtures of Importance Sampling Schemes},
	volume = {35},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1214/009053606000001154}}

@article{DoucEtAl_MinimumVarianceImportance_2007,
	author = {Douc, R. and Guillin, A. and Marin, J.-M. and Robert, C. P.},
	doi = {10.1051/ps:2007028},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\VQ8H6SH8\\Douc et al. - 2007 - Minimum variance importance sampling iviai Po.pdf},
	issn = {1292-8100, 1262-3318},
	journal = {ESAIM: Probability and Statistics},
	month = aug,
	pages = {427--447},
	title = {Minimum Variance Importance Sampling {\emph{via}} {{Population Monte Carlo}}},
	volume = {11},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1051/ps:2007028}}

@inproceedings{DurrieuEtAl_LowerUpperBounds_2012,
	abstract = {Many speech technology systems rely on Gaussian Mixture Models (GMMs). The need for a comparison between two GMMs arises in applications such as speaker verification, model selection or parameter estimation. For this purpose, the Kullback-Leibler (KL) divergence is often used. However, since there is no closed form expression to compute it, it can only be approximated. We propose lower and upper bounds for the KL divergence, which lead to a new approximation and interesting insights into previously proposed approximations. An application to the comparison of speaker models also shows how such approximations can be used to validate assumptions on the models.},
	address = {{Kyoto, Japan}},
	author = {Durrieu, J.-L. and Thiran, J.-Ph. and Kelly, F.},
	booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
	doi = {10.1109/ICASSP.2012.6289001},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\KDQ25PLT\\Durrieu et al. - 2012 - Lower and upper bounds for approximation of the Ku.pdf},
	isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
	language = {en},
	month = mar,
	pages = {4833--4836},
	publisher = {{IEEE}},
	title = {Lower and Upper Bounds for Approximation of the {{Kullback}}-{{Leibler}} Divergence between {{Gaussian Mixture Models}}},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICASSP.2012.6289001}}

@article{EgloffLeippold_QuantileEstimationAdaptive_2010,
	author = {Egloff, Daniel and Leippold, Markus},
	doi = {10.1214/09-AOS745},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\5SRM7GRR\\Egloff et Leippold - 2010 - Quantile estimation with adaptive importance sampl.pdf},
	issn = {0090-5364},
	journal = {The Annals of Statistics},
	language = {en},
	month = apr,
	number = {2},
	pages = {1244--1278},
	title = {Quantile Estimation with Adaptive Importance Sampling},
	volume = {38},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1214/09-AOS745}}

@article{EguchiCopas_InterpretingKullbackLeibler_2006,
	abstract = {Kullback\textendash Leibler divergence and the Neyman\textendash Pearson lemma are two fundamental concepts in statistics. Both are about likelihood ratios: Kullback\textendash Leibler divergence is the expected log-likelihood ratio, and the Neyman\textendash Pearson lemma is about error rates of likelihood ratio tests. Exploring this connection gives another statistical interpretation of the Kullback\textendash Leibler divergence in terms of the loss of power of the likelihood ratio test when the wrong distribution is used for one of the hypotheses. In this interpretation, the standard non-negativity property of the Kullback\textendash Leibler divergence is essentially a restatement of the optimal property of likelihood ratios established by the Neyman\textendash Pearson lemma. The asymmetry of Kullback\textendash Leibler divergence is overviewed in information geometry.},
	author = {Eguchi, Shinto and Copas, John},
	doi = {10.1016/j.jmva.2006.03.007},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\P85K3FG2\\Eguchi et Copas - 2006 - Interpreting Kullback--Leibler divergence with the .pdf},
	issn = {0047259X},
	journal = {Journal of Multivariate Analysis},
	language = {en},
	month = oct,
	number = {9},
	pages = {2034--2040},
	title = {Interpreting {{Kullback}}\textendash{{Leibler}} Divergence with the {{Neyman}}\textendash{{Pearson}} Lemma},
	volume = {97},
	year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.jmva.2006.03.007}}


@inproceedings{El-LahamEtAl_RecursiveShrinkageCovariance_,
  title={Recursive shrinkage covariance learning in adaptive importance sampling},
  author={El-Laham, Yousef and Elvira, V{\'\i}ctor and Bugallo, M{\'o}nica},
  booktitle={2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)},
  pages={624--628},
  year={2019},
  organization={IEEE},
  doi={10.1109/CAMSAP45676.2019.9022450}
}

@article{El-LahamEtAl_RobustCovarianceAdaptation_2018,
	abstract = {Importance sampling (IS) is a Monte Carlo methodology that allows for the approximation of a target distribution using weighted samples generated from another proposal distribution. Adaptive importance sampling (AIS) implements an iterative version of IS, which adapts the parameters of the proposal distribution in order to improve estimation of the target. While the adaptation of the location (mean) of the proposals has been largely studied, an important challenge of AIS relates to the difficulty of adapting the scale parameter (covariance matrix). In the case of weight degeneracy, adapting the covariance matrix using the empirical covariance results in a singular matrix, which leads to a poor performance in subsequent iterations of the algorithm. In this letter, we propose a novel scheme which exploits recent advances in the IS literature to prevent the so-called weight degeneracy. The method efficiently adapts the covariance matrix of a population of proposal distributions and achieves a significant performance improvement in high-dimensional scenarios. We validate the new method through computer simulations.},
	author = {{El-Laham}, Yousef and Elvira, Victor and Bugallo, Monica F.},
	doi = {10.1109/LSP.2018.2841641},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\DYAT924Y\\El-Laham et al. - 2018 - Robust Covariance Adaptation in Adaptive Importanc.pdf},
	issn = {1070-9908, 1558-2361},
	journal = {IEEE Signal Processing Letters},
	language = {en},
	month = jul,
	number = {7},
	pages = {1049--1053},
	title = {Robust {{Covariance Adaptation}} in {{Adaptive Importance Sampling}}},
	volume = {25},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/LSP.2018.2841641}}

@article{ElviraEtAl_GeneralizedMultipleImportance_2019,
	abstract = {Importance sampling (IS) methods are broadly used to approximate posterior distributions or their moments. In the standard IS approach, samples are drawn from a single proposal distribution and weighted adequately. However, since the performance in IS depends on the mismatch between the targeted and the proposal distributions, several proposal densities are often employed for the generation of samples. Under this multiple importance sampling (MIS) scenario, extensive literature has addressed the selection and adaptation of the proposal distributions, interpreting the sampling and weighting steps in different ways. In this paper, we establish a novel general framework with sampling and weighting procedures when more than one proposal is available. The new framework encompasses most relevant MIS schemes in the literature, and novel valid schemes appear naturally. All the MIS schemes are compared and ranked in terms of the variance of the associated estimators. Finally, we provide illustrative examples revealing that, even with a good choice of the proposal densities, a careful interpretation of the sampling and weighting procedures can make a significant difference in the performance of the method.},
	archiveprefix = {arXiv},
	author = {Elvira, V{\'\i}ctor and Martino, Luca and Luengo, David and Bugallo, M{\'o}nica F.},
	doi = {10.1214/18-STS668},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\8EBM642Q\\Elvira et al. - 2019 - Generalized Multiple Importance Sampling.pdf},
	issn = {0883-4237},
	journal = {Statistical Science},
	keywords = {Statistics - Computation},
	language = {en},
	month = feb,
	number = {1},
	pages = {129--155},
	title = {Generalized {{Multiple Importance Sampling}}},
	volume = {34},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1214/18-STS668}}

@inproceedings{ElviraEtAl_GradientAdaptivePopulation_2015,
	abstract = {Monte Carlo (MC) methods are widely used in signal pro\- cessing and machine learning. A well-known class of MC methods is composed of importance sampling and its adap\- tive extensions (e.g., population Monte Carlo). In this paper, we introduce an adaptive importance sampler using a popula\- tion of proposal densities. The novel algorithm dynamically optimizes the cloud of proposals, adapting them using infor\- mation about the gradient and Hessian matrix of the target distribution. Moreover, a new kind of interaction in the adap\- tation of the proposal densities is introduced, establishing a trade-off between attaining a good performance in terms of mean square error and robustness to initialization.},
	address = {{South Brisbane, Queensland, Australia}},
	author = {Elvira, Victor and Martino, Luca and Luengo, David and Corander, Jukka},
	booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
	doi = {10.1109/ICASSP.2015.7178737},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\HX7B7LKK\\Elvira et al. - 2015 - A gradient adaptive population importance sampler.pdf},
	isbn = {978-1-4673-6997-8},
	language = {en},
	month = apr,
	pages = {4075--4079},
	publisher = {{IEEE}},
	title = {A Gradient Adaptive Population Importance Sampler},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICASSP.2015.7178737}}

@article{ElviraEtAl_ImprovingPopulationMonte_2016,
	abstract = {Population Monte Carlo (PMC) sampling methods are powerful tools for approximating distributions of static unknowns given a set of observations. These methods are iterative in nature: at each step they generate samples from a proposal distribution and assign them weights according to the importance sampling principle. Critical issues in applying PMC methods are the choice of the generating functions for the samples and the avoidance of the sample degeneracy. In this paper, we propose three new schemes that considerably improve the performance of the original PMC formulation by allowing for better exploration of the space of unknowns and by selecting more adequately the surviving samples. A theoretical analysis is performed, proving the superiority of the novel schemes in terms of variance of the associated estimators and preservation of the sample diversity. Furthermore, we show that they outperform other state of the art algorithms (both in terms of mean square error and robustness w.r.t. initialization) through extensive numerical simulations.},
	archiveprefix = {arXiv},
	author = {Elvira, V{\'\i}ctor and Martino, Luca and Luengo, David and Bugallo, M{\'o}nica F.},
	eprint = {1607.02758},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\8DI64BSU\\Elvira et al. - 2016 - Improving Population Monte Carlo Alternative Weig.pdf},
	journal = {arXiv:1607.02758 [stat]},
	keywords = {Statistics - Computation},
	language = {en},
	month = jul,
	primaryclass = {stat},
	shorttitle = {Improving {{Population Monte Carlo}}},
	title = {Improving {{Population Monte Carlo}}: {{Alternative Weighting}} and {{Resampling Schemes}}},
	year = {2016}}

@article{ElviraSantamaria_MultipleImportanceSampling_2019,
	abstract = {Digital constellations formed by hexagonal or other non-square two-dimensional lattices are often used in advanced digital communication systems. The integrals required to evaluate the symbol error rate (SER) of these constellations in the presence of Gaussian noise are in general difficult to compute in closed form, and therefore Monte Carlo simulation is typically used to estimate the SER. However, naive Monte Carlo simulation can be very inefficient and requires very long simulation runs, especially at high signal-to-noise ratios. In this letter, we adapt a recently proposed multiple importance sampling technique, called ALOE (for ``at least one rare event''), to this problem. Conditioned to a transmitted symbol, an error (or rare event) occurs when the observation falls in a union of half-spaces or, equivalently, outside a given polytope. The proposal distribution for ALOE samples the system conditionally on an error taking place, which makes it more efficient than other importance sampling techniques. ALOE provides unbiased SER estimates with simulation times orders of magnitude shorter than conventional Monte Carlo.},
	author = {Elvira, Victor and Santamaria, Ignacio},
	doi = {10.1109/LSP.2019.2892835},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\ZCSWEWBA\\Elvira et Santamaria - 2019 - Multiple Importance Sampling for Efficient Symbol .pdf},
	issn = {1070-9908, 1558-2361},
	journal = {IEEE Signal Processing Letters},
	language = {en},
	month = mar,
	number = {3},
	pages = {420--424},
	title = {Multiple {{Importance Sampling}} for {{Efficient Symbol Error Rate Estimation}}},
	volume = {26},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/LSP.2019.2892835}}

@article{Gallimard_AdaptiveReducedBasis_2019,
	abstract = {Monte Carlo methods are well suited to characterize events of which associated probabilities are not too low with respect to the simulation budget. For very seldom observed events, these approaches do not lead to accurate results. Indeed, the number of samples is often insufficient to estimate such low probabilities (at least 10n+2 samples are needed to estimate a probability of 10-n with 10\% relative deviation of the Monte Carlo estimator). Even within the framework of reduced order methods, such as a reduced basis approach, it seems difficult to predict accurately low-probability events. In this paper, we propose to combine a cross-entropy method with a reduced basis algorithm to compute rare-event (failure) probabilities.},
	author = {Gallimard, L.},
	doi = {10.1002/nme.6135},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\CIEQA4T8\\Gallimard - 2019 - Adaptive reduced basis strategy for rare‐event sim.pdf},
	issn = {0029-5981, 1097-0207},
	journal = {International Journal for Numerical Methods in Engineering},
	language = {en},
	month = oct,
	number = {3},
	pages = {283--302},
	title = {Adaptive Reduced Basis Strategy for Rare-event Simulations},
	volume = {120},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1002/nme.6135}}

@article{GeyerEtAl_CrossEntropybasedImportance_2019,
	abstract = {The computation of the probability of a rare (failure) event is a common task in structural reliability analysis. In most applications, the numerical model defining the rare event is nonlinear and the resulting failure domain often multimodal. One strategy for estimating the probability of failure in this context is the importance sampling method. The efficiency of importance sampling depends on the choice of the importance sampling density. A near-optimal sampling density can be found through application of the cross entropy method. The cross entropy method is an adaptive sampling approach that determines the sampling density through minimizing the Kullback-Leibler divergence between the theoretically optimal importance sampling density and a chosen parametric family of distributions. In this paper, we investigate the suitability of the multivariate normal distribution and the Gaussian mixture model as importance sampling densities within the cross entropy method. Moreover, we compare the performance of the cross entropy method to sequential importance sampling, another recently proposed adaptive sampling approach, which uses the Gaussian mixture distribution as a proposal distribution within a Markov Chain Monte Carlo algorithm. For the parameter updating of the Gaussian mixture within the cross entropy method, we propose a modified version of the expectation-maximization algorithm that works with weighted samples. To estimate the number of distributions in the mixture, the density-based spatial clustering of applications with noise (DBSCAN) algorithm is adapted to the use of weighted samples. We compare the performance of the different methods in several examples, including component reliability problems, system reliability problems and reliability in varying dimensions. The results show that the cross entropy method using a single Gaussian outperforms the cross entropy method using Gaussian mixture and that both distribution types are not suitable for high dimensional reliability problems.},
	author = {Geyer, Sebastian and Papaioannou, Iason and Straub, Daniel},
	doi = {10.1016/j.strusafe.2018.07.001},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\ZQ95CIVP\\Geyer et al. - 2019 - Cross entropy-based importance sampling using Gaus.pdf},
	issn = {01674730},
	journal = {Structural Safety},
	language = {en},
	month = jan,
	pages = {15--27},
	title = {Cross Entropy-Based Importance Sampling Using {{Gaussian}} Densities Revisited},
	volume = {76},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.strusafe.2018.07.001}}

@article{GlassermanEtAl_PortfolioValueatRiskHeavyTailed_2002,
	author = {Glasserman, Paul and Heidelberger, Philip and Shahabuddin, Perwez},
	doi = {10.1111/1467-9965.00141},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\ZETKIUZI\\Glasserman et al. - 2002 - Portfolio Value-at-Risk with Heavy-Tailed Risk Fac.pdf},
	issn = {0960-1627, 1467-9965},
	journal = {Mathematical Finance},
	language = {en},
	month = jul,
	number = {3},
	pages = {239--269},
	title = {Portfolio {{Value}}-at-{{Risk}} with {{Heavy}}-{{Tailed Risk Factors}}},
	volume = {12},
	year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1111/1467-9965.00141}}

@article{GlawsEtAl_DimensionReductionMagnetohydrodynamics_2017,
	author = {Glaws, Andrew and Constantine, Paul G. and Shadid, John N. and Wildey, Timothy M.},
	doi = {10.1002/sam.11355},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\JWMZ3YXS\\Glaws et al. - 2017 - Dimension reduction in magnetohydrodynamics power .pdf},
	issn = {19321864},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	language = {en},
	month = oct,
	number = {5},
	pages = {312--325},
	shorttitle = {Dimension Reduction in Magnetohydrodynamics Power Generation Models},
	title = {Dimension Reduction in Magnetohydrodynamics Power Generation Models: {{Dimensional}} Analysis and Active Subspaces: {{GLAWS}} {\textsc{et Al.}}},
	volume = {10},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1002/sam.11355}}

@article{Glynn_IMPORTANCESAMPLINGMONTE_,
	abstract = {This paper is concerned with applying importance sampling as a variance reduction tool for computing extreme quantiles. A central limit theorem is derived for each of four proposed importance sampling quantile estimators. Efficiency comparisons are provided in a certain asymptotic setting, using ideas from large deviation theory.},
	author = {Glynn, Peter W},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\UY2FJ34G\\Glynn - IMPORTANCE SAMPLING FOR MONTE CARLO ESTIMATION OF .pdf},
	language = {en},
	pages = {7},
	title = {{{IMPORTANCE SAMPLING FOR MONTE CARLO ESTIMATION OF QUANTILES}}}}

@article{GoguEtAl_FocusEvenementsRares_,
	abstract = {Engineers increasingly use numerical model to replace the experimentations during the design of new products. With the increase of computer performance and numerical power, these models are more and more complex and time-consuming for a better representation of reality. In practice, optimization is very challenging when considering real mechanical problems since they exhibit uncertainties. Reliability is an interesting metric of the failure risks of design products due to uncertainties. The estimation of this metric, the failure probability, requires a high number of evaluations of the time-consuming model and thus becomes intractable in practice. To deal with this problem, surrogate modeling is used here and more specifically AK-based methods to enable the approximation of the physical model with much fewer time-consuming evaluations. The first objective of this thesis work is to discuss the mathematical formulations of design problems under uncertainties. This formulation has a considerable impact on the solution identified by the optimization during design process of new products. A definition of both concepts of reliability and robustness is also proposed. These works are presented in a publication in the international journal : Structural and Multidisciplinary Optimization (Leli\`evre et al., 2016a). The second objective of this thesis is to propose a new AK-based method to estimate failure probabilities associated with rare events. This new method, named AK-MCSi, presents three enhancements of AK-MCS : (i) sequential Monte Carlo simulations to reduce the time associated with the evaluation of the surrogate model, (ii) a new stricter stopping criterion on learning evaluations to ensure the good classification of the Monte Carlo population and (iii) a multipoints enrichment permitting the parallelization of the evaluation of the time-consuming model. This work has been published in Structural Safety (Leli\`evre et al., 2018a). The last objective of this thesis is to propose new AK-based methods to estimate the failure probability of a high-dimensional reliability problem, i.e. a problem defined by both a time-consuming model and a high number of input random variables. Two new methods, AK-HDMR1 and AK-PCA, are proposed to deal with this problem based on respectively a functional decomposition and a dimensional reduction technique. AK-HDMR1 has been submitted to Reliability Enginnering and Structural Safety on 1st October 2018.},
	author = {Gogu, M Christian and Morio, M J{\'e}r{\^o}me and Helbert, Mme C{\'e}line and Riche, M Rodolphe Le and Sudret, M Bruno and Gayton, M Nicolas and Beaurepaire, M Pierre and Mattrand, Mme C{\'e}cile and Bourinet, M Jean-Marc and {de Conf{\'e}rences}, Ma{\^\i}tre},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\EAYWYLHM\\Gogu et al. - Focus sur les {\'e}v{\`e}nements rares et la grande dimens.pdf},
	language = {fr},
	pages = {160},
	title = {{Focus sur les \'ev\`enements rares et la grande dimension}}}

@article{GraceEtAl_AutomatedStateDependentImportance_2014,
	abstract = {Many complex systems can be modeled via Markov jump processes. Applications include chemical reactions, population dynamics, and telecommunication networks. Rare-event estimation for such models can be difficult and is often computationally expensive, because typically many (or very long) paths of the Markov jump process need to be simulated in order to observe the rare event. We present a state-dependent importance sampling approach to this problem that is adaptive and uses Markov chain Monte Carlo to sample from the zero-variance importance sampling distribution. The method is applicable to a wide range of Markov jump processes and achieves high accuracy, while requiring only a small sample to obtain the importance parameters. We demonstrate its efficiency through benchmark examples in queueing theory and stochastic chemical kinetics.},
	author = {Grace, Adam W. and Kroese, Dirk P. and Sandmann, Werner},
	doi = {10.1239/jap/1409932671},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\EL966TFT\\Grace et al. - 2014 - Automated State-Dependent Importance Sampling for .pdf},
	issn = {0021-9002, 1475-6072},
	journal = {Journal of Applied Probability},
	language = {en},
	month = sep,
	number = {3},
	pages = {741--755},
	title = {Automated {{State}}-{{Dependent Importance Sampling}} for {{Markov Jump Processes}} via {{Sampling}} from the {{Zero}}-{{Variance Distribution}}},
	volume = {51},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1239/jap/1409932671}}

@article{Guerra_OptimisationMultiobjectifSous_,
	author = {Guerra, Jonathan},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\7HYVZRY4\\Guerra - Optimisation multi-objectif sous incertitudes de p.pdf},
	pages = {305},
	title = {Optimisation Multi-Objectif Sous Incertitudes de Ph\'enom\`enes de Thermique Transitoire}}

@article{HakhamaneshiEtAl_GACEMGeneralizedAutoregressive_2020,
	abstract = {In this work we present a new method of blackbox optimization and constraint satisfaction. Existing algorithms that have attempted to solve this problem are unable to consider multiple modes, and are not able to adapt to changes in environment dynamics. To address these issues, we developed a modified Cross-Entropy Method (CEM) that uses a masked auto-regressive neural network for modeling uniform distributions over the solution space. We train the model using maximum entropy policy gradient methods from Reinforcement Learning. Our algorithm is able to express complicated solution spaces, thus allowing it to track a variety of different solution regions. We empirically compare our algorithm with variations of CEM, including one with a Gaussian prior with fixed variance, and demonstrate better performance in terms of: number of diverse solutions, better mode discovery in multi-modal problems, and better sample efficiency in certain cases.},
	archiveprefix = {arXiv},
	author = {Hakhamaneshi, Kourosh and Settaluri, Keertana and Abbeel, Pieter and Stojanovic, Vladimir},
	eprint = {2002.07236},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\UQH82T39\\Hakhamaneshi et al. - 2020 - GACEM Generalized Autoregressive Cross Entropy Me.pdf},
	journal = {arXiv:2002.07236 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
	language = {en},
	month = feb,
	primaryclass = {cs, stat},
	shorttitle = {{{GACEM}}},
	title = {{{GACEM}}: {{Generalized Autoregressive Cross Entropy Method}} for {{Multi}}-{{Modal Black Box Constraint Satisfaction}}},
	year = {2020}}

@inproceedings{HersheyOlsen_ApproximatingKullbackLeibler_2007,
	abstract = {The Kullback Leibler (KL) Divergence is a widely used tool in statistics and pattern recognition. The KL divergence between two Gaussian Mixture Models (GMMs) is frequently needed in the fields of speech and image recognition. Unfortunately the KL divergence between two GMMs is not analytically tractable, nor does any efficient computational algorithm exist. Some techniques cope with this problem by replacing the KL divergence with other functions that can be computed efficiently. We introduce two new methods, the variational approximation and the variational upper bound, and compare them to existing methods. We discuss seven different techniques in total and weigh the benefits of each one against the others. To conclude we evaluate the performance of each one through numerical experiments.},
	address = {{Honolulu, HI}},
	author = {Hershey, John R. and Olsen, Peder A.},
	booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
	doi = {10.1109/ICASSP.2007.366913},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\7QLNFJZE\\Hershey et Olsen - 2007 - Approximating the Kullback Leibler Divergence Betw.pdf},
	isbn = {978-1-4244-0727-9},
	language = {en},
	month = apr,
	pages = {IV-317-IV-320},
	publisher = {{IEEE}},
	title = {Approximating the {{Kullback Leibler Divergence Between Gaussian Mixture Models}}},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICASSP.2007.366913}}

@article{HohenbichlerRackwitz_NonNormalDependentVectors_1981,
	abstract = {A general probability distribution transformation is developed with which complex structural reliability problems involving non-normal, dependent uncertainty vectors can be reduced to the standard case of first-order-reliability, i.e. the problem of determining the failure probability or the reliability index isn the space of independent, standard normal variates. The method requires the knowledge of the joint cumulative distribution function or a certain set of conditional distribution functions of the original vector. Some basic properties of the transformation are discussed. Details of the transformation technique are given. Approximations must be introduced for the shape of the safe domain such that its probability content can easily be evaluated which may involve numerical inversion of distribution functions. A suitable algorithm for computing reliability measures is proposed. The field of potential applications is indicated by a number of examples.},
	annotation = {\_eprint: https://ascelibrary.org/doi/pdf/10.1061/JMCEA3.0002777},
	author = {Hohenbichler, Michael and Rackwitz, R{\"u}diger},
	doi = {10.1061/JMCEA3.0002777},
	journal = {Journal of the Engineering Mechanics Division},
	number = {6},
	pages = {1227--1238},
	title = {Non-{{Normal Dependent Vectors}} in {{Structural Safety}}},
	volume = {107},
	year = {1981},
	Bdsk-Url-1 = {https://doi.org/10.1061/JMCEA3.0002777}}

@article{HolodnakEtAl_ProbabilisticSubspaceBound_2018,
	abstract = {Given a real symmetric positive semi-definite matrix E, and an approximation S that is a sum of n independent matrix-valued random variables, we present bounds on the relative error in S due to randomization. The bounds do not depend on the matrix dimensions but only on the numerical rank (intrinsic dimension) of E. Our approach resembles the low-rank approximation of kernel matrices from random features, but our accuracy measures are more stringent.},
	archiveprefix = {arXiv},
	author = {Holodnak, John T. and Ipsen, Ilse C. F. and Smith, Ralph C.},
	eprint = {1801.00682},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\8IJ9S9AU\\Holodnak et al. - 2018 - A Probabilistic Subspace Bound with Application to.pdf},
	journal = {arXiv:1801.00682 [math]},
	keywords = {15A18,15A18; 15A23; 15A60; 15B10; 35J25; 60G60; 65N30; 65C06; 65C30; 65F15; 65D05,15A23,15A60,15B10,35J25,60G60,65C06,65C30,65D05,65F15,65N30,Mathematics - Numerical Analysis},
	language = {en},
	month = jan,
	primaryclass = {math},
	title = {A {{Probabilistic Subspace Bound}} with {{Application}} to {{Active Subspaces}}},
	year = {2018}}

@article{Homem-de-Mello_StudyCrossEntropyMethod_2007,
	author = {{Homem-de-Mello}, Tito},
	doi = {10.1287/ijoc.1060.0176},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\5QXE8CW9\\Homem-de-Mello - 2007 - A Study on the Cross-Entropy Method for Rare-Event.pdf},
	issn = {1091-9856, 1526-5528},
	journal = {INFORMS Journal on Computing},
	language = {en},
	month = aug,
	number = {3},
	pages = {381--394},
	title = {A {{Study}} on the {{Cross}}-{{Entropy Method}} for {{Rare}}-{{Event Probability Estimation}}},
	volume = {19},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1287/ijoc.1060.0176}}

@article{Homem-de-MelloRubinstein_RareEventEstimation_2002,
	author = {{Homem-de-Mello}, Tito and Rubinstein, Reuven Y.},
	title = {Rare {{Event Estimation}} for {{Static Models}} via {{Cross}}-{{Entropy}} and {{Importance Sampling}}},
	year = {2002}}

@article{HongEtAl_MonteCarloMethods_2014,
	author = {Hong, L. Jeff and Hu, Zhaolin and Liu, Guangwu},
	doi = {10.1145/2661631},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\568HG9IS\\Hong et al. - 2014 - Monte Carlo Methods for Value-at-Risk and Conditio.pdf},
	issn = {10493301},
	journal = {ACM Transactions on Modeling and Computer Simulation},
	language = {en},
	month = nov,
	number = {4},
	pages = {1--37},
	shorttitle = {Monte {{Carlo Methods}} for {{Value}}-at-{{Risk}} and {{Conditional Value}}-at-{{Risk}}},
	title = {Monte {{Carlo Methods}} for {{Value}}-at-{{Risk}} and {{Conditional Value}}-at-{{Risk}}: {{A Review}}},
	volume = {24},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1145/2661631}}

@inproceedings{HongLiu_MonteCarloEstimation_2011,
	abstract = {Value-at-risk and conditional value at risk are two widely used risk measures, employed in the financial industry for risk management purposes. This tutorial discusses Monte Carlo methods for estimating valueat-risk, conditional value-at-risk and their sensitivities. By relating the mathematical representation of value-at-risk to that of conditional value-at-risk, it provides a unified view of simulation methodologies for both risk measures and their sensitivities.},
	address = {{Phoenix, AZ, USA}},
	author = {Hong, L. Jeff and Liu, Guangwu},
	booktitle = {Proceedings of the 2011 {{Winter Simulation Conference}} ({{WSC}})},
	doi = {10.1109/WSC.2011.6147743},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\A7YJQRCR\\Hong et Liu - 2011 - Monte Carlo estimation of value-at-risk, condition.pdf},
	isbn = {978-1-4577-2109-0 978-1-4577-2108-3 978-1-4577-2106-9 978-1-4577-2107-6},
	language = {en},
	month = dec,
	pages = {95--107},
	publisher = {{IEEE}},
	title = {Monte {{Carlo}} Estimation of Value-at-Risk, Conditional Value-at-Risk and Their Sensitivities},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/WSC.2011.6147743}}

@article{HuangEtAl_OverviewStructuralReliability_2016,
	author = {Huang, ChangWu and El Hami, Abdelkhalak and Radi, Boucha{\"\i}b},
	journal = {Incertitudes et fiabilit\'e des syst\`emes multiphysiques, Volume 17},
	title = {Overview of {{Structural Reliability Analysis Methods}}},
	year = {2016}}

@article{HuEtAl_MultidisciplinaryOptimizationHighDimensional_2016,
	abstract = {The popular use of response surface methodology accelerate the solutions of parameter identification and response analysis issues. However, accurate RSM models subject to aleatory and epistemic uncertainties are still challenging to construct, especially for multidimensional inputs, which is widely existed in real-world problems. In this study, an adaptive interval response surface methodology (AIRSM) based on extended active subspaces is proposed for mixed random and interval uncertainties. Based on the idea of subspace dimension reduction, extended active subspaces are given for mixed uncertainties and interval active variable representation is derived for the construction of AIRSM. A weighted response surface strategy is introduced and tested for predicting the accurate boundary. Moreover, an interval dynamic correlation index is defined, and significance check and cross validation are reformulated in active subspaces to evaluate the AIRSM. The effectiveness of AIRSM is demonstrated on two test examples: three-dimensional nonlinear function and speed reducer design. They both possess a dominant onedimensional active subspace with small estimation error and the accuracy of AIRSM is verified by comparing with full-dimensional Monte Carlo simulates, thus providing a potential template for tackling high-dimensional problems involving mixed aleatory and interval uncertainties.},
	author = {Hu, Xingzhi and Chen, Xiaoqian and Lattarulo, Valerio and Parks, Geoffrey T.},
	doi = {10.2514/1.J054627},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\J8P5RA7H\\Hu et al. - 2016 - Multidisciplinary Optimization Under High-Dimensio.pdf},
	issn = {0001-1452, 1533-385X},
	journal = {AIAA Journal},
	language = {en},
	month = may,
	number = {5},
	pages = {1732--1741},
	title = {Multidisciplinary {{Optimization Under High}}-{{Dimensional Uncertainty}} for {{Small Satellite System Design}}},
	volume = {54},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.2514/1.J054627}}

@article{HultNyquist_LargeDeviationsWeighted_2016,
	abstract = {In this paper the efficiency of an importance sampling algorithm is studied by means of large deviations for the associated weighted empirical measure. The main result, stated as a Laplace principle for these weighted empirical measures, can be viewed as an extension of Sanov's theorem. The main theorem is used to quantify the performance of an importance sampling algorithm over a collection of subsets of a given target set as well as quantile estimates. The analysis yields an estimate of the sample size needed to reach a desired precision and of the reduction in cost compared to standard Monte Carlo.},
	archiveprefix = {arXiv},
	author = {Hult, Henrik and Nyquist, Pierre},
	doi = {10.1016/j.spa.2015.08.002},
	eprint = {1210.2251},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\7MFE3FWP\\Hult et Nyquist - 2016 - Large deviations for weighted empirical measures a.pdf},
	issn = {03044149},
	journal = {Stochastic Processes and their Applications},
	keywords = {60F10,60F10; 65C05,65C05,Mathematics - Probability},
	language = {en},
	month = jan,
	number = {1},
	pages = {138--170},
	title = {Large Deviations for Weighted Empirical Measures Arising in Importance Sampling},
	volume = {126},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.spa.2015.08.002}}

@article{JiangLi_HighDimensionalStructural_2017,
	abstract = {For the uncertainty quantification in structural dynamics, random simulate methods such as Monte Carlo Simulation, Probability Density Evolution Method (Le and Chen, 2009) and metamodel method (Hastie et al., 2005) [2] are extensively used because of their usability and universality. Unfortunately, the required computational resource for structural stochastic analysis is still a burdensome task especially structures are involved in nonlinearity and high dimensional uncertainty. The so called ``curse of dimension'' problem means the cost of performing a reliable reliability analysis increases exponentially with the dimension. In present paper, a supervised dimension reduction methodology named Active Subspace Method (Constantine, 2015) is introduced to deal with the high dimension problem of structural reliability. GF-discrepancy based point set is employed to exploit the hidden low-dimensional structure in the mapping from input to the quantity of interest (QOI), a kriging metamodel (Kaymaz, 2005) with higher accuracy and efficiency can be constructed on the low-dimensional subspace. Further, the extreme-value reliability of structure is calculated effectively by incorporating into probability density evolution method based extreme-value system reliability (Li et al., 2007). The proposed approach is then applied to a theoretical four branches system with two-dimensional random variable and a 6-DOF BoucWen nonlinear numerical model with 20-dimension random variable. The results show that the Active Subspace Kriging (ASK) method significantly improved the result of extreme-value reliability analysis for stochastic nonlinear structures, in which high dimensional randomness problem is involved.},
	author = {Jiang, Zhongming and Li, Jie},
	doi = {10.1016/j.strusafe.2017.07.007},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\692Q8W82\\Jiang et Li - 2017 - High dimensional structural reliability with dimen.pdf},
	issn = {01674730},
	journal = {Structural Safety},
	language = {en},
	month = nov,
	pages = {35--46},
	title = {High Dimensional Structural Reliability with Dimension Reduction},
	volume = {69},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.strusafe.2017.07.007}}

@article{JourdainLelong_RobustAdaptiveImportance_2009,
	author = {Jourdain, Benjamin and Lelong, J{\'e}r{\^o}me},
	doi = {10.1214/09-AAP595},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\HBWUKLUX\\Jourdain et Lelong - 2009 - Robust adaptive importance sampling for normal ran.pdf},
	issn = {1050-5164},
	journal = {The Annals of Applied Probability},
	language = {en},
	month = oct,
	number = {5},
	title = {Robust Adaptive Importance Sampling for Normal Random Vectors},
	volume = {19},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1214/09-AAP595}}

@article{KatafygiotisZuev_ESTIMATIONSMALLFAILURE_,
	author = {Katafygiotis, L S and Zuev, K M},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\M2W8WW5Y\\Katafygiotis et Zuev - ESTIMATION OF SMALL FAILURE PROBABILITIES IN HIGH .pdf},
	language = {en},
	pages = {12},
	title = {{{ESTIMATION OF SMALL FAILURE PROBABILITIES IN HIGH DIMENSIONS BY ADAPTIVE LINKED IMPORTANCE SAMPLING}}}}

@article{KatafygiotisZuev_GeometricInsightChallenges_2008,
	abstract = {In this paper we adopt a geometric perspective to highlight the challenges associated with solving high-dimensional reliability problems. Adopting a geometric point of view we highlight and explain a range of results concerning the performance of several well-known reliability methods.},
	author = {Katafygiotis, L.S. and Zuev, K.M.},
	doi = {10.1016/j.probengmech.2007.12.026},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\6XKXJKVA\\Katafygiotis et Zuev - 2008 - Geometric insight into the challenges of solving h.pdf},
	issn = {02668920},
	journal = {Probabilistic Engineering Mechanics},
	language = {en},
	month = apr,
	number = {2-3},
	pages = {208--218},
	title = {Geometric Insight into the Challenges of Solving High-Dimensional Reliability Problems},
	volume = {23},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.probengmech.2007.12.026}}

@article{Kawai_AdaptiveImportanceSampling_2020,
	author = {Kawai, Reiichiro},
	doi = {10.1016/j.jmaa.2019.123608},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\B66S4IVX\\Kawai - 2020 - Adaptive importance sampling and control variates.pdf},
	issn = {0022247X},
	journal = {Journal of Mathematical Analysis and Applications},
	language = {en},
	month = mar,
	number = {1},
	pages = {123608},
	title = {Adaptive Importance Sampling and Control Variates},
	volume = {483},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.jmaa.2019.123608}}

@article{Kawai_OptimizingAdaptiveImportance_2018,
	author = {Kawai, Reiichiro},
	doi = {10.1137/18M1173472},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\A6AEGANK\\Kawai - 2018 - Optimizing Adaptive Importance Sampling by Stochas.pdf},
	issn = {1064-8275, 1095-7197},
	journal = {SIAM Journal on Scientific Computing},
	language = {en},
	month = jan,
	number = {4},
	pages = {A2774-A2800},
	title = {Optimizing {{Adaptive Importance Sampling}} by {{Stochastic Approximation}}},
	volume = {40},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1137/18M1173472}}

@incollection{KroeseEtAl_CrossEntropyMethodEstimation_2013,
	author = {Kroese, Dirk P. and Rubinstein, Reuven Y. and Glynn, Peter W.},
	booktitle = {Handbook of {{Statistics}}},
	doi = {10.1016/B978-0-444-53859-8.00002-3},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\HSS6ETBP\\Kroese et al. - 2013 - The Cross-Entropy Method for Estimation.pdf},
	isbn = {978-0-444-53859-8},
	language = {en},
	pages = {19--34},
	publisher = {{Elsevier}},
	title = {The {{Cross}}-{{Entropy Method}} for {{Estimation}}},
	volume = {31},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1016/B978-0-444-53859-8.00002-3}}

@article{KurtzSong_CrossentropybasedAdaptiveImportance_2013,
	abstract = {Structural reliability analysis frequently requires the use of sampling-based methods, particularly for the situation where the failure domain in the random variable space is complex. One of the most efficient and widely utilized methods to use in such a situation is importance sampling. Recently, an adaptive importance sampling method was proposed to find a near-optimal importance sampling density by minimizing Kullback\textendash Leibler cross entropy, i.e. a measure of the difference between the absolute best sampling density and the one being used for the importance sampling. In this paper, the adaptive importance sampling approach is further developed by incorporating a nonparametric multimodal probability density function model called the Gaussian mixture as the importance sampling density. This model is used to fit the complex shape of the absolute best sampling density functions including those with multiple important regions. An efficient procedure is developed to update the Gaussian mixture model toward a near-optimal density using a small size of pre-samples. The proposed method needs only a few steps to achieve a near-optimal sampling density, and shows significant improvement in efficiency and accuracy for a variety of component and system reliability problems. The method requires far less samples than both crude Monte Carlo simulation and the cross-entropy-based adaptive importance sampling method employing a unimodal density function; thus achieving relatively small values of the coefficient of variation efficiently. The computational efficiency and accuracy of the proposed method are not hampered by the probability level, dimension of random variable space, and curvatures of limit-state function. Moreover, the distribution model parameters of the Gaussian densities in the obtained near-optimal density help identify important areas in the random variable space and their relative importance.},
	author = {Kurtz, Nolan and Song, Junho},
	doi = {10.1016/j.strusafe.2013.01.006},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\GPTP74DY\\Kurtz et Song - 2013 - Cross-entropy-based adaptive importance sampling u.pdf},
	issn = {01674730},
	journal = {Structural Safety},
	language = {en},
	month = may,
	pages = {35--44},
	title = {Cross-Entropy-Based Adaptive Importance Sampling Using {{Gaussian}} Mixture},
	volume = {42},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.strusafe.2013.01.006}}

@article{Lebrun_ContributionsModelisationDependance_,
	author = {Lebrun, R{\'e}gis},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\8CXEFRA3\\Lebrun - Contributions {\`a} la mod{\'e}lisation de la d{\'e}pendance s.pdf},
	language = {fr},
	pages = {141},
	title = {{Contributions \`a la mod\'elisation de la d\'ependance stochastique}}}

@phdthesis{Lebrun_ContributionsModelisationDependance_2013,
	author = {Lebrun, R{\'e}gis},
	school = {Universit\'e Paris-Diderot},
	title = {Contributions \`a La Mod\'elisation de La D\'ependance Stochastique},
	type = {{{PhD Thesis}}},
	year = {2013}}

@article{Lebrun_ContributionsModelisationDependance_a,
	author = {Lebrun, R{\'e}gis},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\5BJ24DR6\\Lebrun - Contributions {\`a} la mod{\'e}lisation de la d{\'e}pendance s.pdf},
	language = {fr},
	pages = {141},
	title = {{Contributions \`a la mod\'elisation de la d\'ependance stochastique}}}

@article{LedoitWolf_WellconditionedEstimatorLargedimensional_2004,
	abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For largedimensional covariance matrices, the usual estimator\textemdash the sample covariance matrix\textemdash is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample.},
	author = {Ledoit, Olivier and Wolf, Michael},
	doi = {10.1016/S0047-259X(03)00096-4},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\9D22YLN3\\Ledoit et Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf},
	issn = {0047259X},
	journal = {Journal of Multivariate Analysis},
	language = {en},
	month = feb,
	number = {2},
	pages = {365--411},
	title = {A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices},
	volume = {88},
	year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1016/S0047-259X(03)00096-4}}

@inproceedings{LeonEtAl_ActiveSubspaceUncertainty_2018,
	abstract = {Quantum-informed ferroelectric phase field models capable of predicting material behavior, are necessary for facilitating the development and production of many adaptive structures and intelligent systems. Uncertainty is present in these models, given the quantum scale at which calculations take place. A necessary analysis is to determine how the uncertainty in the response can be attributed to the uncertainty in the model inputs or parameters. A second analysis is to identify active subspaces within the original parameter space, which quantify directions in which the model response varies most dominantly, thus reducing sampling effort and computational cost. In this investigation, we identify an active subspace for a poly-domain ferroelectric phase-field model. Using the active variables as our independent variables, we then construct a surrogate model and perform Bayesian inference. Once we quantify the uncertainties in the active variables, we obtain uncertainties for the original parameters via an inverse mapping. The analysis provides insight into how active subspace methodologies can be used to reduce computational power needed to perform Bayesian inference on model parameters informed by experimental or simulated data.},
	address = {{Denver, United States}},
	author = {Leon, Lider S. and Smith, Ralph C. and Oates, William S. and Miles, Paul},
	booktitle = {Behavior and {{Mechanics}} of {{Multifunctional Materials}} and {{Composites XII}}},
	doi = {10.1117/12.2297207},
	editor = {Naguib, Hani E.},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\IN3C7ACH\\Leon et al. - 2018 - Active subspace uncertainty quantification for a p.pdf},
	isbn = {978-1-5106-1688-2 978-1-5106-1689-9},
	language = {en},
	month = mar,
	pages = {25},
	publisher = {{SPIE}},
	title = {Active Subspace Uncertainty Quantification for a Polydomain Ferroelectric Phase-Field Model},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1117/12.2297207}}

@article{Liu_MetropolizedIndependentSampling_1996,
	author = {Liu, Jun S.},
	doi = {10.1007/BF00162521},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\N7ZCNNSN\\Liu - 1996 - Metropolized independent sampling with comparisons.pdf},
	issn = {0960-3174, 1573-1375},
	journal = {Statistics and Computing},
	language = {en},
	month = jun,
	number = {2},
	pages = {113--119},
	title = {Metropolized Independent Sampling with Comparisons to Rejection Sampling and Importance Sampling},
	volume = {6},
	year = {1996},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00162521}}

@article{LiuDerKiureghian_MultivariateDistributionModels_1986,
	abstract = {Two multivariate distribution models consistent with prescribed marginal distributions and covariances are presented. The models are applicable to arbitrary number of random variables and are particularly suited for engineering applications. Conditions for validity of each model and applicable ranges of correlation coefficients between the variables are determined. Formulae are developed which facilitate evaluation of the model parameters in terms of the prescribed marginals and covariances. Potential uses of the two models in engineering are discussed.},
	author = {Liu, Pei-Ling and Der Kiureghian, Armen},
	doi = {10.1016/0266-8920(86)90033-0},
	issn = {0266-8920},
	journal = {Probabilistic Engineering Mechanics},
	number = {2},
	pages = {105--112},
	title = {Multivariate Distribution Models with Prescribed Marginals and Covariances},
	volume = {1},
	year = {1986},
	Bdsk-Url-1 = {https://doi.org/10.1016/0266-8920(86)90033-0}}

@article{LuEtAl_GaussianApproximationsProbability_2017,
	abstract = {This paper concerns the approximation of probability measures on Rd with respect to the Kullback\textendash Leibler divergence. Given an admissible target measure, we show the existence of the best approximation, with respect to this divergence, from certain sets of Gaussian measures and Gaussian mixtures. The asymptotic behavior of such best approximations is then studied in the small parameter limit where the measure concentrates; this asympotic behavior is characterized using {$\Gamma$}-convergence. The theory developed is then applied to understand the frequentist consistency of Bayesian inverse problems in finite dimensions. For a fixed realization of additive observational noise, we show the asymptotic normality of the posterior measure in the small noise limit. Taking into account the randomness of the noise, we prove a Bernstein\textendash Von Mises type result for the posterior measure.},
	author = {Lu, Yulong and Stuart, Andrew and Weber, Hendrik},
	doi = {10.1137/16M1105384},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\HW5J78P8\\Lu et al. - 2017 - Gaussian Approximations for Probability Measures o.pdf},
	issn = {2166-2525},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	language = {en},
	month = jan,
	number = {1},
	pages = {1136--1165},
	title = {Gaussian {{Approximations}} for {{Probability Measures}} on \${{R}}\^d\$},
	volume = {5},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1137/16M1105384}}

@article{MachlevEtAl_ModifiedCrossEntropyMethod_2018,
	abstract = {Non-intrusive load monitoring is an algorithm or process that disaggregates the total power in a facility to identify consumption of individual appliances. In this paper, a new algorithm is proposed to classify events of appliance states based on modification of the Cross-Entropy method. The main contribution is a formulation and solution of the problem with the Cross-Entropy method as a constrained optimization problem. This new technique is called the Modified CrossEntropy (MCE) method. The proposed algorithm is simple, as it operates in real time, using low rate sampling of the active power. In addition, there is no need to train the system, or to use complex hardware. The algorithm is tested using the REDD and AMPds datasets, and the results are compared to several state-ofthe-art techniques.},
	author = {Machlev, Ram and Levron, Yoash and Beck, Yuval},
	doi = {10.1109/TSG.2018.2871620},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\MHPKW3MI\\Machlev et al. - 2018 - Modified Cross-Entropy Method for Classification o.pdf},
	issn = {1949-3053, 1949-3061},
	journal = {IEEE Transactions on Smart Grid},
	language = {en},
	pages = {1--1},
	title = {Modified {{Cross}}-{{Entropy Method}} for {{Classification}} of {{Events}} in {{NILM Systems}}},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/TSG.2018.2871620}}

@book{MadsenEtAl_MethodsStructuralSafety_2006,
	author = {Madsen, Henrik O and Krenk, Steen and Lind, Niels Christian},
	publisher = {{Courier Corporation}},
	title = {Methods of Structural Safety},
	year = {2006}}

@article{Margolin_ConvergenceCrossEntropyMethod_2005,
	abstract = {The cross-entropy method is a relatively new method for combinatorial optimization. The idea of this method came from the simulation field and then was successfully applied to different combinatorial optimization problems. The method consists of an iterative stochastic procedure that makes use of the importance sampling technique. In this paper we prove the asymptotical convergence of some modifications of the cross-entropy method.},
	author = {Margolin, L.},
	doi = {10.1007/s10479-005-5731-0},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\2U5Y7AEK\\Margolin - 2005 - On the Convergence of the Cross-Entropy Method.pdf},
	issn = {0254-5330, 1572-9338},
	journal = {Annals of Operations Research},
	language = {en},
	month = feb,
	number = {1},
	pages = {201--214},
	title = {On the {{Convergence}} of the {{Cross}}-{{Entropy Method}}},
	volume = {134},
	year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10479-005-5731-0}}

@article{MarinEtAl_ConsistencyAdaptiveMultiple_2012,
	abstract = {Among Monte Carlo techniques, the importance sampling requires fine tuning of a proposal distribution, which is now fluently resolved through iterative schemes. The Adaptive Multiple Importance Sampling (AMIS) of Cornuet et al. (2012) provides a significant improvement in stability and effective sample size due to the introduction of a recycling procedure. However, the consistency of the AMIS estimator remains largely open. In this work we prove the convergence of the AMIS, at a cost of a slight modification in the learning process. Contrary to Douc et al. (2007a), results are obtained here in the asymptotic regime where the number of iterations is going to infinity while the number of drawings per iteration is a fixed, but growing sequence of integers. Hence some of the results shed new light on adaptive population Monte Carlo algorithms in that last regime.},
	archiveprefix = {arXiv},
	author = {Marin, Jean-Michel and Pudlo, Pierre and Sedki, Mohammed},
	eprint = {1211.2548},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\SKKZX8MN\\Marin et al. - 2012 - Consistency of the Adaptive Multiple Importance Sa.pdf},
	journal = {arXiv:1211.2548 [math, stat]},
	keywords = {65C05 (Primary) 60F17 (Secondary),Mathematics - Statistics Theory,Statistics - Computation},
	language = {en},
	month = nov,
	primaryclass = {math, stat},
	title = {Consistency of the {{Adaptive Multiple Importance Sampling}}},
	year = {2012}}

@article{MartinoEtAl_EffectiveSampleSize_2017,
	abstract = {The Effective Sample Size (ESS) is an important measure of efficiency of Monte Carlo methods such as Markov Chain Monte Carlo (MCMC) and Importance Sampling (IS) techniques. In the IS context, an approximation EnSS of the theoretical ESS definition is widely applied, involving the inverse of the sum of the squares of the normalized importance weights. This formula, EnSS, has become an essential piece within Sequential Monte Carlo (SMC) methods, to assess the convenience of a resampling step. From another perspective, the expression n ESS is related to the Euclidean distance between the probability mass described by the normalized weights and the discrete uniform probability mass function (pmf). In this work, we derive other possible ESS functions based on different discrepancy measures between these two pmfs. Several examples are provided involving, for instance, the geometric mean of the weights, the discrete entropy (including the perplexity measure, already proposed in literature) and the Gini coefficient among others. We list five theoretical requirements which a generic ESS function should satisfy, allowing us to classify different ESS measures. We also compare the most promising ones by means of numerical simulations.},
	author = {Martino, Luca and Elvira, V{\'\i}ctor and Louzada, Francisco},
	doi = {10.1016/j.sigpro.2016.08.025},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\SLHR9IQ2\\Martino et al. - 2017 - Effective sample size for importance sampling base.pdf},
	issn = {01651684},
	journal = {Signal Processing},
	language = {en},
	month = feb,
	pages = {386--401},
	title = {Effective Sample Size for Importance Sampling Based on Discrepancy Measures},
	volume = {131},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.sigpro.2016.08.025}}

@article{MartinoEtAl_LayeredAdaptiveImportance_2017,
	abstract = {Monte Carlo methods represent the de facto standard for approximating complicated integrals involving multidimensional target distributions. In order to generate random realizations from the target distribution, Monte Carlo techniques use simpler proposal probability densities to draw candidate samples. The performance of any such method is strictly related to the specification of the proposal distribution, such that unfortunate choices easily wreak havoc on the resulting estimators. In this work, we introduce a layered (i.e., hierarchical) procedure to generate samples employed within a Monte Carlo scheme. This approach ensures that an appropriate equivalent proposal density is always obtained automatically (thus eliminating the risk of a catastrophic performance), although at the expense of a moderate increase in the complexity. Furthermore, we provide a general unified importance sampling (IS) framework, where multiple proposal densities are employed and several IS schemes are introduced by applying the so-called deterministic mixture approach. Finally, given these schemes, we also propose a novel class of adaptive importance samplers using a population of proposals, where the adaptation is driven by independent parallel or interacting Markov chain Monte Carlo (MCMC) chains. The resulting algorithms efficiently combine the benefits of both IS and MCMC methods.},
	author = {Martino, L. and Elvira, V. and Luengo, D. and Corander, J.},
	doi = {10.1007/s11222-016-9642-5},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\AS36GN37\\Martino et al. - 2017 - Layered adaptive importance sampling.pdf},
	issn = {0960-3174, 1573-1375},
	journal = {Statistics and Computing},
	language = {en},
	month = may,
	number = {3},
	pages = {599--623},
	title = {Layered Adaptive Importance Sampling},
	volume = {27},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11222-016-9642-5}}

@article{MasriEtAl_ImprovementCrossentropyMethod_2020,
  title={Improvement of the cross-entropy method in high dimension for failure probability estimation through a one-dimensional projection without gradient estimation},
  author={El Masri, Maxime and Morio, J{\'e}r{\^o}me and Simatos, Florian},
  journal={Reliability Engineering \& System Safety},
  volume={216},
  pages={107991},
  year={2021},
  publisher={Elsevier},
  doi ={10.1016/j.ress.2021.107991}
}

@article{Mestre_AsymptoticBehaviorSample_2008,
	abstract = {This paper analyzes the asymptotic behavior of the sample estimators of the eigenvalues and eigenvectors of covariance matrices. Rather than considering traditional large samplesize asymptotics, in this paper the focus is on limited sample size situations, whereby the number of available observations is comparable in magnitude to the observation dimension. Using tools from random matrix theory, the asymptotic behavior of the traditional sample estimates is investigated under the assumption that both the sample size and the observation dimension tend to infinity, while their quotient converges to a positive quantity. Assuming that an asymptotic eigenvalue splitting condition is fulfilled, closed form asymptotic expressions of these estimators are derived, proving inconsistency of the traditional sample estimators in these asymptotic conditions. The derived expressions are shown to provide a valuable insight into the behavior of the sample estimators in the small sample size regime.},
	author = {Mestre, Xavier},
	doi = {10.1109/TSP.2008.929662},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\SB37DKKU\\Mestre - 2008 - On the Asymptotic Behavior of the Sample Estimates.pdf},
	issn = {1053-587X, 1941-0476},
	journal = {IEEE Transactions on Signal Processing},
	language = {en},
	month = nov,
	number = {11},
	pages = {5353--5368},
	title = {On the {{Asymptotic Behavior}} of the {{Sample Estimates}} of {{Eigenvalues}} and {{Eigenvectors}} of {{Covariance Matrices}}},
	volume = {56},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/TSP.2008.929662}}

@article{Mestre_ImprovedEstimationEigenvalues_2008,
	abstract = {The problem of estimating the eigenvalues and eigenvectors of the covariance matrix associated with a multivariate stochastic process is considered. The focus is on finite sample size situations, whereby the number of observations is limited and comparable in magnitude to the observation dimension. Using tools from random matrix theory, and assuming a certain eigenvalue splitting condition, new estimators of the eigenvalues and eigenvectors of the covariance matrix are derived, that are shown to be consistent in a more general asymptotic setting than the traditional one. Indeed, these estimators are proven to be consistent, not only when the sample size increases without bound for a fixed observation dimension, but also when the observation dimension increases to infinity at the same rate as the sample size. Numerical evaluations indicate that the estimators have an excellent performance in small sample size scenarios, where the observation dimension and the sample size are comparable in magnitude.},
	author = {Mestre, Xavier},
	doi = {10.1109/TIT.2008.929938},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\SY97JNRG\\Mestre - 2008 - Improved Estimation of Eigenvalues and Eigenvector.pdf},
	issn = {0018-9448},
	journal = {IEEE Transactions on Information Theory},
	language = {en},
	month = nov,
	number = {11},
	pages = {5113--5129},
	title = {Improved {{Estimation}} of {{Eigenvalues}} and {{Eigenvectors}} of {{Covariance Matrices Using Their Sample Estimates}}},
	volume = {54},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/TIT.2008.929938}}

@article{MiarNaeimiEtAl_MultilevelCrossEntropy_2018,
	abstract = {This work proposes a new meta-heuristic optimization algorithm called multi-level cross entropy Optimizer (MCEO). This algorithm is conducted by combination of a group of cross entropy operators. Situations, with a low probability for optimal point are searched with high speed, and also, locations with a high probability for existence of optimal point are investigated with a low speed and high accuracy. The algorithm is then benchmarked on 13 well-known test functions in high dimension spaces (100 dimensions), and the answers are verified by a comparative study with thermal exchange optimization, selfish herds optimization, water evaporation optimization, Moth-Flame optimization, Flower Pollination Algorithm, states of matter search, and gray wolf optimizer. The results indicate that the MCEO algorithm can provide very competitive results in comparison to these well-known meta-heuristics in a similar condition (in term of NFEs). The paper also considers solving three classical engineering design problems (tension/compression spring, welded beam, and pressure vessel designs) and presents a genuine application of the proposed method to the field of dam engineering. The results of the classical engineering design problems and the real application validate that the proposed algorithm is applicable to challenging difficulties with unknown search spaces.},
	author = {MiarNaeimi, Farid and Azizyan, Gholamreza and Rashki, Mohsen},
	doi = {10.1007/s00366-017-0569-z},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\S6WBVKIF\\MiarNaeimi et al. - 2018 - Multi-level cross entropy optimizer (MCEO) an evo.pdf},
	issn = {0177-0667, 1435-5663},
	journal = {Engineering with Computers},
	language = {en},
	month = oct,
	number = {4},
	pages = {719--739},
	shorttitle = {Multi-Level Cross Entropy Optimizer ({{MCEO}})},
	title = {Multi-Level Cross Entropy Optimizer ({{MCEO}}): An Evolutionary Optimization Algorithm for Engineering Problems},
	volume = {34},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00366-017-0569-z}}

@article{MorioEtAl_SurveyRareEvent_2014,
	abstract = {Crude Monte-Carlo or quasi Monte-Carlo methods are well suited to characterize events of which associated probabilities are not too low with respect to the simulation budget. For very seldom observed events, such as the collision probability between two aircraft in airspace, these approaches do not lead to accurate results. Indeed, the number of available samples is often insufficient to estimate such low probabilities (at least 106 samples are needed to estimate a probability of order 10\`A4 with 10\% relative error with Monte-Carlo simulations). In this article, one reviewed different appropriate techniques to estimate rare event probabilities that require a fewer number of samples. These methods can be divided into four main categories: parameterization techniques of probability density function tails, simulation techniques such as importance sampling or importance splitting, geometric methods to approximate input failure space and finally, surrogate modeling. Each technique is detailed, its advantages and drawbacks are described and a synthesis that aims at giving some clues to the following question is given: ``which technique to use for which problem?''. \'O 2014 Elsevier B.V. All rights reserved.},
	author = {Morio, J{\'e}r{\^o}me and Balesdent, Mathieu and Jacquemart, Damien and Verg{\'e}, Christelle},
	doi = {10.1016/j.simpat.2014.10.007},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\6KRXDKQ4\\Morio et al. - 2014 - A survey of rare event simulation methods for stat.pdf},
	issn = {1569190X},
	journal = {Simulation Modelling Practice and Theory},
	language = {en},
	month = dec,
	pages = {287--304},
	title = {A Survey of Rare Event Simulation Methods for Static Input\textendash Output Models},
	volume = {49},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.simpat.2014.10.007}}

@article{Neal_AnnealedImportanceSampling_,
	abstract = {Simulated annealing\textemdash moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions\textemdash has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers.},
	author = {Neal, Radford M},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\2ANG9PWZ\\Neal - Annealed importance sampling.pdf},
	language = {en},
	pages = {15},
	title = {Annealed Importance Sampling}}

@article{OwenZhou_SafeEffectiveImportance_2000,
	author = {Owen, Art and Zhou, Yi},
	doi = {10.1080/01621459.2000.10473909},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\JPN79M3S\\Owen et Zhou - 2000 - Safe and Effective Importance Sampling.pdf},
	issn = {0162-1459, 1537-274X},
	journal = {Journal of the American Statistical Association},
	language = {en},
	month = mar,
	number = {449},
	pages = {135--143},
	title = {Safe and {{Effective Importance Sampling}}},
	volume = {95},
	year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1080/01621459.2000.10473909}}

@book{PapadrakakisEtAl_ComputationalStructuralDynamics_2008,
	doi = {10.1201/9780203881637},
	edition = {Zeroth},
	editor = {Papadrakakis, Manolis and Charmpis, Dimos C. and Tsompanakis, Yannis and Lagaros, Nikos D.},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\N9W7YV6G\\(Structures and Infrastructures) Papadrakakis, Manolis - Computational Structural Dynamics and Earthquake Engineering_ Structures and Infrastructures.pdf},
	isbn = {978-0-429-20700-6},
	language = {en},
	month = dec,
	publisher = {{CRC Press}},
	shorttitle = {Computational {{Structural Dynamics}} and {{Earthquake Engineering}}},
	title = {Computational {{Structural Dynamics}} and {{Earthquake Engineering}}: {{Structures}} and {{Infrastructures Book Series}}, {{Vol}}. 2},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1201/9780203881637}}


@article{PapaioannouEtAl_ImprovedCrossEntropybased_2019,
	abstract = {The probability of a rare event or failure event is defined through a potentially high-dimensional integral, whose integration domain is often only known point-wise in terms of the outcome of a numerical model. The probability of failure can be estimated efficiently through importance sampling (IS), provided that an effective IS density is chosen. The cross entropy (CE) method is an adaptive sampling approach that determines the IS density through minimizing the Kullback\textendash Leibler divergence between the theoretically optimal IS density and a chosen parametric family of distributions. We propose an improved version of the classical CE method that introduces a smooth transition to make better use of the samples from intermediate sampling levels for fitting the sought IS density. The improved CE method is combined with a novel flexible parametric distribution model that is able to handle low- and high-dimensional problems as well as problems with multimodal failure domains. A set of numerical examples demonstrate that the proposed approach performs consistently better than the classical CE method in various problem settings.},
	author = {Papaioannou, Iason and Geyer, Sebastian and Straub, Daniel},
	doi = {10.1016/j.ress.2019.106564},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\9V4SAZQK\\Papaioannou et al. - 2019 - Improved cross entropy-based importance sampling w.pdf},
	issn = {09518320},
	journal = {Reliability Engineering \& System Safety},
	language = {en},
	month = nov,
	pages = {106564},
	title = {Improved Cross Entropy-Based Importance Sampling with a Flexible Mixture Model},
	volume = {191},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.ress.2019.106564}}

@article{PapaioannouEtAl_SequentialImportanceSampling_2016,
	abstract = {This paper proposes the application of sequential importance sampling (SIS) to the estimation of the probability of failure in structural reliability. SIS was developed originally in the statistical community for exploring posterior distributions and estimating normalizing constants in the context of Bayesian analysis. The basic idea of SIS is to gradually translate samples from the prior distribution to samples from the posterior distribution through a sequential reweighting operation. In the context of structural reliability, SIS can be applied to produce samples of an approximately optimal importance sampling density, which can then be used for estimating the sought probability. The transition of the samples is defined through the construction of a sequence of intermediate distributions. We present a particular choice of the intermediate distributions and discuss the properties of the derived algorithm. Moreover, we introduce two MCMC algorithms for application within the SIS procedure; one that is applicable to general problems with small to moderate number of random variables and one that is especially efficient for tackling high-dimensional problems.},
	author = {Papaioannou, Iason and Papadimitriou, Costas and Straub, Daniel},
	doi = {10.1016/j.strusafe.2016.06.002},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\ILNJYVYR\\Papaioannou et al. - 2016 - Sequential importance sampling for structural reli.pdf},
	issn = {01674730},
	journal = {Structural Safety},
	language = {en},
	month = sep,
	pages = {66--75},
	title = {Sequential Importance Sampling for Structural Reliability Analysis},
	volume = {62},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.strusafe.2016.06.002}}

@article{Parente_ProbabilisticFrameworkApproximating_2018,
	abstract = {This paper develops a comprehensive probabilistic setup to compute approximating functions in active subspaces. Constantine et al. proposed the active subspace method in [8] to reduce the dimension of computational problems. It can be seen as an attempt to approximate a high-dimensional function of interest f by a low-dimensional one. To do this, a common approach is to integrate f over the inactive, i. e. non-dominant, directions with a suitable conditional density function. In practice, this can be done with a finite Monte Carlo sum, making not only the resulting approximation random in the inactive variable for each fixed input from the active subspace, but also its expectation, i. e. the integral of the low-dimensional function weighted with a probability measure on the active variable. In this regard we develop a fully probabilistic framework extending results from [8, 10]. The results are supported by a simple numerical example.},
	archiveprefix = {arXiv},
	author = {Parente, Mario Teixeira},
	eprint = {1809.06581},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\3AE6MQQU\\Parente - 2018 - A probabilistic framework for approximating functi.pdf},
	journal = {arXiv:1809.06581 [math, stat]},
	keywords = {65C60,Mathematics - Numerical Analysis,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Methodology},
	language = {en},
	month = sep,
	primaryclass = {math, stat},
	title = {A Probabilistic Framework for Approximating Functions in Active Subspaces},
	year = {2018}}

@inproceedings{Perez-Cruz_KullbackLeiblerDivergenceEstimation_2008,
	abstract = {We present a method for estimating the KL divergence between continuous densities and we prove it converges almost surely. Divergence estimation is typically solved estimating the densities first. Our main result shows this intermediate step is unnecessary and that the divergence can be either estimated using the empirical cdf or k-nearest-neighbour density estimation, which does not converge to the true measure for finite k. The convergence proof is based on describing the statistics of our estimator using waiting-times distributions, as the exponential or Erlang. We illustrate the proposed estimators and show how they compare to existing methods based on density estimation, and we also outline how our divergence estimators can be used for solving the two-sample problem.},
	address = {{Toronto, ON, Canada}},
	author = {{Perez-Cruz}, Fernando},
	booktitle = {2008 {{IEEE International Symposium}} on {{Information Theory}}},
	doi = {10.1109/ISIT.2008.4595271},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\YP57ZXP4\\Perez-Cruz - 2008 - Kullback-Leibler divergence estimation of continuo.pdf},
	isbn = {978-1-4244-2256-2},
	language = {en},
	month = jul,
	pages = {1666--1670},
	publisher = {{IEEE}},
	title = {Kullback-{{Leibler}} Divergence Estimation of Continuous Distributions},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISIT.2008.4595271}}

@article{Piera-Martinez_ModelisationComportementsExtremes_,
	author = {{Piera-Martinez}, Miguel},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\Y85QRVSL\\Piera-Martinez - Mod{\'e}lisation des comportements extr{\^e}mes en ing{\'e}nie.pdf},
	pages = {253},
	title = {Mod\'elisation Des Comportements Extr\^emes En Ing\'enierie}}

@phdthesis{Piera-Martinez_ModelisationComportementsExtremes_2008,
	author = {{Piera-Martinez}, Miguel},
	school = {Universit\'e Paris Sud},
	title = {Mod\'elisation Des Comportements Extr\^emes En Ing\'enierie},
	type = {{{PhD Thesis}}},
	year = {2008}}

@article{PortierDelyon_AsymptoticOptimalityAdaptive_,
	abstract = {Adaptive importance sampling (AIS) uses past samples to update the sampling policy qt at each stage t. Each stage t is formed with two steps : (i) to explore the space with nt points according to qt and (ii) to exploit the current amount of information to update the sampling policy. The very fundamental question raised in this paper concerns the behavior of empirical sums based on AIS. Without making any assumption on the allocation policy nt, the theory developed involves no restriction on the split of computational resources between the explore (i) and the exploit (ii) step. It is shown that AIS is asymptotically optimal : the asymptotic behavior of AIS is the same as some ``oracle'' strategy that knows the targeted sampling policy from the beginning. From a practical perspective, weighted AIS is introduced, a new method that allows to forget poor samples from early stages.},
	author = {Portier, Fran{\c c}ois and Delyon, Bernard},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\XG4SMVRR\\Portier et Delyon - Asymptotic optimality of adaptive importance sampl.pdf},
	language = {en},
	pages = {11},
	title = {Asymptotic Optimality of Adaptive Importance Sampling}}

@article{RaghavanCox_AdaptiveMixtureImportance_1998,
	author = {Raghavan, Nandini and Cox, Dennis D.},
	doi = {10.1080/00949659808811890},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\978EU8LI\\Raghavan et Cox - 1998 - Adaptive mixture importance sampling.pdf},
	issn = {0094-9655, 1563-5163},
	journal = {Journal of Statistical Computation and Simulation},
	language = {en},
	month = may,
	number = {3},
	pages = {237--259},
	title = {Adaptive Mixture Importance Sampling},
	volume = {60},
	year = {1998},
	Bdsk-Url-1 = {https://doi.org/10.1080/00949659808811890}}

@article{RichardZhang_EfficientHighdimensionalImportance_2007,
	abstract = {The paper describes a simple, generic and yet highly accurate efficient importance sampling (EIS) Monte Carlo (MC) procedure for the evaluation of high-dimensional numerical integrals. EIS is based upon a sequence of auxiliary weighted regressions which actually are linear under appropriate conditions. It can be used to evaluate likelihood functions and byproducts thereof, such as ML estimators, for models which depend upon unobservable variables. A dynamic stochastic volatility model and a logit panel data model with unobserved heterogeneity (random effects) in both dimensions are used to provide illustrations of EIS high numerical accuracy, even under small number of MC draws. MC simulations are used to characterize the finite sample numerical and statistical properties of EIS-based ML estimators.},
	author = {Richard, Jean-Francois and Zhang, Wei},
	doi = {10.1016/j.jeconom.2007.02.007},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\ILVLL57D\\Richard et Zhang - 2007 - Efficient high-dimensional importance sampling.pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	language = {en},
	month = dec,
	number = {2},
	pages = {1385--1411},
	title = {Efficient High-Dimensional Importance Sampling},
	volume = {141},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.jeconom.2007.02.007}}

@article{RobertCasella_MonteCarloStatistical_,
	author = {Robert, Christian P and Casella, George},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\RVQFVRS6\\Robert et Casella - Monte Carlo Statistical Methods.pdf},
	language = {en},
	pages = {85},
	title = {Monte {{Carlo Statistical Methods}}}}

@incollection{Rockafellar_CoherentApproachesRisk_2007,
	author = {Rockafellar, R. Tyrrell},
	booktitle = {{{OR Tools}} and {{Applications}}: {{Glimpses}} of {{Future Technologies}}},
	doi = {10.1287/educ.1073.0032},
	editor = {Klastorin, Theodore and Gray, Paul and Greenberg, Harvey J.},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\ESZYGGTU\\Rockafellar - 2007 - Coherent Approaches to Risk in Optimization Under .pdf},
	isbn = {978-1-877640-22-3},
	language = {en},
	month = sep,
	pages = {38--61},
	publisher = {{INFORMS}},
	title = {Coherent {{Approaches}} to {{Risk}} in {{Optimization Under Uncertainty}}},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1287/educ.1073.0032}}

@article{RockafellarEtAl_SuperquantileRegressionApplications_2014,
	abstract = {The paper presents a generalized regression technique centered on a superquantile (also called conditional value-at-risk) that is consistent with that coherent measure of risk and yields more conservatively fitted curves than classical least-squares and quantile regression. In contrast to other generalized regression techniques that approximate conditional superquantiles by various combinations of conditional quantiles, we directly and in perfect analog to classical regression obtain superquantile regression functions as optimal solutions of certain error minimization problems. We show the existence and possible uniqueness of regression functions, discuss the stability of regression functions under perturbations and approximation of the underlying data, and propose an extension of the coefficient of determination R-squared for assessing the goodness of fit. The paper presents two numerical methods for solving the error minimization problems and illustrates the methodology in several numerical examples in the areas of uncertainty quantification, reliability engineering, and financial risk management.},
	author = {Rockafellar, R.T. and Royset, J.O. and Miranda, S.I.},
	doi = {10.1016/j.ejor.2013.10.046},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\WWYITPHI\\Rockafellar et al. - 2014 - Superquantile regression with applications to buff.pdf},
	issn = {03772217},
	journal = {European Journal of Operational Research},
	language = {en},
	month = apr,
	number = {1},
	pages = {140--154},
	title = {Superquantile Regression with Applications to Buffered Reliability, Uncertainty Quantification, and Conditional Value-at-Risk},
	volume = {234},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.ejor.2013.10.046}}

@article{RockafellarUryasev_ConditionalValueatriskGeneral_2002,
	abstract = {Fundamental properties of conditional value-at-risk (CVaR), as a measure of risk with significant advantages over value-at-risk (VaR), are derived for loss distributions in finance that can involve discreetness. Such distributions are of particular importance in applications because of the prevalence of models based on scenarios and finite sampling. CVaR is able to quantify dangers beyond VaR and moreover it is coherent. It provides optimization short-cuts which, through linear programming techniques, make practical many large-scale calculations that could otherwise be out of reach. The numerical efficiency and stability of such calculations, shown in several case studies, are illustrated further with an example of index tracking.},
	author = {Rockafellar, R Tyrrell and Uryasev, Stanislav},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\GQ3UKRGC\\Rockafellar et Uryasev - 2002 - Conditional value-at-risk for general loss distrib.pdf},
	language = {en},
	pages = {29},
	title = {Conditional Value-at-Risk for General Loss Distributions},
	year = {2002}}

@article{RockafellarUryasev_OptimizationConditionalValueatrisk_2000,
	author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
	doi = {10.21314/JOR.2000.038},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\EF7VYUMJ\\Rockafellar et Uryasev - 2000 - Optimization of conditional value-at-risk.pdf},
	issn = {14651211},
	journal = {The Journal of Risk},
	language = {en},
	number = {3},
	pages = {21--41},
	title = {Optimization of Conditional Value-at-Risk},
	volume = {2},
	year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.21314/JOR.2000.038}}

@article{Rubinstein_CrossEntropyMethodCombinatorial_,
	abstract = {We present a new and fast method, called the cross-entropy method, for \textregistered nding the optimal solution of combinatorial and continuous nonconvex optimization problems with convex bounded domains. To \textregistered nd the optimal solution we solve a sequence of simple auxiliary smooth optimization problems based on KullbackLeibler cross-entropy, importance sampling, Markov chain and Boltzmann distribution. We use importance sampling as an important ingredient for adaptive adjustment of the temperature in the Boltzmann distribution and use Kullback-Leibler cross-entropy to \textregistered nd the optimal solution. In fact, we use the mode of a unimodal importance sampling distribution, like the mode of beta distribution, as an estimate of the optimal solution for continuous optimization and Markov chains approach for combinatorial optimization. In the later case we show almost surely convergence of our algorithm to the optimal solution. Supporting numerical results for both continuous and combinatorial optimization problems are given as well. Our empirical studies suggest that the cross-entropy method has polynomial in the size of the problem running time complexity.},
	author = {Rubinstein, Reuven},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\Q82EPD3W\\Rubinstein - The Cross-Entropy Method for Combinatorial and Con.pdf},
	language = {en},
	pages = {64},
	title = {The {{Cross}}-{{Entropy Method}} for {{Combinatorial}} and {{Continuous Optimization}}}}

@article{Rubinstein_StochasticMinimumCrossEntropy_2005,
	abstract = {We present a new method, called the minimum cross-entropy (MCE) method for approximating the optimal solution of NP-hard combinatorial optimization problems and rare-event probability estimation, which can be viewed as an alternative to the standard cross entropy (CE) method. The MCE method presents a generic adaptive stochastic version of Kull-back's classic MinxEnt method. We discuss its similarities and differences with the standard cross-entropy (CE) method and prove its convergence. We show numerically that MCE is a little more accurate than CE, but at the same time a little slower than CE. We also present a new method for trajectory generation for TSP and some related problems. We finally give some numerical results using MCE for rare-events probability estimation for simple static models, the maximal cut problem and the TSP, and point out some new areas of possible applications.},
	author = {Rubinstein, R. Y.},
	doi = {10.1007/s11009-005-6653-7},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\QFDJZRAN\\Rubinstein - 2005 - A Stochastic Minimum Cross-Entropy Method for Comb.pdf},
	issn = {1387-5841, 1573-7713},
	journal = {Methodology and Computing in Applied Probability},
	language = {en},
	month = mar,
	number = {1},
	pages = {5--50},
	title = {A {{Stochastic Minimum Cross}}-{{Entropy Method}} for {{Combinatorial Optimization}} and {{Rare}}-Event {{Estimation}}*},
	volume = {7},
	year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11009-005-6653-7}}

@article{RubinsteinGlynn_HowDealCurse_2009,
	author = {Rubinstein, Reuven Y. and Glynn, Peter W.},
	doi = {10.1080/15326340903291248},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\ZN7GD2GM\\Rubinstein et Glynn - 2009 - How to Deal with the Curse of Dimensionality of Li.pdf},
	issn = {1532-6349, 1532-4214},
	journal = {Stochastic Models},
	language = {en},
	month = nov,
	number = {4},
	pages = {547--568},
	title = {How to {{Deal}} with the {{Curse}} of {{Dimensionality}} of {{Likelihood Ratios}} in {{Monte Carlo Simulation}}},
	volume = {25},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1080/15326340903291248}}

@book{RubinsteinKroese_CrossentropyMethodUnified_2011,
	address = {{New York; London}},
	annotation = {OCLC: 1064531163},
	author = {Rubinstein, Reuven Y. and Kroese, Dirk P},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\UTKGLD5V\\Rubinstein et Kroese - 2011 - The cross-entropy method a unified approach to co.pdf},
	isbn = {978-1-4419-1940-3},
	doi ={10.1007/978-1-4757-4321-0},
	language = {en},
	publisher = {{Springer}},
	shorttitle = {The Cross-Entropy Method},
	title = {The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, {{Monte}}-{{Carlo}} Simulation and Machine Learning},
	year = {2011}}

@book{RubinsteinKroese_SimulationMonteCarlo_2008,
	address = {{Hoboken, N.J}},
	annotation = {OCLC: ocn155756971},
	author = {Rubinstein, Reuven Y. and Kroese, Dirk P.},
	edition = {2nd ed},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\UFV26BD2\\Rubinstein et Kroese - 2008 - Simulation and the monte carlo method.pdf},
	isbn = {978-0-470-17794-5},
	keywords = {Digital computer simulation,Monte Carlo method},
	language = {en},
	lccn = {QA298 .R8 2008},
	publisher = {{John Wiley \& Sons}},
	series = {Wiley Series in Probability and Statistics},
	title = {Simulation and the Monte Carlo Method},
	year = {2008}}

@book{RubinsteinKroese_SimulationMonteCarlo_2017,
	address = {{Hoboken, New Jersey}},
	annotation = {OCLC: 994882892},
	author = {Rubinstein, Reuven Y. and Kroese, Dirk P.},
	edition = {Third edition},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\ERPQ2BX7\\Rubinstein et Kroese - 2017 - Simulation and the Monte Carlo method.pdf},
	isbn = {978-1-118-63220-8 978-1-118-63216-1},
	language = {en},
	publisher = {{Wiley}},
	series = {Wiley Series in Probability and Statistics},
	title = {Simulation and the {{Monte Carlo}} Method},
	year = {2017},
	doi = {10.1002/9781118631980}
	}

@inproceedings{RubioMestre_EstimationCovarianceEigenspectrum_2008,
	abstract = {In this paper, we propose an estimator of the eigenspectrum of the array observation covariance matrix that builds upon the well-known power method and is consistent for an arbitrarily large array dimension. Traditional estimators based on the eigendecomposition of the sample covariance matrix are known to be consistent provided that the number of observations grow to infinity with respect to any other dimension in the signal model. On the contrary, in order to avoid the loss in the estimation accuracy associated with practical finite sample-size situations, a generalization of the conventional implementation is derived that proves to be a very good approximation for a sample-size and an array dimension that are comparatively large. The proposed solution is applied to the construction of a subspace-based extension of the Capon source power estimator. For our purposes, we resort to the theory of the spectral analysis of large dimensional random matrices, or random matrix theory. As it is shown via numerical simulations, the new estimator turns out to allow for a significantly improved estimation accuracy in practical finite sample-support scenarios.},
	address = {{Darmstadt, Germany}},
	author = {Rubio, Francisco and Mestre, Xavier},
	booktitle = {2008 5th {{IEEE Sensor Array}} and {{Multichannel Signal Processing Workshop}}},
	doi = {10.1109/SAM.2008.4606900},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\AM5GZIGJ\\Rubio et Mestre - 2008 - On the estimation of the covariance eigenspectrum .pdf},
	isbn = {978-1-4244-2240-1},
	language = {en},
	month = jul,
	pages = {404--408},
	publisher = {{IEEE}},
	title = {On the Estimation of the Covariance Eigenspectrum of Array Sample Observations},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/SAM.2008.4606900}}

@article{Sanz-Alonso_ImportanceSamplingNecessary_2016,
	abstract = {Importance sampling approximates expectations with respect to a target measure by using samples from a proposal measure. The performance of the method over large classes of test functions depends heavily on the closeness between both measures. We derive a general bound that needs to hold for importance sampling to be successful, and relates the f -divergence between the target and the proposal to the sample size. The bound is deduced from a new and simple information theory paradigm for the study of importance sampling. As examples of the general theory we give necessary conditions on the sample size in terms of the Kullback-Leibler and {$\chi$}2 divergences, and the total variation and Hellinger distances. Our approach is non-asymptotic, and its generality allows to tell apart the relative merits of these metrics. Unsurprisingly, the nonsymmetric divergences give sharper bounds than total variation or Hellinger. Our results extend existing necessary conditions \textemdash and complement sufficient ones\textemdash{} on the sample size required for importance sampling.},
	archiveprefix = {arXiv},
	author = {{Sanz-Alonso}, Daniel},
	eprint = {1608.08814},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\DAGS5HLJ\\Sanz-Alonso - 2016 - Importance Sampling and Necessary Sample Size an .pdf;C\:\\Users\\m.el-masri\\Zotero\\storage\\RJQEYIU3\\Sanz-Alonso - 2016 - Importance Sampling and Necessary Sample Size an .pdf},
	journal = {arXiv:1608.08814 [stat]},
	keywords = {Statistics - Computation},
	language = {en},
	month = aug,
	primaryclass = {stat},
	shorttitle = {Importance {{Sampling}} and {{Necessary Sample Size}}},
	title = {Importance {{Sampling}} and {{Necessary Sample Size}}: An {{Information Theory Approach}}},
	year = {2016}}

@article{ScharthKohn_ParticleEfficientImportance_2016,
	abstract = {The efficient importance sampling (EIS) method is a general principle for the numerical evaluation of highdimensional integrals that uses the sequential structure of target integrands to build variance minimising importance samplers. Despite a number of successful applications in high dimensions, it is well known that importance sampling strategies are subject to an exponential growth in variance as the dimension of the integration increases. We solve this problem by recognising that the EIS framework has an offline sequential Monte Carlo interpretation. The particle EIS method is based on non-standard resampling weights that take into account the construction of the importance sampler as a sequential approximation to the state smoothing density. We apply the method for a range of univariate and bivariate stochastic volatility specifications. We also develop a new application of the EIS approach to state space models with Student's t state innovations. Our results show that the particle EIS method strongly outperforms both the standard EIS method and particle filters for likelihood evaluation in high dimensions. We illustrate the efficiency of the method for Bayesian inference using the particle marginal Metropolis\textendash Hastings and importance sampling squared algorithms.},
	author = {Scharth, Marcel and Kohn, Robert},
	doi = {10.1016/j.jeconom.2015.03.047},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\WBTQGPAZ\\Scharth et Kohn - 2016 - Particle efficient importance sampling.pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	language = {en},
	month = jan,
	number = {1},
	pages = {133--147},
	title = {Particle Efficient Importance Sampling},
	volume = {190},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.jeconom.2015.03.047}}

@article{SchuellerPradlwarter_BenchmarkStudyReliability_2007,
	abstract = {This work is concerned with a Benchmark study on reliability estimation of structural systems. The Benchmark study attempts to assess various recently proposed alternative procedures for reliability estimation with respect to their accuracy and computational efficiency. The emphasis of this study is on systems which include a large number of random variables. For this purpose three sample problems have been selected which cover a wide range of cases of interest in the engineering practice and involve linear and nonlinear systems with uncertainties in the material properties and/or the loading conditions. Here an overview of the Benchmark study is provided.},
	author = {Schu{\"e}ller, G.I. and Pradlwarter, H.J.},
	doi = {10.1016/j.strusafe.2006.07.010},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\W8SP7SRK\\Schu{\"e}ller et Pradlwarter - 2007 - Benchmark study on reliability estimation in highe.pdf},
	issn = {01674730},
	journal = {Structural Safety},
	language = {en},
	month = jul,
	number = {3},
	pages = {167--182},
	title = {Benchmark Study on Reliability Estimation in Higher Dimensions of Structural Systems \textendash{} {{An}} Overview},
	volume = {29},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.strusafe.2006.07.010}}

@article{SchusterEtAl_ExactActiveSubspace_2017,
	abstract = {We consider the application of active subspaces to inform a Metropolis\textendash Hastings algorithm, thereby aggressively reducing the computational dimension of the sampling problem. We show that the original formulation, as proposed by Constantine, Kent, and BuiThanh (SIAM J. Sci. Comput., 38(5):A2779\textendash A2805, 2016), possesses asymptotic bias. Using pseudo-marginal arguments, we develop an asymptotically unbiased variant. Our algorithm is applied to a synthetic multimodal target distribution as well as a Bayesian formulation of a parameter inference problem for a Lorenz-96 system.},
	archiveprefix = {arXiv},
	author = {Schuster, Ingmar and Constantine, Paul G. and Sullivan, T. J.},
	eprint = {1712.02749},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\3VXUI5T3\\Schuster et al. - 2017 - Exact active subspace Metropolis-Hastings, with ap.pdf},
	journal = {arXiv:1712.02749 [stat]},
	keywords = {62H25,65C05,65C40,65C40; 65C05; 62H25,Statistics - Applications,Statistics - Computation},
	language = {en},
	month = dec,
	primaryclass = {stat},
	title = {Exact Active Subspace {{Metropolis}}-{{Hastings}}, with Applications to the {{Lorenz}}-96 System},
	year = {2017}}

@article{ShiEtAl_AccurateEfficientEstimation_2019,
	abstract = {Abstract                            Motivation               Small P-values are often required to be accurately estimated in large-scale genomic studies for the adjustment of multiple hypothesis tests and the ranking of genomic features based on their statistical significance. For those complicated test statistics whose cumulative distribution functions are analytically intractable, existing methods usually do not work well with small P-values due to lack of accuracy or computational restrictions. We propose a general approach for accurately and efficiently estimating small P-values for a broad range of complicated test statistics based on the principle of the cross-entropy method and Markov chain Monte Carlo sampling techniques.                                         Results               We evaluate the performance of the proposed algorithm through simulations and demonstrate its application to three real-world examples in genomic studies. The results show that our approach can accurately evaluate small to extremely small P-values (e.g. 10-6 to 10-100). The proposed algorithm is helpful for the improvement of some existing test procedures and the development of new test procedures in genomic studies.                                         Availability and implementation               R programs for implementing the algorithm and reproducing the results are available at: https://github.com/shilab2017/MCMC-CE-codes.                                         Supplementary information               Supplementary data are available at Bioinformatics online.},
	author = {Shi, Yang and Wang, Mengqiao and Shi, Weiping and Lee, Ji-Hyun and Kang, Huining and Jiang, Hui},
	doi = {10.1093/bioinformatics/bty1005},
	editor = {Berger, Bonnie},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\SRPHTJEU\\Shi et al. - 2019 - Accurate and efficient estimation of small P-value.pdf},
	issn = {1367-4803, 1460-2059},
	journal = {Bioinformatics},
	language = {en},
	month = jul,
	number = {14},
	pages = {2441--2448},
	shorttitle = {Accurate and Efficient Estimation of Small {{P}}-Values with the Cross-Entropy Method},
	title = {Accurate and Efficient Estimation of Small {{P}}-Values with the Cross-Entropy Method: Applications in Genomic Data Analysis},
	volume = {35},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1093/bioinformatics/bty1005}}

@article{SunEtAl_RobustEstimationStructured_2016,
	abstract = {This paper considers the problem of robustly estimating a structured covariance matrix with an elliptical underlying distribution with a known mean. In applications where the covariance matrix naturally possesses a certain structure, taking the prior structure information into account in the estimation procedure is beneficial to improving the estimation accuracy. We propose incorporating the prior structure information into Tyler's M-estimator and formulating the problem as minimizing the cost function of Tyler's estimator under the prior structural constraint. First, the estimation under a general convex structural constraint is introduced with an efficient algorithm for finding the estimator derived based on the majorization-minimization (MM) algorithm framework. Then, the algorithm is tailored to several special structures that enjoy a wide range of applications in signal processing related fields, namely, sum of rank-one matrices, Toeplitz, and banded Toeplitz structure. In addition, two types of non-convex structures, i.e., the Kronecker structure and the spiked covariance structure, are also discussed, where it is shown that simple algorithms can be derived under the guidelines of MM. The algorithms are guaranteed to converge to a stationary point of the problems. Furthermore, if the constraint set is geodesically convex, such as the Kronecker structure set, then the algorithm converges to a global minimum. Numerical results show that the proposed estimator achieves a smaller estimation error than the benchmark estimators at a lower computational cost.},
	author = {Sun, Ying and Babu, Prabhu and Palomar, Daniel P.},
	doi = {10.1109/TSP.2016.2546222},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\LP9LUYFK\\Sun et al. - 2016 - Robust Estimation of Structured Covariance Matrix .pdf},
	issn = {1053-587X, 1941-0476},
	journal = {IEEE Transactions on Signal Processing},
	language = {en},
	month = jul,
	number = {14},
	pages = {3576--3590},
	title = {Robust {{Estimation}} of {{Structured Covariance Matrix}} for {{Heavy}}-{{Tailed Elliptical Distributions}}},
	volume = {64},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/TSP.2016.2546222}}

@article{SunHong_AsymptoticRepresentationsImportancesampling_2010,
	abstract = {Value-at-risk (VaR) and conditional value-at-risk (CVaR) are important risk measures. They are often estimated by using importance-sampling (IS) techniques. In this paper, we derive the asymptotic representations for IS estimators of VaR and CVaR. Based on these representations, we are able to prove the consistency and asymptotic normality of the estimators and to provide simple conditions under which the IS estimators have smaller asymptotic variances than the ordinary Monte Carlo estimators.},
	author = {Sun, Lihua and Hong, L. Jeff},
	doi = {10.1016/j.orl.2010.02.007},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\4SNTQP9T\\Sun et Hong - 2010 - Asymptotic representations for importance-sampling.pdf},
	issn = {01676377},
	journal = {Operations Research Letters},
	language = {en},
	month = jul,
	number = {4},
	pages = {246--251},
	title = {Asymptotic Representations for Importance-Sampling Estimators of Value-at-Risk and Conditional Value-at-Risk},
	volume = {38},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.orl.2010.02.007}}

@article{TrindadeEtAl_FinancialPredictionConstrained_2007,
	abstract = {A new class of asymmetric loss functions derived from the least absolute deviations or least squares loss with a constraint on the mean of one tail of the residual error distribution, is introduced for analyzing financial data. Motivated by risk management principles, the primary intent is to provide ``cautious'' forecasts under uncertainty. The net effect on fitted models is to shape the residuals so that on average only a prespecified proportion of predictions tend to fall above or below a desired threshold. The loss functions are reformulated as objective functions in the context of parameter estimation for linear regression models, and it is demonstrated how optimization can be implemented via linear programming. The method is a competitor of quantile regression, but is more flexible and broader in scope. An application is illustrated on prediction of NDX and SPX index returns data, while controlling the magnitude of a fraction of worst losses.},
	author = {Trindade, A. Alexandre and Uryasev, Stan and Shapiro, Alexander and Zrazhevsky, Grigory},
	doi = {10.1016/j.jbankfin.2007.04.014},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\TAT9SEBS\\Trindade et al. - 2007 - Financial prediction with constrained tail risk.pdf},
	issn = {03784266},
	journal = {Journal of Banking \& Finance},
	language = {en},
	month = nov,
	number = {11},
	pages = {3524--3538},
	title = {Financial Prediction with Constrained Tail Risk},
	volume = {31},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.jbankfin.2007.04.014}}

@inproceedings{TuffinRidder_ProbabilisticBoundedRelative_2012,
	abstract = {In rare event simulation, we look for estimators such that the relative accuracy of the output is ``controlled'' when the rarity is getting more and more critical. Different robustness properties of estimators have been defined in the literature. However, these properties are not adapted to estimators coming from a parametric family for which the optimal parameter is random due to a learning algorithm. These estimators have random accuracy. For this reason, we motivate in this paper the need to define probabilistic robustness properties. We especially focus on the so-called probabilistic bounded relative error property. We additionally provide sufficient conditions, both in general and Markov settings, to satisfy such a property, and hope that it will foster discussions and new works in the area.},
	address = {{Berlin, Germany}},
	author = {Tuffin, Bruno and Ridder, Ad},
	booktitle = {Proceedings {{Title}}: {{Proceedings}} of the 2012 {{Winter Simulation Conference}} ({{WSC}})},
	doi = {10.1109/WSC.2012.6465041},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\WFQFJTPN\\Tuffin et Ridder - 2012 - Probabilistic bounded relative error for rare even.pdf},
	isbn = {978-1-4673-4782-2 978-1-4673-4779-2 978-1-4673-4780-8 978-1-4673-4781-5},
	language = {en},
	month = dec,
	pages = {1--12},
	publisher = {{IEEE}},
	title = {Probabilistic Bounded Relative Error for Rare Event Simulation Learning Techniques},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/WSC.2012.6465041}}

@article{TumminelloEtAl_KullbackLeiblerDistanceMeasure_2007,
	abstract = {We show that the Kullback-Leibler distance is a good measure of the statistical uncertainty of correlation matrices estimated by using a finite set of data. For correlation matrices of multivariate Gaussian variables we analytically determine the expected values of the Kullback-Leibler distance of a sample correlation matrix from a reference model and we show that the expected values are known also when the specific model is unknown. We propose to make use of the Kullback-Leibler distance to estimate the information extracted from a correlation matrix by correlation filtering procedures. We also show how to use this distance to measure the stability of filtering procedures with respect to statistical uncertainty. We explain the effectiveness of our method by comparing four filtering procedures, two of them being based on spectral analysis and the other two on hierarchical clustering. We compare these techniques as applied both to simulations of factor models and empirical data. We investigate the ability of these filtering procedures in recovering the correlation matrix of models from simulations. We discuss such an ability in terms of both the heterogeneity of model parameters and the length of data series. We also show that the two spectral techniques are typically more informative about the sample correlation matrix than techniques based on hierarchical clustering, whereas the latter are more stable with respect to statistical uncertainty.},
	archiveprefix = {arXiv},
	author = {Tumminello, Michele and Lillo, Fabrizio and Mantegna, Rosario Nunzio},
	doi = {10.1103/PhysRevE.76.031123},
	eprint = {0706.0168},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\SP3PUH8C\\Tumminello et al. - 2007 - Kullback-Leibler distance as a measure of the info.pdf},
	issn = {1539-3755, 1550-2376},
	journal = {Physical Review E},
	keywords = {Physics - Data Analysis,Physics - Data Analysis; Statistics and Probability,Physics - Physics and Society,Quantitative Finance - Statistical Finance,Statistics and Probability},
	language = {en},
	month = sep,
	number = {3},
	pages = {031123},
	title = {Kullback-{{Leibler}} Distance as a Measure of the Information Filtered from Multivariate Data},
	volume = {76},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1103/PhysRevE.76.031123}}

@inproceedings{UnverenAcan_MultiobjectiveOptimizationCross_2007,
	abstract = {This paper presents a novel multiobjective optimization strategy based on the cross entropy method (MOCE). The cross-entropy method (CE) is a stochastic learning algorithm inspired from rare event simulations and proved to be successful in the solution of difficult single objective real-valued optimization problems. The presented work extends the use of cross-entropy method to real-valued multiobjective optimization. For this purpose, parameters of CE search are adapted using the information collected from clustered nondominated solutions on the Pareto front.},
	address = {{Singapore}},
	author = {Unveren, Ahmet and Acan, Adnan},
	booktitle = {2007 {{IEEE Congress}} on {{Evolutionary Computation}}},
	doi = {10.1109/CEC.2007.4424862},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\X9IZYAZ7\\Unveren et Acan - 2007 - Multi-objective optimization with cross entropy me.pdf},
	isbn = {978-1-4244-1339-3},
	language = {en},
	month = sep,
	pages = {3065--3071},
	publisher = {{IEEE}},
	shorttitle = {Multi-Objective Optimization with Cross Entropy Method},
	title = {Multi-Objective Optimization with Cross Entropy Method: {{Stochastic}} Learning with Clustered Pareto Fronts},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/CEC.2007.4424862}}

@book{Uriasev_ProbabilisticConstrainedOptimization_2011,
	address = {{New York; London}},
	annotation = {OCLC: 752482835},
	author = {Ur\t{ia}s'ev, S. P},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\7IAHVNH8\\Uri︠a︡sʹev - 2011 - Probabilistic constrained optimization methodolog.pdf},
	isbn = {978-1-4419-4840-3},
	language = {en},
	publisher = {{Springer}},
	shorttitle = {Probabilistic Constrained Optimization},
	title = {Probabilistic Constrained Optimization: Methodology and Applications},
	year = {2011}}
	
	@article{UribeEtAl_CrossentropybasedImportanceSampling_2020,
author = {Uribe, Felipe and Papaioannou, Iason and Marzouk, Youssef M. and Straub, Daniel},
title = {Cross-Entropy-Based Importance Sampling with Failure-Informed Dimension Reduction for Rare Event Simulation},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {9},
number = {2},
pages = {818-847},
year = {2021},
doi = {10.1137/20M1344585},
}




@article{VohraEtAl_SubspacebasedDimensionReduction_2018,
	abstract = {We focus on an efficient approach for quantification of uncertainty in complex chemical reaction networks with a large number of uncertain parameters. Parameter dimension reduction is accomplished by computing an active subspace that predominantly captures the variability in the quantity of interest (QoI). In the present work, we compute the active subspace for a H2/O2 mechanism that involves 19 chemical reactions, using an efficient iterative strategy. The active subspace is first computed for a 19-parameter problem wherein only the uncertainty in the pre-exponents of the individual reaction rates is considered. This is followed by the analysis of a 33-dimensional case wherein the activation energies are also considered uncertain. In both cases, a 1-dimensional active subspace is identified, which indicates enormous potential for efficient statistical analysis of complex chemical systems. In addition, we explore links between active subspaces and global sensitivity analysis, and exploit these links for identification of key contributors to the variability in the model response.},
	archiveprefix = {arXiv},
	author = {Vohra, M. and Alexanderian, A. and Guy, H. and Mahadevan, S.},
	eprint = {1810.00955},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\QZJSHG8Y\\Vohra et al. - 2018 - Subspace-based dimension reduction for chemical ki.pdf},
	journal = {arXiv:1810.00955 [physics]},
	keywords = {Physics - Chemical Physics},
	language = {en},
	month = oct,
	primaryclass = {physics},
	title = {Subspace-Based Dimension Reduction for Chemical Kinetics Applications with Epistemic Uncertainty},
	year = {2018}}

@article{WangEtAl_BayesianOptimizationHigh_,
	abstract = {Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple and applies to domains with both categorical and continuous variables. The experiments demonstrate that REMBO can effectively solve high-dimensional problems, including automatic parameter configuration of a popular mixed integer linear programming solver.},
	author = {Wang, Ziyu and Zoghi, Masrour and Hutter, Frank and Matheson, David},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\7SWZZGRK\\Wang et al. - Bayesian Optimization in High Dimensions via Rando.pdf},
	language = {en},
	pages = {7},
	title = {Bayesian {{Optimization}} in {{High Dimensions}} via {{Random Embeddings}}}}

@article{WangFan_HighdimensionalCorrelationMatrix_2021,
	author = {Wang, Chaojie and Fan, Xiaodan},
	doi = {10.4310/20-SII655},
	issn = {19387989, 19387997},
	journal = {Statistics and Its Interface},
	language = {en},
	number = {3},
	pages = {351--358},
	shorttitle = {High-Dimensional Correlation Matrix Estimation for {{Gaussian}} Data},
	title = {High-Dimensional Correlation Matrix Estimation for {{Gaussian}} Data: A {{Bayesian}} Perspective},
	volume = {14},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.4310/20-SII655}}

@article{WangSong_CrossentropybasedAdaptiveImportance_2016,
	abstract = {In order to address challenges in performing importance sampling in a high dimensional space of random variables, the paper develops a cross-entropy-based adaptive importance sampling technique that employs a von Mises-Fisher mixture as the sampling density model. By small-size pre-samplings, the proposed approach first finds a near-optimal sampling density by minimizing the Kullback\textendash Leibler cross entropy between a von Mises-Fisher mixture model and the absolute best importance sampling density. To facilitate the minimization process, updating rules for parameters of the von Mises-Fisher mixture model are derived. Various practical issues associated with the updating rules are discussed and heuristic rules to improve the performance of the importance sampling are introduced. At the stage of final sampling, two slightly different sampling strategies are proposed to provide analysis options. Three numerical examples are investigated to test and demonstrate the proposed importance sampling method. The numerical examples show that the proposed approach, applicable to both component and system reliability problems, has superior performance for high dimensional reliability analysis problems with low failure probabilities.},
	author = {Wang, Ziqi and Song, Junho},
	doi = {10.1016/j.strusafe.2015.11.002},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\FHMAZ36M\\Wang et Song - 2016 - Cross-entropy-based adaptive importance sampling u.pdf},
	issn = {01674730},
	journal = {Structural Safety},
	language = {en},
	month = mar,
	pages = {42--52},
	title = {Cross-Entropy-Based Adaptive Importance Sampling Using von {{Mises}}-{{Fisher}} Mixture for High Dimensional Reliability Analysis},
	volume = {59},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.strusafe.2015.11.002}}

@article{WangZhou_ExplicitCrossEntropy_2013,
	abstract = {The key issue in importance sampling is the choice of the alternative sampling distribution, which is often chosen from the exponential tilt family of the underlying distribution. However, when the problem exhibits certain kind of nonconvexity, it is very likely that a single exponential change of measure will never attain asymptotic optimality and may lead to erroneous estimates. In this paper we introduce an explicit iterative scheme which combines the traditional cross-entropy method and the EM algorithm to find an efficient alternative sampling distribution in the form of mixtures. We also study the applications of this scheme to option price estimation.},
	archiveprefix = {arXiv},
	author = {Wang, Hui and Zhou, Xiang},
	eprint = {1305.3226},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\G52LZNQW\\Wang et Zhou - 2013 - An Explicit Cross Entropy Scheme for Mixtures.pdf},
	journal = {arXiv:1305.3226 [math]},
	keywords = {Mathematics - Probability},
	language = {en},
	month = may,
	primaryclass = {math},
	title = {An {{Explicit Cross Entropy Scheme}} for {{Mixtures}}},
	year = {2013}}

@article{WeiEtAl_EfficientSamplingMethods_2012,
	abstract = {An important problem in structure reliability analysis is how to reduce the failure probability. In this work, we introduce a main and total effect indices framework of global reliability sensitivity. By decreasing the uncertainty of input variables with high main effect indices, the most reduction of failure probability can be obtained. By decreasing the uncertainty of the input variables with small total effect indices (close to zero), the failure probability will not be reduced significantly. The efficient sampling methods for evaluating the main and total effect indices are presented. For the problem with large failure probability, a single-loop Monte Carlo simulation (MCS) is derived for computing these sensitivity indices. For the problem with small failure probability, the single-loop sampling methods combined with the importance sampling procedure (IS) and the truncated importance sampling procedure (TIS) respectively are derived for improving the calculation efficiency. Two numerical examples and one engineering example are introduced for demonstrating the efficiency and precision of the calculation methods and illustrating the engineering significance of the global reliability sensitivity indices.},
	author = {Wei, Pengfei and Lu, Zhenzhou and Hao, Wenrui and Feng, Jun and Wang, Bintuan},
	doi = {10.1016/j.cpc.2012.03.014},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\U9IN362M\\Wei et al. - 2012 - Efficient sampling methods for global reliability .pdf},
	issn = {00104655},
	journal = {Computer Physics Communications},
	language = {en},
	month = aug,
	number = {8},
	pages = {1728--1743},
	title = {Efficient Sampling Methods for Global Reliability Sensitivity Analysis},
	volume = {183},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.cpc.2012.03.014}}

@article{Wiesel_UnifiedFrameworkRegularized_2012,
	abstract = {We consider regularized covariance estimation in scaled Gaussian settings, e.g., elliptical distributions, compound-Gaussian processes and spherically invariant random vectors. Asymptotically in the number of samples, the classical maximum likelihood (ML) estimate is optimal under different criteria and can be efficiently computed even though the optimization is nonconvex. We propose a unified framework for regularizing this estimate in order to improve its finite sample performance. Our approach is based on the discovery of hidden convexity within the ML objective. We begin by restricting the attention to diagonal covariance matrices. Using a simple change of variables, we transform the problem into a convex optimization that can be efficiently solved. We then extend this idea to nondiagonal matrices using convexity on the manifold of positive definite matrices. We regularize the problem using appropriately convex penalties. These allow for shrinkage towards the identity matrix, shrinkage towards a diagonal matrix, shrinkage towards a given positive definite matrix, and regularization of the condition number. We demonstrate the advantages of these estimators using numerical simulations.},
	author = {Wiesel, Ami},
	doi = {10.1109/TSP.2011.2170685},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\YI6DEUTQ\\Wiesel - 2012 - Unified Framework to Regularized Covariance Estima.pdf},
	issn = {1053-587X, 1941-0476},
	journal = {IEEE Transactions on Signal Processing},
	language = {en},
	month = jan,
	number = {1},
	pages = {29--38},
	title = {Unified {{Framework}} to {{Regularized Covariance Estimation}} in {{Scaled Gaussian Models}}},
	volume = {60},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/TSP.2011.2170685}}

@book{Williams_ProbabilityMartingales_2018,
	address = {{Cambridge}},
	annotation = {OCLC: 1080326332},
	author = {Williams, David},
	edition = {22nd printing},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\EEIFILMJ\\Williams - 2018 - Probability with martingales.pdf},
	isbn = {978-0-521-40605-5},
	language = {en},
	publisher = {{Cambridge Univ. Pr}},
	series = {Cambridge Mathematical Textbooks},
	title = {Probability with Martingales},
	year = {2018}}

@article{WycoffEtAl_SequentialLearningActive_2019,
	abstract = {In recent years, active subspace methods (ASMs) have become a popular means of performing subspace sensitivity analysis on black-box functions. Naively applied, however, ASMs require gradient evaluations of the target function. In the event of noisy, expensive, or stochastic simulators, evaluating gradients via finite differencing may be infeasible. In such cases, often a surrogate model is employed, on which finite differencing is performed. When the surrogate model is a Gaussian process, we show that the ASM estimator is available in closed form, rendering the finite-difference approximation unnecessary. We use our closed-form solution to develop acquisition functions focused on sequential learning tailored to sensitivity analysis on top of ASMs. We also show that the traditional ASM estimator may be viewed as a method of moments estimator for a certain class of Gaussian processes. We demonstrate how uncertainty on Gaussian process hyperparameters may be propagated to uncertainty on the sensitivity analysis, allowing model-based confidence intervals on the active subspace. Our methodological developments are illustrated on several examples.},
	archiveprefix = {arXiv},
	author = {Wycoff, Nathan and Binois, Mickael and Wild, Stefan M.},
	eprint = {1907.11572},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\FVQSBCSP\\Wycoff et al. - 2019 - Sequential Learning of Active Subspaces.pdf},
	journal = {arXiv:1907.11572 [cs, stat]},
	keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
	language = {en},
	month = jul,
	primaryclass = {cs, stat},
	title = {Sequential {{Learning}} of {{Active Subspaces}}},
	year = {2019}}

@article{XueqianZhaoEtAl_HierarchicalCrossEntropyOptimization_2011,
	abstract = {Decoupling capacitor (decap) has been widely used to effectively reduce dynamic power supply noise. Traditional decap budgeting algorithms usually explore the sensitivity-based nonlinear optimizations or conjugate gradient (CG) methods, which can be prohibitively expensive for large-scale decap budgeting problems and cannot be easily parallelized. In this paper, we propose a hierarchical cross-entropy based optimization technique which is more efficient and parallel-friendly. Cross-entropy (CE) is an advanced optimization framework which explores the power of rare event probability theory and importance sampling. To achieve the high efficiency, a sensitivity-guided cross-entropy (SCE) algorithm is introduced which integrates CE with a partitioning-based sampling strategy to effectively reduce the solution space in solving the large-scale decap budgeting problems. Compared to improved CG method and conventional CE method, SCE with Latin hypercube sampling method (SCELHS) can provide 2\texttimes{} speedups, while achieving up to 25\% improvement on power supply noise. To further improve decap optimization solution quality, SCE with sequential importance sampling (SCE-SIS) method is also studied and implemented. Compared to SCE-LHS, in similar runtime, SCE-SIS can lead to 16.8\% further reduction on the total power supply noise.},
	author = {{Xueqian Zhao} and {Yonghe Guo} and {Xiaodao Chen} and {Zhuo Feng} and {Shiyan Hu}},
	doi = {10.1109/TCAD.2011.2162068},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\KRTXZXYH\\Xueqian Zhao et al. - 2011 - Hierarchical Cross-Entropy Optimization for Fast O.pdf},
	issn = {0278-0070, 1937-4151},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	language = {en},
	month = nov,
	number = {11},
	pages = {1610--1620},
	title = {Hierarchical {{Cross}}-{{Entropy Optimization}} for {{Fast On}}-{{Chip Decap Budgeting}}},
	volume = {30},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCAD.2011.2162068}}

@article{YangEtAl_SystemReliabilityAnalysis_2020,
	abstract = {System reliability analysis with small failure probability is investigated in this paper. Because multiple failure modes exist, the system performance function has multiple failure regions and multiple most probable points (MPPs). This paper reports an innovative method combining active learning Kriging (ALK) model with multimodal adaptive important sampling (MAIS). In each iteration of the proposed method, MPPs on a so-called surrogate limit state surface (LSS) of the system are explored, important samples are generated, optimal training points are chosen, the Kriging models are updated, and the surrogate LSS is refined. After several iterations, the surrogate LSS will converge to the true LSS. A recently proposed evolutionary multimodal optimization algorithm is adapted to obtain all the potential MPPs on the surrogate LSS, and a filtering technique is introduced to exclude improper solutions. In this way, the unbiasedness of our method is guaranteed. To avoid approximating the unimportant components, the training points are only chosen from the important samples located in the truncated candidate region (TCR). The proposed method is termed as ALK-MAIS-TCR. The accuracy and efficiency of ALK-MAIS-TCR are demonstrated by four complicated case studies.},
	author = {Yang, Xufeng and Cheng, Xin and Wang, Tai and Mi, Caiying},
	doi = {10.1007/s00158-020-02515-5},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\YL87HEN4\\Yang et al. - 2020 - System reliability analysis with small failure pro.pdf},
	issn = {1615-147X, 1615-1488},
	journal = {Structural and Multidisciplinary Optimization},
	language = {en},
	month = feb,
	title = {System Reliability Analysis with Small Failure Probability Based on Active Learning {{Kriging}} Model and Multimodal Adaptive Importance Sampling},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00158-020-02515-5}}

@article{ZahmEtAl_CertifiedDimensionReduction_2018,
	abstract = {We propose a dimension reduction technique for Bayesian inverse problems with nonlinear forward operators, non-Gaussian priors, and non-Gaussian observation noise. The likelihood function is approximated by a ridge function, i.e., a map which depends non-trivially only on a few linear combinations of the parameters. We build this ridge approximation by minimizing an upper bound on the Kullback\textendash Leibler divergence between the posterior distribution and its approximation. This bound, obtained via logarithmic Sobolev inequalities, allows one to certify the error of the posterior approximation. Computing the bound requires computing the second moment matrix of the gradient of the log-likelihood function. In practice, a sample-based approximation of the upper bound is then required. We provide an analysis that enables control of the posterior approximation error due to this sampling. Numerical and theoretical comparisons with existing methods illustrate the benefits of the proposed methodology.},
	archiveprefix = {arXiv},
	author = {Zahm, Olivier and Cui, Tiangang and Law, Kody and Spantini, Alessio and Marzouk, Youssef},
	eprint = {1807.03712},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\BHNPAYJ3\\Zahm et al. - 2018 - Certified dimension reduction in nonlinear Bayesia.pdf},
	journal = {arXiv:1807.03712 [math, stat]},
	keywords = {Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Methodology},
	language = {en},
	month = jul,
	primaryclass = {math, stat},
	title = {Certified Dimension Reduction in Nonlinear {{Bayesian}} Inverse Problems},
	year = {2018}}

@article{ZahmEtAl_GradientbasedDimensionReduction_2018,
	abstract = {Multivariate functions encountered in high-dimensional uncertainty quantification problems often vary most strongly along a few dominant directions in the input parameter space. We propose a gradient-based method for detecting these directions and using them to construct ridge approximations of such functions, in the case where the functions are vector-valued (e.g., taking values in Rn). The methodology consists of minimizing an upper bound on the approximation error, obtained by subspace Poincar\textasciiacute e inequalities. We provide a thorough mathematical analysis in the case where the parameter space is equipped with a Gaussian probability measure. The resulting method generalizes the notion of active subspaces associated with scalar-valued functions. A numerical illustration shows that using gradients of the function yields effective dimension reduction. We also show how the choice of norm on the codomain of the function has an impact on the function's low-dimensional approximation.},
	archiveprefix = {arXiv},
	author = {Zahm, Olivier and Constantine, Paul and Prieur, Cl{\'e}mentine and Marzouk, Youssef},
	eprint = {1801.07922},
	eprinttype = {arxiv},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\ADN8BGU9\\Zahm et al. - 2018 - Gradient-based dimension reduction of multivariate.pdf},
	journal = {arXiv:1801.07922 [math]},
	keywords = {41A30,41A30; 41A63; 65D15,41A63,65D15,Mathematics - Analysis of PDEs},
	language = {en},
	month = jan,
	primaryclass = {math},
	title = {Gradient-Based Dimension Reduction of Multivariate Vector-Valued Functions},
	year = {2018}}

@inproceedings{ZulianiEtAl_RareeventVerificationStochastic_2012,
	abstract = {In this paper we address the problem of verifying in stochastic hybrid systems temporal logic properties whose probability of being true is very small \textemdash{} rare events. It is well known that sampling-based (Monte Carlo) techniques, such as statistical model checking, do not perform well for estimating rare-event probabilities. The problem is that the sample size required for good accuracy grows too large as the event probability tends to zero. However, several techniques have been developed to address this problem. We focus on importance sampling techniques, which bias the original system to compute highly accurate and efficient estimates. The main difficulty in importance sampling is to devise a good biasing density, that is, a density yielding a low-variance estimator. In this paper, we show how to use the cross-entropy method for generating approximately optimal biasing densities for statistical model checking. We apply the method with importance sampling and statistical model checking for estimating rare-event probabilities in stochastic hybrid systems coded as Stateflow/Simulink diagrams.},
	address = {{Beijing, China}},
	author = {Zuliani, Paolo and Baier, Christel and Clarke, Edmund M.},
	booktitle = {Proceedings of the 15th {{ACM}} International Conference on {{Hybrid Systems}}: {{Computation}} and {{Control}} - {{HSCC}} '12},
	doi = {10.1145/2185632.2185665},
	file = {C\:\\Users\\m.el-masri\\Zotero\\storage\\UNEUAI3S\\Zuliani et al. - 2012 - Rare-event verification for stochastic hybrid syst.pdf},
	isbn = {978-1-4503-1220-2},
	language = {en},
	pages = {217},
	publisher = {{ACM Press}},
	title = {Rare-Event Verification for Stochastic Hybrid Systems},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1145/2185632.2185665}}

@book{bucklew2013introduction,
  title={Introduction to rare event simulation},
  author={Bucklew, James},
  year={2013},
  publisher={Springer Science \& Business Media},
  doi = {10.1007/978-1-4757-4078-3}}
}

@article{papaioannou2019improved,
  title={Improved cross entropy-based importance sampling with a flexible mixture model},
  author={Papaioannou, Iason and Geyer, Sebastian and Straub, Daniel},
  journal={Reliability Engineering \& System Safety},
  volume={191},
  pages={106564},
  year={2019},
  publisher={Elsevier}
}

